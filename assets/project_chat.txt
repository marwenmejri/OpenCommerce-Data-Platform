i am learning DBT from a tutorial on Linkedin Learning , here is a brief description of the video i am following right now : 

In the "Modeling in dbt" video, you will learn how to work with a MariaDB MySQL database using dbt (data build tool). Here’s a detailed breakdown:

Database Access: Learn how to access your MariaDB database through VS Code.
Installing dbt: Instructions on installing the dbt MySQL package using pip install dbt-mysql.
Creating a dbt Project: Steps to create a new dbt project named "Intro" using dbt init.
Project Structure: Overview of the folders and files created during the dbt Bootstrap process, including analyses, macros, models, seeds, snapshots, target, tests, and dbt_project.yml.
Configuration: Explanation of the dbt_project.yml file and the importance of profiles for connecting to databases.
Profiles: Insight into the profiles.yml file, which contains configuration details like the type, server, port, and environment settings (dev, prod).
Models Directory: Exploration of the models directory, including default models and the use of the dbt Power user plugin for visualizing dbt models.
Lineage View: Demonstration of the lineage view to see how dbt models are related.
Schema Definition: Details on the schema.yml file, which defines table schemas and column tests for each model.

This video provides a hands-on approach to setting up and managing dbt projects, making it easier to operationalize SQL and manage data transformations effectively.

I want to work on my local laptop, its an ubuntu distribution, to start whats Maria DB? and why he installed mysql dbt package to work with it ? and let's start replicating all this in my lapop and myabe we will work with Postgres instead bc i already have it installed 
You said:
yes lets do these steps: 
If you want, I can help you:

✅ Create a minimal PostgreSQL database + sample table

✅ Configure profiles.yml automatically

✅ Build your first real model + schema.yml test

✅ Use the lineage view in dbt docs

and first let's connect to postgres shell 
You said:

postgres=# \db
       List of tablespaces
    Name    |  Owner   | Location 
------------+----------+----------
 pg_default | postgres | 
 pg_global  | postgres | 
(2 rows)

postgres=# \l
postgres=# CREATE DATABASE DBT_Tutorial;
CREATE DATABASE
postgres=# \l
                                   List of databases
     Name     |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges   
--------------+----------+----------+-------------+-------------+-----------------------
 alpha        | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
 dbt_tutorial | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
 marwen       | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =Tc/postgres         +
              |          |          |             |             | postgres=CTc/postgres+
              |          |          |             |             | marwen=CTc/postgres
 postgres     | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
 template0    | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
              |          |          |             |             | postgres=CTc/postgres
 template1    | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
              |          |          |             |             | postgres=CTc/postgres
(6 rows)
 let's connect the dbttuorial db and show schema
You said:
help me configuring this vs code extension to test my postgres db connection :
You said:
okay fine now all is working i want to execute the dbt-project  intro_dbt with the examples tables 
You said:
how can I drop all tables from a DB in postgres
You said:
 marwen  dbt run 
20:31:45  Running with dbt=1.10.13
20:31:45  Registered adapter: postgres=1.9.1
20:31:45  Found 3 models, 4 data tests, 1 seed, 447 macros
20:31:45  
20:31:45  Concurrency: 1 threads (target='dev')
20:31:45  
20:31:45  1 of 3 START sql table model public.my_first_dbt_model ......................... [RUN]
20:31:45  1 of 3 OK created sql table model public.my_first_dbt_model .................... [SELECT 2 in 0.12s]
20:31:45  2 of 3 START sql table model public.taxi_trips ................................. [RUN]
20:31:45  2 of 3 ERROR creating sql table model public.taxi_trips ........................ [ERROR in 0.02s]
20:31:45  3 of 3 START sql table model public.my_second_dbt_model ........................ [RUN]
20:31:45  3 of 3 OK created sql table model public.my_second_dbt_model ................... [SELECT 1 in 0.05s]
20:31:45  
20:31:45  Finished running 3 table models in 0 hours 0 minutes and 0.30 seconds (0.30s).
20:31:45  
20:31:45  Completed with 1 error, 0 partial successes, and 0 warnings:
20:31:45  
20:31:45  Failure in model taxi_trips (models/example/taxi_trips.sql)
20:31:45    Database Error in model taxi_trips (models/example/taxi_trips.sql)
  relation "public.raw_taxi_trips" does not exist
  LINE 16: SELECT * FROM "dbt_tutorial"."public"."raw_taxi_trips"
                         ^
  compiled code at target/run/intro_dbt/models/example/taxi_trips.sql
20:31:45  
20:31:45    compiled code at target/compiled/intro_dbt/models/example/taxi_trips.sql
20:31:45  
20:31:45  Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3


taxi_trips.sql : 

-- Loads data from the seed file "raw_taxi_trips" into a table.

-- {{ config(materialized='table') }}

SELECT * FROM {{ ref('raw_taxi_trips') }}

schema.yml: 


version: 2

models:
  - name: my_first_dbt_model
    description: "A starter dbt model"
    columns:
      - name: id
        description: "The primary key for this table"
        data_tests:
          - unique
          - not_null

  - name: my_second_dbt_model
    description: "A starter dbt model"
    columns:
      - name: id
        description: "The primary key for this table"
        data_tests:
          - unique
          - not_null

  - name: taxi_trips
    description: "Information about taxi trips"
    columns:
      - name: pickup
        description: "Pickup Time"
      - name: dropoff
        description: "Dropoff Time"
      - name: passengers
        description: "Number of Passangers"
      - name: distance
        description: "Distance"
      - name: fare
        description: "Fare"
      - name: tip
        description: "Tip"
      - name: tolls
        description: "Tolls"
      - name: total
        description: "Total"
      - name: color
        description: "Color"
      - name: payment
        description: "Payment Method"
      - name: pickup_zone
        description: "Pickup Zone"
      - name: dropoff_zone
        description: "Dropoff Zone"
      - name: pickup_borough
        description: "Pickup Borough"
      - name: dropoff_borough
        description: "Dropoff Borough"

in the screenshot you can see my project architecture 
You said:
can i only use dbt run ditrectly and maybe try ti update this ref path : 
{{ ref('raw_taxi_trips') }}
You said:
hey could you tell the difference between table and view , materialized tables and materialized views and also if there was related topics also mention them 

You said:
explain to me the incremental process in dbt shown in the screenshot , i want to understand each line of code 
You said:
how can i show the number of disctinct dates for each currency : 

dbt_incremental_db=# select * from raw_crypto_data limit 10;
 currency | detail_date | closing_price | 24_hour_open | 24h_high_usd | 24h_low_usd 
----------+-------------+---------------+--------------+--------------+-------------
 BTC      | 10/1/13     |     123.65499 |    124.30466 |    124.75166 |   122.56349
 BTC      | 10/2/13     |       125.455 |    123.65499 |     125.7585 |   123.63383
 BTC      | 10/3/13     |     108.58483 |      125.455 |    125.66566 |    83.32833
 BTC      | 10/4/13     |     118.67466 |    108.58483 |      118.675 |   107.05816
 BTC      | 10/5/13     |     121.33866 |    118.67466 |    121.93633 |   118.00566
 BTC      | 10/6/13     |     120.65533 |    121.33866 |    121.85216 |    120.5545
 BTC      | 10/7/13     |       121.795 |    120.65533 |    121.99166 |   120.43199
 BTC      | 10/8/13     |       123.033 |      121.795 |    123.64016 |   121.35066
 BTC      | 10/9/13     |       124.049 |      123.033 |     124.7835 |   122.59266
 BTC      | 10/10/13    |     125.96116 |      124.049 |    128.01683 |   123.81966
You said:
explain more this statement : 

Views are better for analytical queries that don't need persistence
You said:
yes go ahead and i want also to know a little bit more about what happens behind the scene
You said:
You want lightweight transformations and always up-to-date logic.

how can i mesure this in real life senario
You said:
i want to know more about DBT built in tests and custom tests and how they will help in a datapipeline, here is an example of a custom test: 

-- This is a custom test that checks for NULL values in a specified column

{% test custom_test(model, column_name) %}

SELECT *
FROM {{ model }}
WHERE {{ column_name }} IS NULL

{% endtest %}


version: 2

models:
  - name: my_first_dbt_model
    description: "A starter dbt model"
    columns:
      - name: id
        description: "The primary key for this table"
        data_tests:
          - unique
          - custom_test
          # - not_null

  - name: my_second_dbt_model
    description: "A starter dbt model"
    columns:
      - name: id
        description: "The primary key for this table"
        data_tests:
          - unique
          - not_null

 marwen  dbt test
16:22:18  Running with dbt=1.10.13
16:22:18  Registered adapter: postgres=1.9.1
16:22:19  Found 2 models, 4 data tests, 448 macros
16:22:19  
16:22:19  Concurrency: 2 threads (target='dev')
16:22:19  
16:22:19  1 of 4 START test custom_test_my_first_dbt_model_id ............................ [RUN]
16:22:19  2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
16:22:19  2 of 4 PASS not_null_my_second_dbt_model_id .................................... [PASS in 0.07s]
16:22:19  1 of 4 FAIL 1 custom_test_my_first_dbt_model_id ................................ [FAIL 1 in 0.07s]
16:22:19  3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
16:22:19  4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
16:22:19  3 of 4 PASS unique_my_first_dbt_model_id ....................................... [PASS in 0.03s]
16:22:19  4 of 4 PASS unique_my_second_dbt_model_id ...................................... [PASS in 0.03s]
16:22:19  
16:22:19  Finished running 4 data tests in 0 hours 0 minutes and 0.20 seconds (0.20s).
16:22:19  
16:22:19  Completed with 1 error, 0 partial successes, and 0 warnings:
16:22:19  
16:22:19  Failure in test custom_test_my_first_dbt_model_id (models/example/schema.yml)
16:22:19    Got 1 result, configured to fail if != 0
16:22:19  
16:22:19    compiled code at target/compiled/test_lab/models/example/schema.yml/custom_test_my_first_dbt_model_id.sql
16:22:19  
16:22:19  Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
               
 [~/Documents/Tutorial/DBT_Tutorial/test_lab]
 ✘  marwen   

so how can i use this in a real world scenario and maybe what to do if i want to get the invalidated data 
You said:
how could we implement this : 

If any test fails → pipeline stops → Slack/Email alert triggered

how do we catch if tests fail
You said:
let's try to solve this dbt tests challenge, : 


version: 2

# To Do: 
# 1. Include some of the built in dbt tests into the columns.
# 2. Create a custom dbt test to that will fail if any of the cohorts have more than 100 distinct adids.

models:
  - name: adid_data
    columns:
      - name: adid
        description: "The advertisting ID for this data point. Every value in column should be a distinct ad_id. This value is required."
      - name: latitude
        description: "The latitude of the data point. This value is required." 
      - name: longitude
        description: "The longitude of the data point. This value is required." 
      - name: city
        description: "The city of the data point. This value is required." 
      - name: eventdate
        description: "The date of the data point. This value is required."  
      - name: cohort
        description: "The cohort of the adid. Theree are only 6 cohorts, each named either 'one', 'two', 'three', 'four', 'five', or 'six'. This value is required. Each cohort should have a maximum of 100 distinct adids." 

first i want you to start with the built in tests
You said:
what's the error here : 


version: 2

# To Do: 
# 1. Include some of the built in dbt tests into the columns.
# 2. Create a custom dbt test to that will fail if any of the cohorts have more than 100 distinct adids.

models:
  - name: adid_data
    columns:
      - name: adid
        description: "The advertisting ID for this data point. Every value in column should be a distinct ad_id. This value is required."
        tests:
          - unique
          - not_null
      - name: latitude
        description: "The latitude of the data point. This value is required." 
        tests:
          - not_null
      - name: longitude
        description: "The longitude of the data point. This value is required." 
        tests:
          - not_null
      - name: city
        description: "The city of the data point. This value is required." 
        tests:
          - not_null
      - name: eventdate
        description: "The date of the data point. This value is required."  
        tests:
          - not_null
      - name: cohort
        description: "The cohort of the adid. Theree are only 6 cohorts, each named either 'one', 'two', 'three', 'four', 'five', or 'six'. This value is required. Each cohort should have a maximum of 100 distinct adids." 
        tests:
          - not_null
          - accepted_values:
              values: ['one', 'two', 'three', 'four', 'five', 'six']
          - cohort_max_adids_test

-- Create a custom dbt test to that will fail if any of the cohorts have more than 100 distinct adids.

{% test cohort_max_adids_test(model) %}

SELECT cohort, count(*) as adids_count
FROM {{ model }}
GROUP BY cohort
HAVING count(*) > 100
ORDER BY adids_count DESC

{% endtest %} [~/Documents/Tutorial/DBT_Tutorial/dbt_test_lab2]
 marwen  dbt test
20:18:20  Running with dbt=1.10.13
20:18:20  Registered adapter: postgres=1.9.1
20:18:21  Found 1 seed, 1 model, 10 data tests, 447 macros
20:18:21  
20:18:21  Concurrency: 1 threads (target='dev')
20:18:21  
20:18:21  1 of 10 START test accepted_values_adid_data_cohort__one__two__three__four__five__six  [RUN]
20:18:21  1 of 10 PASS accepted_values_adid_data_cohort__one__two__three__four__five__six  [PASS in 0.05s]
20:18:21  2 of 10 START test cohort_max_adids_test ....................................... [RUN]
20:18:21  2 of 10 ERROR cohort_max_adids_test ............................................ [ERROR in 0.02s]
20:18:21  3 of 10 START test cohort_max_adids_test_adid_data_cohort ...................... [RUN]
20:18:21  3 of 10 ERROR cohort_max_adids_test_adid_data_cohort ........................... [ERROR in 0.10s]
20:18:21  4 of 10 START test not_null_adid_data_adid ..................................... [RUN]
20:18:21  4 of 10 PASS not_null_adid_data_adid ........................................... [PASS in 0.02s]
20:18:21  5 of 10 START test not_null_adid_data_city ..................................... [RUN]
20:18:21  5 of 10 PASS not_null_adid_data_city ........................................... [PASS in 0.02s]
20:18:21  6 of 10 START test not_null_adid_data_cohort ................................... [RUN]
20:18:21  6 of 10 PASS not_null_adid_data_cohort ......................................... [PASS in 0.02s]
20:18:21  7 of 10 START test not_null_adid_data_eventdate ................................ [RUN]
20:18:21  7 of 10 PASS not_null_adid_data_eventdate ...................................... [PASS in 0.02s]
20:18:21  8 of 10 START test not_null_adid_data_latitude ................................. [RUN]
20:18:21  8 of 10 PASS not_null_adid_data_latitude ....................................... [PASS in 0.02s]
20:18:21  9 of 10 START test not_null_adid_data_longitude ................................ [RUN]
20:18:21  9 of 10 PASS not_null_adid_data_longitude ...................................... [PASS in 0.02s]
20:18:21  10 of 10 START test unique_adid_data_adid ...................................... [RUN]
20:18:21  10 of 10 PASS unique_adid_data_adid ............................................ [PASS in 0.02s]
20:18:21  
20:18:21  Finished running 10 data tests in 0 hours 0 minutes and 0.41 seconds (0.41s).
20:18:21  
20:18:21  Completed with 2 errors, 0 partial successes, and 0 warnings:
20:18:21  
20:18:21  Failure in test cohort_max_adids_test (tests/custom/cohort_max_adids_test.sql)
20:18:21    Database Error in test cohort_max_adids_test (tests/custom/cohort_max_adids_test.sql)
  syntax error at or near ")"
  LINE 16:     ) dbt_internal_test
               ^
  compiled code at target/run/dbt_test_lab2/tests/custom/cohort_max_adids_test.sql
20:18:21  
20:18:21    compiled code at target/compiled/dbt_test_lab2/tests/custom/cohort_max_adids_test.sql
20:18:21  
20:18:21  Failure in test cohort_max_adids_test_adid_data_cohort (models/schema.yml)
20:18:21    Compilation Error in test cohort_max_adids_test_adid_data_cohort (models/schema.yml)
  'test_cohort_max_adids_test' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
20:18:21  
20:18:21  Done. PASS=8 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=10


You said:
 marwen  dbt run 
20:26:02  Running with dbt=1.10.13
20:26:02  Registered adapter: postgres=1.9.1
20:26:02  Encountered an error:
Compilation Error
  dbt found two macros named "test_cohort_max_adids_test" in the project
  "dbt_test_lab2".
   To fix this error, rename or remove one of the following macros:
      - macros/cohort_max_adids_test.sql
      - macros/cohort_max_adids_test.sql
               
 [~/Documents/Tutorial/DBT_Tutorial/dbt_test_lab2]
 ✘  marwen  ls -l 
total 44
drwxrwxr-x 2 marwen marwen 4096 Oct 16 14:33 analyses
drwxrwxr-x 2 marwen marwen 4096 Oct 23 21:03 dbt_packages
-rw-rw-r-- 1 marwen marwen 1230 Oct 23 21:06 dbt_project.yml
drwxrwxr-x 2 marwen marwen 4096 Oct 23 21:03 logs
drwxrwxr-x 2 marwen marwen 4096 Oct 23 21:24 macros
drwxrwxr-x 2 marwen marwen 4096 Oct 23 21:05 models
-rw-rw-r-- 1 marwen marwen  571 Oct 16 14:33 README.md
drwxrwxr-x 2 marwen marwen 4096 Oct 23 21:05 seeds
drwxrwxr-x 2 marwen marwen 4096 Oct 16 14:33 snapshots
drwxrwxr-x 4 marwen marwen 4096 Oct 23 21:10 target
drwxrwxr-x 2 marwen marwen 4096 Oct 23 21:25 tests
               
 [~/Documents/Tutorial/DBT_Tutorial/dbt_test_lab2]
 marwen  ls -l macros 
total 4
-rw-rw-r-- 1 marwen marwen 476 Oct 23 21:24 cohort_max_adids_test.sql
               
 [~/Documents/Tutorial/DBT_Tutorial/dbt_test_lab2]
 marwen  


You said:
 [~/Documents/Tutorial/DBT_Tutorial/dbt_test_lab2]
 ✘  marwen  dbt seed
20:33:57  Running with dbt=1.10.13
20:33:57  Registered adapter: postgres=1.9.1
20:33:57  Unable to do partial parsing because saved manifest not found. Starting full parse.
20:33:58  [WARNING][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test accepted_values. Arguments to generic tests
should be nested under the arguments property.
20:33:58  Encountered an error:
Compilation Error in test cohort_max_adids_test_adid_data_cohort (models/schema.yml)
  macro 'dbt_macro__test_cohort_max_adids_test' takes no keyword argument 'column_name'
20:33:58  [WARNING][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the --show-all-deprecations flag. You may also need to
run with --no-partial-parse as some deprecations are only encountered during
parsing.
           
You said:
great we now have the custom test implemented: marwen  dbt seed 07:34:29 Running with dbt=1.10.13 07:34:30 Registered adapter: postgres=1.9.1 07:34:30 Found 1 model, 1 seed, 9 data tests, 448 macros 07:34:30 07:34:30 Concurrency: 1 threads (target='dev') 07:34:30 07:34:31 1 of 1 START seed file public.raw_adid_data .................................... [RUN] 07:34:32 1 of 1 OK loaded seed file public.raw_adid_data ................................ [INSERT 1000 in 1.01s] 07:34:32 07:34:32 Finished running 1 seed in 0 hours 0 minutes and 1.83 seconds (1.83s). 07:34:32 07:34:32 Completed successfully 07:34:32 07:34:32 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1 [~/Documents/Tutorial/DBT_Tutorial/dbt_test_lab2] marwen  dbt run 07:35:39 Running with dbt=1.10.13 07:35:39 Registered adapter: postgres=1.9.1 07:35:40 Found 1 model, 1 seed, 9 data tests, 448 macros 07:35:40 07:35:40 Concurrency: 1 threads (target='dev') 07:35:40 07:35:40 1 of 1 START sql view model public.adid_data ................................... [RUN] 07:35:41 1 of 1 OK created sql view model public.adid_data .............................. [CREATE VIEW in 0.66s] 07:35:41 07:35:41 Finished running 1 view model in 0 hours 0 minutes and 0.90 seconds (0.90s). 07:35:41 07:35:41 Completed successfully 07:35:41 07:35:41 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1 [~/Documents/Tutorial/DBT_Tutorial/dbt_test_lab2] marwen  dbt test 07:35:46 Running with dbt=1.10.13 07:35:46 Registered adapter: postgres=1.9.1 07:35:47 Found 1 model, 1 seed, 9 data tests, 448 macros 07:35:47 07:35:47 Concurrency: 1 threads (target='dev') 07:35:47 07:35:47 1 of 9 START test accepted_values_adid_data_cohort__one__two__three__four__five__six [RUN] 07:35:47 1 of 9 PASS accepted_values_adid_data_cohort__one__two__three__four__five__six . [PASS in 0.07s] 07:35:47 2 of 9 START test cohort_max_adids_test_adid_data_cohort ....................... [RUN] 07:35:47 2 of 9 FAIL 2 cohort_max_adids_test_adid_data_cohort ........................... [FAIL 2 in 0.03s] 07:35:47 3 of 9 START test not_null_adid_data_adid ...................................... [RUN] 07:35:47 3 of 9 PASS not_null_adid_data_adid ............................................ [PASS in 0.04s] 07:35:47 4 of 9 START test not_null_adid_data_city ...................................... [RUN] 07:35:47 4 of 9 PASS not_null_adid_data_city ............................................ [PASS in 0.03s] 07:35:47 5 of 9 START test not_null_adid_data_cohort .................................... [RUN] 07:35:47 5 of 9 PASS not_null_adid_data_cohort .......................................... [PASS in 0.03s] 07:35:47 6 of 9 START test not_null_adid_data_eventdate ................................. [RUN] 07:35:47 6 of 9 PASS not_null_adid_data_eventdate ....................................... [PASS in 0.03s] 07:35:47 7 of 9 START test not_null_adid_data_latitude .................................. [RUN] 07:35:47 7 of 9 PASS not_null_adid_data_latitude ........................................ [PASS in 0.03s] 07:35:47 8 of 9 START test not_null_adid_data_longitude ................................. [RUN] 07:35:47 8 of 9 PASS not_null_adid_data_longitude ....................................... [PASS in 0.03s] 07:35:47 9 of 9 START test unique_adid_data_adid ........................................ [RUN] 07:35:47 9 of 9 PASS unique_adid_data_adid .............................................. [PASS in 0.03s] 07:35:47 07:35:47 Finished running 9 data tests in 0 hours 0 minutes and 0.50 seconds (0.50s). 07:35:47 07:35:47 Completed with 1 error, 0 partial successes, and 0 warnings: 07:35:47 07:35:47 Failure in test cohort_max_adids_test_adid_data_cohort (models/schema.yml) 07:35:47 Got 2 results, configured to fail if != 0 07:35:47 07:35:47 compiled code at target/compiled/dbt_test_lab2/models/schema.yml/cohort_max_adids_test_adid_data_cohort.sql 07:35:47 07:35:47 Done. PASS=8 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=9 let's try to implement a way in the definition of the test to be able to ingest the not validated data into a table in a new schmea as you suggested eralier and then we will pass to the pipeline you mentioned in a way we use airflow with email alerting or any other suggestion you have
You said:
explain again the new test code peace by peace 
You said:
{% test cohort_max_adids_with_monitoring_test(model, column_name) %}

-- Define target schema/table for failed results
{% set dq_schema = 'dq_monitoring' %}
{% set dq_table = 'failed_cohorts' %}

-- Create schema if not exists (Postgres)
{% do run_query("CREATE SCHEMA IF NOT EXISTS " ~ dq_schema) %}

-- Identify failing cohorts
with cohort_counts as (
    select
        {{ column_name }},
        count(distinct adid) as adid_count
    from {{ model }}
    group by {{ column_name }}
)
, failing_cohorts as (
    select *
    from cohort_counts
    where adid_count > 100
)

-- Persist failing cohorts into dq_monitoring.failed_cohorts
{% do run_query(
    "create table if not exists " ~ dq_schema ~ "." ~ dq_table ~ " (cohort text, adid_count int)"
) %}
{% do run_query(
    "insert into " ~ dq_schema ~ "." ~ dq_table ~ "
     select cohort, adid_count from (" ~ sql ~ ") as failing_cohorts"
) %}

-- Return failing rows to dbt
select * from failing_cohorts

{% endtest %}

I run this but i got an error i think it's with the (" ~ sql ~ ")  It didnt get compiled , here is the output of the execution : 

 ✘  marwen  dbt test 
07:55:03  Running with dbt=1.10.13
07:55:03  Registered adapter: postgres=1.9.1
07:55:04  Found 1 seed, 1 model, 9 data tests, 449 macros
07:55:04  
07:55:04  Concurrency: 1 threads (target='dev')
07:55:04  
07:55:04  1 of 9 START test accepted_values_adid_data_cohort__one__two__three__four__five__six  [RUN]
07:55:04  1 of 9 PASS accepted_values_adid_data_cohort__one__two__three__four__five__six . [PASS in 0.08s]
07:55:04  2 of 9 START test cohort_max_adids_with_monitoring_test_adid_data_cohort ....... [RUN]
07:55:04  2 of 9 ERROR cohort_max_adids_with_monitoring_test_adid_data_cohort ............ [ERROR in 0.32s]
07:55:04  3 of 9 START test not_null_adid_data_adid ...................................... [RUN]
07:55:04  3 of 9 PASS not_null_adid_data_adid ............................................ [PASS in 0.04s]
07:55:04  4 of 9 START test not_null_adid_data_city ...................................... [RUN]
07:55:04  4 of 9 PASS not_null_adid_data_city ............................................ [PASS in 0.03s]
07:55:04  5 of 9 START test not_null_adid_data_cohort .................................... [RUN]
07:55:04  5 of 9 PASS not_null_adid_data_cohort .......................................... [PASS in 0.03s]
07:55:04  6 of 9 START test not_null_adid_data_eventdate ................................. [RUN]
07:55:04  6 of 9 PASS not_null_adid_data_eventdate ....................................... [PASS in 0.03s]
07:55:04  7 of 9 START test not_null_adid_data_latitude .................................. [RUN]
07:55:04  7 of 9 PASS not_null_adid_data_latitude ........................................ [PASS in 0.03s]
07:55:04  8 of 9 START test not_null_adid_data_longitude ................................. [RUN]
07:55:04  8 of 9 PASS not_null_adid_data_longitude ....................................... [PASS in 0.03s]
07:55:04  9 of 9 START test unique_adid_data_adid ........................................ [RUN]
07:55:04  9 of 9 PASS unique_adid_data_adid .............................................. [PASS in 0.04s]
07:55:04  
07:55:04  Finished running 9 data tests in 0 hours 0 minutes and 0.78 seconds (0.78s).
07:55:04  
07:55:04  Completed with 1 error, 0 partial successes, and 0 warnings:
07:55:04  
07:55:04  Failure in test cohort_max_adids_with_monitoring_test_adid_data_cohort (models/schema.yml)
07:55:04    Database Error in test cohort_max_adids_with_monitoring_test_adid_data_cohort (models/schema.yml)
  syntax error at or near ")"
  LINE 4:      select cohort, adid_count from (None) as failing_cohort...
                                                   ^
07:55:04  
07:55:04  Done. PASS=8 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=9
You said:
the test worked fine but nothing got created not the schema neither the table of the failling cohordst : 

 marwen  dbt test 
08:02:23  Running with dbt=1.10.13
08:02:23  Registered adapter: postgres=1.9.1
08:02:24  Found 1 seed, 1 model, 9 data tests, 449 macros
08:02:24  
08:02:24  Concurrency: 1 threads (target='dev')
08:02:24  
08:02:24  1 of 9 START test accepted_values_adid_data_cohort__one__two__three__four__five__six  [RUN]
08:02:24  1 of 9 PASS accepted_values_adid_data_cohort__one__two__three__four__five__six . [PASS in 0.07s]
08:02:24  2 of 9 START test cohort_max_adids_with_monitoring_test_adid_data_cohort ....... [RUN]
08:02:24  2 of 9 FAIL 2 cohort_max_adids_with_monitoring_test_adid_data_cohort ........... [FAIL 2 in 0.15s]
08:02:24  3 of 9 START test not_null_adid_data_adid ...................................... [RUN]
08:02:24  3 of 9 PASS not_null_adid_data_adid ............................................ [PASS in 0.03s]
08:02:24  4 of 9 START test not_null_adid_data_city ...................................... [RUN]
08:02:24  4 of 9 PASS not_null_adid_data_city ............................................ [PASS in 0.03s]
08:02:24  5 of 9 START test not_null_adid_data_cohort .................................... [RUN]
08:02:24  5 of 9 PASS not_null_adid_data_cohort .......................................... [PASS in 0.03s]
08:02:24  6 of 9 START test not_null_adid_data_eventdate ................................. [RUN]
08:02:24  6 of 9 PASS not_null_adid_data_eventdate ....................................... [PASS in 0.03s]
08:02:24  7 of 9 START test not_null_adid_data_latitude .................................. [RUN]
08:02:24  7 of 9 PASS not_null_adid_data_latitude ........................................ [PASS in 0.03s]
08:02:24  8 of 9 START test not_null_adid_data_longitude ................................. [RUN]
08:02:24  8 of 9 PASS not_null_adid_data_longitude ....................................... [PASS in 0.03s]
08:02:24  9 of 9 START test unique_adid_data_adid ........................................ [RUN]
08:02:24  9 of 9 PASS unique_adid_data_adid .............................................. [PASS in 0.03s]
08:02:24  
08:02:24  Finished running 9 data tests in 0 hours 0 minutes and 0.60 seconds (0.60s).
08:02:24  
08:02:24  Completed with 1 error, 0 partial successes, and 0 warnings:
08:02:24  
08:02:24  Failure in test cohort_max_adids_with_monitoring_test_adid_data_cohort (models/schema.yml)
08:02:24    Got 2 results, configured to fail if != 0
08:02:24  
08:02:24    compiled code at target/compiled/dbt_test_lab2/models/schema.yml/cohort_max_adids_with_monitoring_test_adid_data_cohort.sql
08:02:24  
08:02:24  Done. PASS=8 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=9

and when inspecting the compiled sql : 



-- Define target schema/table for failed results



-- Create schema if not exists (Postgres)


-- Identify failing cohorts
with cohort_counts as (
    select
        cohort,
        count(distinct adid) as adid_count
    from "dbt_test_db"."public"."adid_data"
    group by cohort
)
, failing_cohorts as (
    select *
    from cohort_counts
    where adid_count > 100
)

-- Create table if not exists


-- Insert failing data directly (no need to use 'sql')


-- Return failing rows to dbt
select * from failing_cohorts

You said:
yes go ahead and then we will go with airflow and email alerting 
You said:
before passing to airflow + Alerting is there any way to make dbt only create tables and store test data onmly for failing tests ??
You said:
you are wrong dude : 

dbt_test_db=# \dt *.*
dbt_test_db=# \dt public_test_results.*
                                          List of relations
       Schema        |                            Name                            | Type  |  Owner   
---------------------+------------------------------------------------------------+-------+----------
 public_test_results | accepted_values_adid_data_384847f1b788ecc89839016bfd8f3db1 | table | postgres
 public_test_results | cohort_max_adids_test_adid_data_cohort                     | table | postgres
 public_test_results | not_null_adid_data_adid                                    | table | postgres
 public_test_results | not_null_adid_data_city                                    | table | postgres
 public_test_results | not_null_adid_data_cohort                                  | table | postgres
 public_test_results | not_null_adid_data_eventdate                               | table | postgres
 public_test_results | not_null_adid_data_latitude                                | table | postgres
 public_test_results | not_null_adid_data_longitude                               | table | postgres
 public_test_results | unique_adid_data_adid                                      | table | postgres
(9 rows)

dbt_test_db=# select count(*) from public_test_results.not_null_adid_data_adid ;
 count 
-------
     0
(1 row)
You said:
i want you to update my dbt_project.yml file with this hook : 

Then add this to your dbt_project.yml hooks (under the top-level config):

on-run-end:
  - "{{ drop_empty_test_tables('public_test_results') }}"

here is the file : 


# Name your project! Project names should contain only lowercase characters
# and underscores. A good package name should reflect your organization's
# name or the intended use of these models
name: 'dbt_test_lab2'
version: '1.0.0'

# This setting configures which "profile" dbt uses for this project.
profile: 'dbt_test_lab2'

# These configurations specify where dbt should look for different types of files.
# The model-paths config, for example, states that models in this project can be
# found in the "models/" directory. You probably won't need to change these!
model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

clean-targets:         # directories to be removed by dbt clean
  - "target"
  - "dbt_packages"


# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt to build all models in the example/
# directory as views. These settings can be overridden in the individual model
# files using the {{ config(...) }} macro.
models:
  dbt_test_lab2:
    # Config indicated by + and applies to all files under models/
    +materialized: view

tests:
  +store_failures: true
  +schema: test_results


you can remove all unnecessary comments 
You said:
it only worked when i updated this section of the dbt_project.tml file : 

tests:
  +store_failures: true
  +schema: test_results

on-run-end:
  - "{{ drop_empty_test_tables('public_test_results') }}"

when marking this : 
  +schema: test_results

dbt will create the schema with the name public_ test_results

so the hook should work on the updated name 
You said:
great now let's move to the pipeline implementation using airflow, dbt and alerting via email for example but i want to dockerize the hole pipeline , lets start, give all the necessary configurations and steps to implement this pipeline in one message please and then we will start debuging and testing the implementation, be aware i am working on my local ubuntu laptop 
You said:
explain to me what's with the docker file and where we are using it in our pipeline to strat then i will share with you result of the implementation
You said:
yes go ahead
You said:
=> => extracting sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1                                                                        0.0s
 => [internal] load build context                                                                                                                                0.9s
 => => transferring context: 90.83kB                                                                                                                             0.0s
 => ERROR [2/7] RUN apt-get update && apt-get install -y     git     python3-pip     libpq-dev     && pip install --upgrade pip                                353.2s
------
 > [2/7] RUN apt-get update && apt-get install -y     git     python3-pip     libpq-dev     && pip install --upgrade pip:
1.625 Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB]
1.788 Get:2 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
1.853 Get:3 https://packages.microsoft.com/debian/12/prod bookworm InRelease [3618 B]
1.887 Get:4 http://deb.debian.org/debian-security bookworm-security InRelease [48.0 kB]
1.950 Get:5 https://download.docker.com/linux/debian bookworm InRelease [46.6 kB]
1.970 Get:6 https://apt.postgresql.org/pub/repos/apt bookworm-pgdg InRelease [107 kB]
1.974 Get:7 http://deb.debian.org/debian bookworm/main amd64 Packages [8791 kB]
2.008 Get:8 https://archive.mariadb.org/mariadb-10.11/repo/debian bookworm InRelease [4628 B]
2.084 Get:9 https://packages.microsoft.com/debian/12/prod bookworm/main amd64 Packages [124 kB]
2.328 Get:10 https://download.docker.com/linux/debian bookworm/stable amd64 Packages [49.4 kB]
2.328 Get:11 https://packages.microsoft.com/debian/12/prod bookworm/main all Packages [573 B]
2.360 Get:12 https://apt.postgresql.org/pub/repos/apt bookworm-pgdg/main arm64 Packages [387 kB]
2.593 Get:13 https://archive.mariadb.org/mariadb-10.11/repo/debian bookworm/main amd64 Packages [40.0 kB]
2.643 Get:14 https://packages.microsoft.com/debian/12/prod bookworm/main arm64 Packages [37.8 kB]
3.319 Get:15 https://apt.postgresql.org/pub/repos/apt bookworm-pgdg/main amd64 Packages [402 kB]
3.530 Get:16 https://archive.mariadb.org/mariadb-10.11/repo/debian bookworm/main arm64 Packages [34.4 kB]
8.983 Get:17 http://deb.debian.org/debian bookworm-updates/main amd64 Packages [6924 B]
8.983 Get:18 http://deb.debian.org/debian-security bookworm-security/main amd64 Packages [283 kB]
9.523 Fetched 10.6 MB in 8s (1287 kB/s)
9.523 Reading package lists...

49.2 Setting up g++-12 (12.2.0-14+deb12u1) ...
349.5 Setting up gcc (4:12.2.0-3) ...
349.9 Setting up libpython3.11-dev:amd64 (3.11.2-6+deb12u6) ...
350.1 Setting up g++ (4:12.2.0-3) ...
350.3 update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode
350.4 Setting up build-essential (12.9) ...
350.6 Setting up libpython3-dev:amd64 (3.11.2-1+b1) ...
350.8 Setting up python3.11-dev (3.11.2-6+deb12u6) ...
351.1 Setting up python3-dev (3.11.2-1+b1) ...
351.3 Processing triggers for libc-bin (2.36-9+deb12u13) ...
351.7 
351.7 You are running pip as root. Please use 'airflow' user to run pip!
351.7 
351.7 See: https://airflow.apache.org/docs/docker-stack/build.html#adding-a-new-pypi-package
351.7 
------
Dockerfile:8
--------------------
   7 |     # Install essential packages and Postgres client
   8 | >>> RUN apt-get update && apt-get install -y \
   9 | >>>     git \
  10 | >>>     python3-pip \
  11 | >>>     libpq-dev \
  12 | >>>     && pip install --upgrade pip
  13 |     
--------------------
ERROR: failed to build: failed to solve: process "/bin/bash -o pipefail -o errexit -o nounset -o nolog -c apt-get update && apt-get install -y     git     python3-pip     libpq-dev     && pip install --upgrade pip" did not complete successfully: exit code: 1
ERROR: Service 'airflow-webserver' failed to build : Build failed
You said:
 => [2/6] RUN apt-get update && apt-get install -y     git     libpq-dev     && apt-get clean     && rm -rf /var/lib/apt/lists/*                                46.6s
 => ERROR [3/6] RUN pip install --no-cache-dir --user dbt-core dbt-postgres psycopg2-binary                                                                      5.5s
------
 > [3/6] RUN pip install --no-cache-dir --user dbt-core dbt-postgres psycopg2-binary:
4.952 
4.952 [notice] A new release of pip is available: 24.2 -> 25.2
4.952 [notice] To update, run: pip install --upgrade pip
4.952 ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.
------
Dockerfile:17
--------------------
  15 |     
  16 |     # Install dbt and Postgres adapter for dbt
  17 | >>> RUN pip install --no-cache-dir --user dbt-core dbt-postgres psycopg2-binary
  18 |     
  19 |     # Copy dbt project and Airflow DAGs into container
--------------------
ERROR: failed to build: failed to solve: process "/bin/bash -o pipefail -o errexit -o nounset -o nolog -c pip install --no-cache-dir --user dbt-core dbt-postgres psycopg2-binary" did not complete successfully: exit code: 1

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/7ul3fvpvtfm2d8fh62s1ygsa5
ERROR: Service 'airflow-webserver' failed to build : Build failed
               
You said:
 marwen  docker-compose up

ERROR: Network "dbt_dq_pipeline_default" needs to be recreated - option "com.docker.network.enable_ipv6" has changed
               
 [~/Documents/Projects/DBT_DQ_pipeline]
 ✘  marwen  docker-compose down
Removing network dbt_dq_pipeline_default
               
 [~/Documents/Projects/DBT_DQ_pipeline]
 marwen  docker-compose up  

Creating network "dbt_dq_pipeline_default" with the default driver
Creating postgres ... error

ERROR: for postgres  Cannot create container for service postgres: Conflict. The container name "/postgres" is already in use by container "ec7336c9de1d411fbf8546ed702515ca74c34a6477899f88578e30ee26846bc6". You have to remove (or rename) that container to be able to reuse that name.

ERROR: for postgres  Cannot create container for service postgres: Conflict. The container name "/postgres" is already in use by container "ec7336c9de1d411fbf8546ed702515ca74c34a6477899f88578e30ee26846bc6". You have to remove (or rename) that container to be able to reuse that name.
ERROR: Encountered errors while bringing up the project.
               
 [~/Documents/Projects/DBT_DQ_pipeline]
 ✘  marwen  docker-compose ps
Name   Command   State   Ports
------------------------------
               
 [~/Documents/Projects/DBT_DQ_pipeline]
 marwen  

You said:
 ✘  marwen  docker-compose down      
WARNING: Found orphan containers (postgres) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.
Removing dbt_postgres ... done
Removing network dbt_dq_pipeline_default
               
 [~/Documents/Projects/DBT_DQ_pipeline]
 marwen  docker-compose up --build

Creating network "dbt_dq_pipeline_default" with the default driver
WARNING: Found orphan containers (postgres) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.
Building airflow-webserver
[+] Building 2.5s (11/11) FINISHED                                                                                                               docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                             0.2s
 => => transferring dockerfile: 593B                                                                                                                             0.0s
 => [internal] load metadata for docker.io/apache/airflow:2.9.3-python3.11                                                                                       0.9s
 => [internal] load .dockerignore                                                                                                                                0.2s
 => => transferring context: 2B                                                                                                                                  0.0s
 => [1/6] FROM docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                                 0.0s
 => [internal] load build context                                                                                                                                0.2s
 => => transferring context: 1.09kB                                                                                                                              0.0s
 => CACHED [2/6] RUN apt-get update && apt-get install -y     git     libpq-dev     && apt-get clean     && rm -rf /var/lib/apt/lists/*                          0.0s
 => CACHED [3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary                                                                            0.0s
 => CACHED [4/6] WORKDIR /opt/airflow                                                                                                                            0.0s
 => CACHED [5/6] COPY dbt_test_lab2 ./dbt_test_lab2                                                                                                              0.0s
 => CACHED [6/6] COPY dags ./dags                                                                                                                                0.0s
 => exporting to image                                                                                                                                           0.1s
 => => exporting layers                                                                                                                                          0.0s
 => => writing image sha256:be1dbd1178c80d60fd844606c9e31c3c55995406ebeae228ace849a274d2c25a                                                                     0.0s
 => => naming to docker.io/library/dbt_dq_pipeline_airflow-webserver                                                                                             0.0s

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/os00r14a824nwfuc6qn7g1oe5

What's next:
    View a summary of image vulnerabilities and recommendations → docker scout quickview 
Building airflow-scheduler
[+] Building 2.3s (11/11) FINISHED                                                                                                               docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                             0.2s
 => => transferring dockerfile: 593B                                                                                                                             0.0s
 => [internal] load metadata for docker.io/apache/airflow:2.9.3-python3.11                                                                                       0.6s
 => [internal] load .dockerignore                                                                                                                                0.2s
 => => transferring context: 2B                                                                                                                                  0.0s
 => [1/6] FROM docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                                 0.0s
 => [internal] load build context                                                                                                                                0.2s
 => => transferring context: 1.09kB                                                                                                                              0.0s
 => CACHED [2/6] RUN apt-get update && apt-get install -y     git     libpq-dev     && apt-get clean     && rm -rf /var/lib/apt/lists/*                          0.0s
 => CACHED [3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary                                                                            0.0s
 => CACHED [4/6] WORKDIR /opt/airflow                                                                                                                            0.0s
 => CACHED [5/6] COPY dbt_test_lab2 ./dbt_test_lab2                                                                                                              0.0s
 => CACHED [6/6] COPY dags ./dags                                                                                                                                0.0s
 => exporting to image                                                                                                                                           0.1s
 => => exporting layers                                                                                                                                          0.0s
 => => writing image sha256:be1dbd1178c80d60fd844606c9e31c3c55995406ebeae228ace849a274d2c25a                                                                     0.0s
 => => naming to docker.io/library/dbt_dq_pipeline_airflow-scheduler                                                                                             0.0s

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/t55jremri7tly4oi92evyjlag

What's next:
    View a summary of image vulnerabilities and recommendations → docker scout quickview 
Creating dbt_postgres ... done
Creating airflow-webserver ... error

ERROR: for airflow-webserver  Cannot create container for service airflow-webserver: Conflict. The container name "/airflow-webserver" is already in use by container "2cbd15c5c178da7ef3d25a74a7567a34ee11bc6ca8e0f76c9b26a1daf4c0f653". You have to remove (or rename) that container to be able to reuse that name.

ERROR: for airflow-webserver  Cannot create container for service airflow-webserver: Conflict. The container name "/airflow-webserver" is already in use by container "2cbd15c5c178da7ef3d25a74a7567a34ee11bc6ca8e0f76c9b26a1daf4c0f653". You have to remove (or rename) that container to be able to reuse that name.
ERROR: Encountered errors while bringing up the project.

version: "3.8"

services:
  dbt_postgres:
    image: postgres:15
    container_name: dbt_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: dbt_test_db
    ports:
      - "5433:5433"
    volumes:
      - dbt_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d dbt_test_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-webserver:
    build: .
    container_name: airflow-webserver
    depends_on:
      - dbt_postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@dbt_postgres/dbt_test_db
      AIRFLOW__CORE__FERNET_KEY: "fernet_key_here"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__RBAC: "true"
      AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp"
      AIRFLOW__EMAIL__SMTP_HOST: smtp.gmail.com
      AIRFLOW__EMAIL__SMTP_PORT: 587
      AIRFLOW__EMAIL__SMTP_USER: your_email@gmail.com
      AIRFLOW__EMAIL__SMTP_PASSWORD: your_app_password
      AIRFLOW__EMAIL__SMTP_STARTTLS: "true"
      AIRFLOW__EMAIL__SMTP_SSL: "false"
      AIRFLOW__EMAIL__FROM_EMAIL: "Airflow <your_email@gmail.com>"
      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow db init &&
               airflow users create -u admin -p admin -r Admin -e admin@example.com -f Admin -l User &&
               airflow webserver"

  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    depends_on:
      - dbt_postgres
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@dbt_postgres/dbt_test_db
      AIRFLOW__CORE__FERNET_KEY: "fernet_key_here"
      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: >
      bash -c "airflow db upgrade &&
               airflow scheduler"

volumes:
  dbt_postgres_data:

let's fix all issues and update our docker compose file 
You said:
marwen  docker-compose up --build

Creating network "dbt_dq_pipeline_default" with the default driver
Creating volume "dbt_dq_pipeline_dbt_postgres_data" with default driver
Building airflow-init
[+] Building 5.0s (11/11) FINISHED                                                                                                               docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                             0.6s
 => => transferring dockerfile: 593B                                                                                                                             0.0s
 => [internal] load metadata for docker.io/apache/airflow:2.9.3-python3.11                                                                                       1.6s
 => [internal] load .dockerignore                                                                                                                                0.4s
 => => transferring context: 2B                                                                                                                                  0.0s
 => [1/6] FROM docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                                 0.0s
 => [internal] load build context                                                                                                                                0.4s
 => => transferring context: 90.83kB                                                                                                                             0.0s
 => CACHED [2/6] RUN apt-get update && apt-get install -y     git     libpq-dev     && apt-get clean     && rm -rf /var/lib/apt/lists/*                          0.0s
 => CACHED [3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary                                                                            0.0s
 => CACHED [4/6] WORKDIR /opt/airflow                                                                                                                            0.0s
 => CACHED [5/6] COPY dbt_test_lab2 ./dbt_test_lab2                                                                                                              0.0s
 => CACHED [6/6] COPY dags ./dags                                                                                                                                0.0s
 => exporting to image                                                                                                                                           0.6s
 => => exporting layers                                                                                                                                          0.0s
 => => writing image sha256:be1dbd1178c80d60fd844606c9e31c3c55995406ebeae228ace849a274d2c25a                                                                     0.2s
 => => naming to docker.io/library/dbt_dq_pipeline_airflow-init                                                                                                  0.1s

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/b0jytmtfw2owpgold4i0d9sb6

What's next:
    View a summary of image vulnerabilities and recommendations → docker scout quickview 
Building airflow-webserver
[+] Building 2.2s (11/11) FINISHED                                                                                                               docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                             0.2s
 => => transferring dockerfile: 593B                                                                                                                             0.0s
 => [internal] load metadata for docker.io/apache/airflow:2.9.3-python3.11                                                                                       0.5s
 => [internal] load .dockerignore                                                                                                                                0.2s
 => => transferring context: 2B                                                                                                                                  0.0s
 => [1/6] FROM docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                                 0.0s
 => [internal] load build context                                                                                                                                0.2s
 => => transferring context: 1.09kB                                                                                                                              0.0s
 => CACHED [2/6] RUN apt-get update && apt-get install -y     git     libpq-dev     && apt-get clean     && rm -rf /var/lib/apt/lists/*                          0.0s
 => CACHED [3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary                                                                            0.0s
 => CACHED [4/6] WORKDIR /opt/airflow                                                                                                                            0.0s
 => CACHED [5/6] COPY dbt_test_lab2 ./dbt_test_lab2                                                                                                              0.0s
 => CACHED [6/6] COPY dags ./dags                                                                                                                                0.0s
 => exporting to image                                                                                                                                           0.1s
 => => exporting layers                                                                                                                                          0.0s
 => => writing image sha256:be1dbd1178c80d60fd844606c9e31c3c55995406ebeae228ace849a274d2c25a                                                                     0.0s
 => => naming to docker.io/library/dbt_dq_pipeline_airflow-webserver                                                                                             0.0s

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/mxc4bv37x1b7rsmkhf1lso1za

What's next:
    View a summary of image vulnerabilities and recommendations → docker scout quickview 
Building airflow-scheduler
[+] Building 2.4s (11/11) FINISHED                                                                                                               docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                             0.3s
 => => transferring dockerfile: 593B                                                                                                                             0.0s
 => [internal] load metadata for docker.io/apache/airflow:2.9.3-python3.11                                                                                       0.7s
 => [internal] load .dockerignore                                                                                                                                0.2s
 => => transferring context: 2B                                                                                                                                  0.0s
 => [1/6] FROM docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                                 0.0s
 => [internal] load build context                                                                                                                                0.2s
 => => transferring context: 1.09kB                                                                                                                              0.0s
 => CACHED [2/6] RUN apt-get update && apt-get install -y     git     libpq-dev     && apt-get clean     && rm -rf /var/lib/apt/lists/*                          0.0s
 => CACHED [3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary                                                                            0.0s
 => CACHED [4/6] WORKDIR /opt/airflow                                                                                                                            0.0s
 => CACHED [5/6] COPY dbt_test_lab2 ./dbt_test_lab2                                                                                                              0.0s
 => CACHED [6/6] COPY dags ./dags                                                                                                                                0.0s
 => exporting to image                                                                                                                                           0.1s
 => => exporting layers                                                                                                                                          0.0s
 => => writing image sha256:be1dbd1178c80d60fd844606c9e31c3c55995406ebeae228ace849a274d2c25a                                                                     0.0s
 => => naming to docker.io/library/dbt_dq_pipeline_airflow-scheduler                                                                                             0.0s

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/ryhwgh2v5v5jsp1lfg2a4nt8r

What's next:
    View a summary of image vulnerabilities and recommendations → docker scout quickview 
Creating dbt_postgres ... done
Creating airflow-init ... done

ERROR: for airflow-webserver  Container "44e293276d6a" exited with code 1.
ERROR: Encountered errors while bringing up the project.
You said:
 marwen  docker logs airflow-webserver

Error response from daemon: No such container: airflow-webserver
You said:
let's start over i deleted all volumes and running containers, images and i will use the last version of Dockerfile and docker-compose files to run my pipeline 
You said:
Creating network "dbt_dq_pipeline_default" with the default driver
Creating volume "dbt_dq_pipeline_postgres_data" with default driver
Pulling postgres (postgres:15)...
15: Pulling from library/postgres
38513bd72563: Pull complete
7cf8703a60ae: Pull complete
1c3c3432cfaa: Pull complete
d5997a3d3e9c: Pull complete
4b34d7a7ab2a: Pull complete
0cdc9eb2a585: Pull complete
f0e812253d9a: Pull complete
70a82bc39e86: Pull complete
b12e04c2e850: Pull complete
bb86ed266eaf: Pull complete
9be150812fa2: Pull complete
9d368c9c6ba6: Pull complete
5a1b3bfd0b0b: Pull complete
8bbe1487ab04: Pull complete
Digest: sha256:424e79b81868f5fc5cf515eaeac69d288692ebcca7db86d98f91b50d4bce64bb
Status: Downloaded newer image for postgres:15
Building airflow-init
[+] Building 469.0s (7/10)                                                                                                                       docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                             0.5s
 => => transferring dockerfile: 493B                                                                                                                             0.0s
 => [internal] load metadata for docker.io/apache/airflow:2.9.3-python3.11                                                                                      11.7s
 => [internal] load .dockerignore                                                                                                                                0.4s
 => => transferring context: 2B                                                                                                                                  0.0s
 => [internal] load build context                                                                                                                                0.8s
 => => transferring context: 1.15MB                                                                                                                              0.0s
 => [1/6] FROM docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                               407.9s
 => => resolve docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                                 0.6s
 => => sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef 1.61kB / 1.61kB                                                                   0.0s
 => => sha256:e65aa7d2eb4026b4dfa96d8f9c0f4257eea4dd829d71f1281c91603ad2fbc77e 4.47kB / 4.47kB                                                                   0.0s
 => => sha256:ee1b31e2b8a3536e0621976827abae9eda321f7e877a1db5f65d9141b9d1d0d7 25.23kB / 25.23kB                                                                 0.0s
 => => sha256:efc2b5ad9eec05befa54239d53feeae3569ccbef689aa5e5dbfc25da6c4df559 29.13MB / 29.13MB                                                                57.2s
 => => sha256:60462faabbc27679d4e1a907afda153c5f2294df5f752f0e706937779faa6d22 3.51MB / 3.51MB                                                                  20.2s
 => => sha256:11f0c4afa075fefa38e75f0bc033f3c60251827dfbecce64bf57bc67fc3718f0 12.87MB / 12.87MB                                                                62.8s
 => => sha256:d8393bf961f1ad054e82d4740c6557c77315c37cb25c19036e329d6194617c13 230B / 230B                                                                      20.9s
 => => sha256:e1558965ee47a72ec3c70592a93ae13e931a2d0f5719ab6c643c4a531c103236 3.21MB / 3.21MB                                                                  52.8s
 => => sha256:06dd6181452eca2d106c969f62da765d8f70ec5f460b8122a32ccaa53e54fc31 1.67kB / 1.67kB                                                                  57.4s
 => => extracting sha256:efc2b5ad9eec05befa54239d53feeae3569ccbef689aa5e5dbfc25da6c4df559                                                                        0.8s
 => => sha256:7fd3231ba6801c37c1388b9e9c7df3f1e6120353e54bd4b87f2efd0cbf48079d 88.48MB / 88.48MB                                                               255.2s
 => => sha256:52c32768aa5c075a9ede555d4dca139c4c343044772ef30faf3764ccda6a8f58 2.31kB / 2.31kB                                                                  58.1s
 => => sha256:1ff940eb2461432da30e17181ccb403ca920e323d0c4c8644a48c36a8510cd88 2.03kB / 2.03kB                                                                  61.6s
 => => extracting sha256:60462faabbc27679d4e1a907afda153c5f2294df5f752f0e706937779faa6d22                                                                        0.2s
 => => sha256:8a5cf0911796ee465fe05d6ab82ff3d1b066412b348afdf71a0480e5c86dc7d7 22.04MB / 22.04MB                                                               126.3s
 => => extracting sha256:11f0c4afa075fefa38e75f0bc033f3c60251827dfbecce64bf57bc67fc3718f0                                                                        0.7s
 => => sha256:75281a0898c789c75ada9980eac0d017fc18d4c1e2a794395dafcb41b3b081f0 247.88MB / 247.88MB                                                             373.5s
 => => extracting sha256:d8393bf961f1ad054e82d4740c6557c77315c37cb25c19036e329d6194617c13                                                                        0.0s
 => => extracting sha256:e1558965ee47a72ec3c70592a93ae13e931a2d0f5719ab6c643c4a531c103236                                                                        0.3s
 => => extracting sha256:06dd6181452eca2d106c969f62da765d8f70ec5f460b8122a32ccaa53e54fc31                                                                        0.0s
 => => sha256:0f476cb3a61ac20066abfcf6606d7c6ee6ca237d32a11d3f03b167458840202b 7.06kB / 7.06kB                                                                 127.6s
 => => sha256:f983fce4639db4ea06123eee1a5aa1cd902c6169f8b4bdae0bc3aab1152f1d00 18.49kB / 18.49kB                                                               129.1s
 => => sha256:804e344abf3027e337b7ee063eee7eaf8f34d57535a7d2e42f033ed9c4c2fac6 3.87kB / 3.87kB                                                                 130.1s
 => => sha256:2704d283b94faf47ce6753badb3d5a0ba0c511424c7ad7ae59b97a31de22056f 490B / 490B                                                                     130.8s
 => => sha256:59d0aded76d43c45f8def9f7c8b27bbdf863175cceebb7c9d63806da2c6503ce 291B / 291B                                                                     131.8s
 => => sha256:98151276bd78060c713bd13dffb5c340704d5b3016d7bbe4c362f009ec1850a4 5.22kB / 5.22kB                                                                 132.4s
 => => sha256:3e7476bb82fa6f2ebe4aa3679b000d95d614e6267abe303a9193e5189aab6bb7 1.06kB / 1.06kB                                                                 133.1s
 => => sha256:1f612cd96dd4dfd667138e2154d8dc6879932f1ca923bbc7227e71448c5b052a 114B / 114B                                                                     133.9s
 => => sha256:52d9ef60a3c3d108bac5c3d35e81f3f36d271e361973a71a601b5fd6ae215629 423B / 423B                                                                     135.7s
 => => sha256:1860cf68ce67e395fc68761f33c68d157aa2682052634411dc7bcb1e003a6545 422B / 422B                                                                     136.2s
 => => sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 32B / 32B                                                                       136.8s
 => => extracting sha256:7fd3231ba6801c37c1388b9e9c7df3f1e6120353e54bd4b87f2efd0cbf48079d                                                                        1.6s
 => => extracting sha256:52c32768aa5c075a9ede555d4dca139c4c343044772ef30faf3764ccda6a8f58                                                                        0.0s
 => => extracting sha256:1ff940eb2461432da30e17181ccb403ca920e323d0c4c8644a48c36a8510cd88                                                                        0.0s
 => => extracting sha256:8a5cf0911796ee465fe05d6ab82ff3d1b066412b348afdf71a0480e5c86dc7d7                                                                        0.4s
 => => extracting sha256:75281a0898c789c75ada9980eac0d017fc18d4c1e2a794395dafcb41b3b081f0                                                                       10.4s
 => => extracting sha256:0f476cb3a61ac20066abfcf6606d7c6ee6ca237d32a11d3f03b167458840202b                                                                        0.0s
 => => extracting sha256:f983fce4639db4ea06123eee1a5aa1cd902c6169f8b4bdae0bc3aab1152f1d00                                                                        0.0s
 => => extracting sha256:804e344abf3027e337b7ee063eee7eaf8f34d57535a7d2e42f033ed9c4c2fac6                                                                        0.0s
 => => extracting sha256:2704d283b94faf47ce6753badb3d5a0ba0c511424c7ad7ae59b97a31de22056f                                                                        0.0s
 => => extracting sha256:59d0aded76d43c45f8def9f7c8b27bbdf863175cceebb7c9d63806da2c6503ce                                                                        0.0s
 => => extracting sha256:98151276bd78060c713bd13dffb5c340704d5b3016d7bbe4c362f009ec1850a4                                                                        0.0s
 => => extracting sha256:3e7476bb82fa6f2ebe4aa3679b000d95d614e6267abe303a9193e5189aab6bb7                                                                        0.0s
 => => extracting sha256:1f612cd96dd4dfd667138e2154d8dc6879932f1ca923bbc7227e71448c5b052a                                                                        0.0s
 => => extracting sha256:52d9ef60a3c3d108bac5c3d35e81f3f36d271e361973a71a601b5fd6ae215629                                                                        0.0s
 => => extracting sha256:1860cf68ce67e395fc68761f33c68d157aa2682052634411dc7bcb1e003a6545                                                                        0.0s
 => => extracting sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1                                                                        0.0s
 => [2/6] RUN apt-get update && apt-get install -y     git     libpq-dev  && apt-get clean  && rm -rf /var/lib/apt/lists/*                                      45.7s
 => ERROR [3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary                                                                             1.4s
------
 > [3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary:
1.166 
1.166 You are running pip as root. Please use 'airflow' user to run pip!
1.166 
1.166 See: https://airflow.apache.org/docs/docker-stack/build.html#adding-a-new-pypi-package
1.166 
------
Dockerfile:13
--------------------
  11 |     
  12 |     # Install dbt and Postgres adapter
  13 | >>> RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary
  14 |     
  15 |     # Set working directory
--------------------
ERROR: failed to build: failed to solve: process "/bin/bash -o pipefail -o errexit -o nounset -o nolog -c pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary" did not complete successfully: exit code: 1

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/la9l1xhc51n2vyj9t85yqse6o
ERROR: Service 'airflow-init' failed to build : Build failed
You said:
 marwen  docker-compose build --no-cache

postgres uses an image, skipping
Building airflow-init
[+] Building 8.9s (9/10)                                                                                                                         docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                             0.6s
 => => transferring dockerfile: 741B                                                                                                                             0.0s
 => [internal] load metadata for docker.io/apache/airflow:2.9.3-python3.11                                                                                       3.1s
 => [internal] load .dockerignore                                                                                                                                0.4s
 => => transferring context: 2B                                                                                                                                  0.0s
 => CANCELED [1/6] FROM docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                        1.4s
 => => resolve docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef                                 0.5s
 => => sha256:e65aa7d2eb4026b4dfa96d8f9c0f4257eea4dd829d71f1281c91603ad2fbc77e 4.47kB / 4.47kB                                                                   0.0s
 => => sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef 1.61kB / 1.61kB                                                                   0.0s
 => => sha256:ee1b31e2b8a3536e0621976827abae9eda321f7e877a1db5f65d9141b9d1d0d7 25.23kB / 25.23kB                                                                 0.0s
 => [internal] load build context                                                                                                                                0.7s
 => => transferring context: 3.12kB                                                                                                                              0.0s
 => CACHED [2/6] RUN apt-get update && apt-get install -y     git     libpq-dev  && apt-get clean  && rm -rf /var/lib/apt/lists/*                                0.0s
 => CACHED [3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary                                                                            0.0s
 => CACHED [4/6] WORKDIR /opt/airflow                                                                                                                            0.0s
 => ERROR [5/6] COPY --chown=airflow:root dbt /opt/airflow/dbt                                                                                                   0.0s
------
 > [5/6] COPY --chown=airflow:root dbt /opt/airflow/dbt:
------
Dockerfile:23
--------------------
  21 |     
  22 |     # Copy dbt project and Airflow DAGs
  23 | >>> COPY --chown=airflow:root dbt /opt/airflow/dbt
  24 |     COPY --chown=airflow:root dags /opt/airflow/dags
  25 |     
--------------------
ERROR: failed to build: failed to solve: failed to compute cache key: failed to calculate checksum of ref c9b6081b-f3e6-4013-8f32-5d5f84407236::o78uj6pnol9d996gjm9f5jll2: "/dbt": not found

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/d0la9p4ed0z25159lqkcmkrd5
ERROR: Service 'airflow-init' failed to build : Build failed
               
You said:
You said:
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:40 +0000] "GET /static/appbuilder/js/jquery-latest.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:40 +0000] "GET /static/appbuilder/js/ab_actions.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:40 +0000] "GET /static/appbuilder/js/bootstrap.min.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:40 +0000] "GET /static/appbuilder/js/ab_filters.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:40 +0000] "GET /static/appbuilder/js/bootstrap-datepicker/bootstrap-datepicker.min.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:40 +0000] "GET /static/appbuilder/js/select2/select2.min.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:40 +0000] "GET /static/dist/moment.0fcb6b41ff6a87cf079e.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:40 +0000] "GET /static/appbuilder/js/ab.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:41 +0000] "GET /static/dist/main.6f9728400381098372e3.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:41 +0000] "GET /static/dist/bootstrap-datetimepicker.min.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:41 +0000] "GET /static/dist/bootstrap3-typeahead.min.js HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:41 +0000] "GET /static/appbuilder/css/webfonts/fa-solid-900.woff2 HTTP/1.1" 200 0 "http://0.0.0.0:8080/static/appbuilder/css/fontawesome/solid.min.css" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:39:42 +0000] "GET /static/pin_32.png HTTP/1.1" 200 0 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | [2025-10-25T18:40:24.407+0000] {override.py:2161} INFO - Login Failed for user: admin
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:40:24 +0000] "POST /login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome HTTP/1.1" 302 281 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:40:24 +0000] "GET /login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome HTTP/1.1" 200 18266 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | [2025-10-25T18:41:11.570+0000] {override.py:2161} INFO - Login Failed for user: admin
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:41:11 +0000] "POST /login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome HTTP/1.1" 302 281 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:41:11 +0000] "GET /login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome HTTP/1.1" 200 18266 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:41:14 +0000] "GET /login/ HTTP/1.1" 200 18079 "http://0.0.0.0:8080/login/?next=http%3A%2F%2F0.0.0.0%3A8080%2Fhome" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow_postgres     | 2025-10-25 18:42:17.124 UTC [62] LOG:  checkpoint starting: time
airflow-webserver    | [2025-10-25T18:42:52.175+0000] {override.py:2161} INFO - Login Failed for user: admin
airflow_postgres     | 2025-10-25 18:42:52.185 UTC [62] LOG:  checkpoint complete: wrote 344 buffers (2.1%); 0 WAL file(s) added, 0 removed, 0 recycled; write=34.526 s, sync=0.156 s, total=35.062 s; sync files=285, longest=0.039 s, average=0.001 s; distance=1894 kB, estimate=1894 kB
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:42:52 +0000] "POST /login/ HTTP/1.1" 302 227 "http://0.0.0.0:8080/login/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
airflow-webserver    | 192.168.65.1 - - [25/Oct/2025:18:42:52 +0000] "GET /login/?next=%2Fhome HTTP/1.1" 200 18266 "http://0.0.0.0:8080/login/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"

okay it worked but i couldnt connect with user admin and psw admin
You said:
 [~/Documents/Projects/DBT_DQ_pipeline]
 marwen  docker-compose down

Removing airflow-scheduler ... done
Removing airflow-init      ... done
Removing airflow-webserver ... done
Removing airflow_postgres  ... done
Removing network dbt_dq_pipeline_default
               
 [~/Documents/Projects/DBT_DQ_pipeline]
 marwen  docker-compose run airflow-webserver airflow db reset

Creating network "dbt_dq_pipeline_default" with the default driver
Creating airflow_postgres ... done
Creating dbt_dq_pipeline_airflow-webserver_run ... done

/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
DB: postgresql+psycopg2://airflow:***@postgres/airflow
This will drop existing tables if they exist. Proceed? (y/n)y
[2025-10-25T18:45:31.434+0000] {db.py:1651} INFO - Dropping tables that exist
[2025-10-25T18:45:32.377+0000] {migration.py:215} INFO - Context impl PostgresqlImpl.
[2025-10-25T18:45:32.377+0000] {migration.py:218} INFO - Will assume transactional DDL.
[2025-10-25T18:45:32.415+0000] {migration.py:215} INFO - Context impl PostgresqlImpl.
[2025-10-25T18:45:32.415+0000] {migration.py:218} INFO - Will assume transactional DDL.
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running stamp_revision  -> 686269002441
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/cryptography/fernet.py", line 34, in __init__
    key = base64.urlsafe_b64decode(key)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/base64.py", line 134, in urlsafe_b64decode
    return b64decode(s)
           ^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/base64.py", line 88, in b64decode
    return binascii.a2b_base64(s, strict_mode=validate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
binascii.Error: Incorrect padding

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/crypto.py", line 86, in get_fernet
    [Fernet(fernet_part.encode("utf-8")) for fernet_part in fernet_key.split(",")]
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/crypto.py", line 86, in <listcomp>
    [Fernet(fernet_part.encode("utf-8")) for fernet_part in fernet_key.split(",")]
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/cryptography/fernet.py", line 36, in __init__
    raise ValueError(
ValueError: Fernet key must be 32 url-safe base64-encoded bytes.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py", line 64, in resetdb
    db.resetdb(skip_init=args.skip_init, use_migration_files=args.use_migration_files)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/db.py", line 1662, in resetdb
    initdb(session=session, use_migration_files=use_migration_files)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/db.py", line 756, in initdb
    create_default_connections(session=session)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/db.py", line 157, in create_default_connections
    Connection(
  File "<string>", line 4, in __init__
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/connection.py", line 175, in __init__
    self.extra = extra
    ^^^^^^^^^^
  File "<string>", line 1, in __set__
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/connection.py", line 376, in set_extra
    fernet = get_fernet()
             ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/crypto.py", line 90, in get_fernet
    raise AirflowException(f"Could not create Fernet object: {value_error}")
airflow.exceptions.AirflowException: Could not create Fernet object: Fernet key must be 32 url-safe base64-encoded bytes.
ERROR: 1
               
 [~/Documents/Projects/DBT_DQ_pipeline]
 ✘  marwen  

You said:
Now all my services are working , let's do a step back and explain to me every piece of informations written in both Dockerfile and docker-compose.yml file

Start by giving a pretty deep description of the choices for this pipeline before explaining docker files code : 


here is my Dockerfile: 

# Use official Apache Airflow image as base
FROM apache/airflow:2.9.3-python3.11

USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    libpq-dev \
 && apt-get clean \
 && rm -rf /var/lib/apt/lists/*

# Switch to airflow user (Airflow images require this for pip installs)
USER airflow

# Install dbt-core and dbt-postgres under the airflow user
RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary

# Set working directory
WORKDIR /opt/airflow
    
# Copy dbt project and Airflow DAGs
COPY --chown=airflow:root dbt_test_lab2 /opt/airflow/dbt
COPY --chown=airflow:root dags /opt/airflow/dags

# Environment variables
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False


and here is my docker-compose file : 

version: "3.8"

services:
  postgres:
    image: postgres:15
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5433:5433"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    build: .
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "fernet_key_here"
    command: bash -c "airflow db init && airflow users create -u admin -p admin -r Admin -e admin@example.com -f Admin -l User"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2

  airflow-webserver:
    build: .
    container_name: airflow-webserver
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "fernet_key_here"
      AIRFLOW__WEBSERVER__RBAC: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp"
      AIRFLOW__EMAIL__SMTP_HOST: smtp.gmail.com
      AIRFLOW__EMAIL__SMTP_PORT: 587
      AIRFLOW__EMAIL__SMTP_USER: your_email@gmail.com
      AIRFLOW__EMAIL__SMTP_PASSWORD: your_app_password
      AIRFLOW__EMAIL__SMTP_STARTTLS: "true"
      AIRFLOW__EMAIL__SMTP_SSL: "false"
      AIRFLOW__EMAIL__FROM_EMAIL: "Airflow <your_email@gmail.com>"
      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: bash -c "airflow webserver"

  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "fernet_key_here"
      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: bash -c "airflow scheduler"

volumes:
  postgres_data:


and also expalain how my containers app read or use all declared varialbles in he .env file

You said:
don't you think that we should update our docker-compose file with the env varobales 

You can then reference them inside docker-compose.yml using the ${VAR_NAME} syntax:

here is the .env file : 

POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=dbt_test_db
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__FERNET_KEY=iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8=
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=True
AIRFLOW__SMTP__SMTP_SSL=False
AIRFLOW__SMTP__SMTP_USER=mejri.marwen00@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=your_app_password
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=mejri.marwen00@gmail.com

the docker-compose file : 

version: "3.8"

services:
  postgres:
    image: postgres:15
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5433:5433"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    build: .
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8"
    command: bash -c "airflow db init && airflow users create -u admin -p admin -r Admin -e admin@example.com -f Admin -l User"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2

  airflow-webserver:
    build: .
    container_name: airflow-webserver
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8"
      AIRFLOW__WEBSERVER__RBAC: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp"
      AIRFLOW__EMAIL__SMTP_HOST: smtp.gmail.com
      AIRFLOW__EMAIL__SMTP_PORT: 587
      AIRFLOW__EMAIL__SMTP_USER: mejri.marwen00@gmail.com
      AIRFLOW__EMAIL__SMTP_PASSWORD: your_app_password
      AIRFLOW__EMAIL__SMTP_STARTTLS: "true"
      AIRFLOW__EMAIL__SMTP_SSL: "false"
      AIRFLOW__EMAIL__FROM_EMAIL: "Airflow <mejri.marwen00@gmail.com>"
      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: bash -c "airflow webserver"

  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8"
      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: bash -c "airflow scheduler"

volumes:
  postgres_data:
You said:
very well now create for me the .gitignore file and let's also talk about where to put the profile attribute (db, ....) for our dbt project dbt_testlab2 inside the container knowing that we already declared this in he deocker compose file : 

      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
You said:
okay but don't you think that we should use anther db for dbt not the db that store airflow metadata so let's call it dbt_test_db with the same user 
You said:
i found it a little bit augly !! DBT can create the db it self and do the modelling and testing even if the db does not exist 
You said:
okay now how to test that the connection between dbt and postgres dbt_test_db works fine inside our containers 
You said:
well i managed to connect to the postgres container and it's workiing and created manually the test_db for dbt but when testing the connection between dbt and postgres db with dbt check here is what i got : 

airflow@55db2404ae6e:/opt/airflow/dbt_test_lab2$ dbt debug
07:40:52  Running with dbt=1.10.13
07:40:52  dbt version: 1.10.13
07:40:52  python version: 3.11.9
07:40:52  python path: /home/airflow/.local/bin/python
07:40:52  os info: Linux-6.10.14-linuxkit-x86_64-with-glibc2.36
07:40:52  Using profiles dir at /opt/airflow/dbt_test_lab2
07:40:52  Using profiles.yml file at /opt/airflow/dbt_test_lab2/profiles.yml
07:40:52  Using dbt_project.yml file at /opt/airflow/dbt_test_lab2/dbt_project.yml
07:40:52  adapter type: postgres
07:40:52  adapter version: 1.9.1
07:40:52  Configuration:
07:40:52    profiles.yml file [OK found and valid]
07:40:52    dbt_project.yml file [OK found and valid]
07:40:52  Required dependencies:
07:40:52   - git [OK found]

07:40:52  Connection:
07:40:52    host: postgres
07:40:52    port: 5433
07:40:52    user: airflow
07:40:52    database: dbt_test_db
07:40:52    schema: public
07:40:52    connect_timeout: 10
07:40:52    role: None
07:40:52    search_path: None
07:40:52    keepalives_idle: 0
07:40:52    sslmode: None
07:40:52    sslcert: None
07:40:52    sslkey: None
07:40:52    sslrootcert: None
07:40:52    application_name: dbt
07:40:52    retries: 1
07:40:52  Registered adapter: postgres=1.9.1
07:40:53    Connection test: [ERROR]

07:40:53  1 check failed:
07:40:53  dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  connection to server at "postgres" (172.18.0.2), port 5433 failed: Connection refused
  	Is the server running on that host and accepting TCP/IP connections?
  

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


airflow@55db2404ae6e:/opt/airflow/dbt_test_lab2$ 

I think the host name is a problem or anything else is causing this 
You said:
You are wrong i already tested the connection with this version of the profile.yml file : 

dbt_test_lab2:
  target: dev
  outputs:
    dev:
      type: postgres
      host: postgres
      user: airflow
      password: airflow
      port: 5433
      dbname: dbt_test_db   # ✅ use your dedicated DB here
      schema: public     # or your custom schema name
      threads: 4
You said:
i didnt do all what you suggested but i think you are right : 
 [~/Documents/Projects/DBT_DQ_pipeline]
 ✘  marwen  docker exec -it airflow-webserver bash

airflow@55db2404ae6e:/opt/airflow$ psql -h postgres -p 5432 -U airflow -d dbt_test_db
Password for user airflow: 
psql (16.3 (Debian 16.3-1.pgdg120+1), server 15.14 (Debian 15.14-1.pgdg13+1))
Type "help" for help.

dbt_test_db=# \l
                                                      List of databases
    Name     |  Owner  | Encoding | Locale Provider |  Collate   |   Ctype    | ICU Locale | ICU Rules |  Access privileges  
-------------+---------+----------+-----------------+------------+------------+------------+-----------+---------------------
 airflow     | airflow | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | 
 dbt_test_db | airflow | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | 
 postgres    | airflow | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | 
 template0   | airflow | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | =c/airflow         +
             |         |          |                 |            |            |            |           | airflow=CTc/airflow
 template1   | airflow | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | =c/airflow         +
             |         |          |                 |            |            |            |           | airflow=CTc/airflow
(5 rows)

dbt_test_db=# \q
airflow@55db2404ae6e:/opt/airflow$ psql -h postgres -p 5433 -U airflow -d dbt_test_db
psql: error: connection to server at "postgres" (172.18.0.2), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

so i want to ask why we do the port binding in docker compose if postgres will always listen to 5432 and only expose 5432 to connect on the DB inside the container 
You said:
great and thank you for these demonstartion : 

airflow@45572c209412:/opt/airflow/dbt_test_lab2$ dbt debug
08:16:28  Running with dbt=1.10.13
08:16:28  dbt version: 1.10.13
08:16:28  python version: 3.11.9
08:16:28  python path: /home/airflow/.local/bin/python
08:16:28  os info: Linux-6.10.14-linuxkit-x86_64-with-glibc2.36
08:16:28  Using profiles dir at /opt/airflow/dbt_test_lab2
08:16:28  Using profiles.yml file at /opt/airflow/dbt_test_lab2/profiles.yml
08:16:28  Using dbt_project.yml file at /opt/airflow/dbt_test_lab2/dbt_project.yml
08:16:28  adapter type: postgres
08:16:28  adapter version: 1.9.1
08:16:28  Configuration:
08:16:28    profiles.yml file [OK found and valid]
08:16:28    dbt_project.yml file [OK found and valid]
08:16:28  Required dependencies:
08:16:28   - git [OK found]

08:16:28  Connection:
08:16:28    host: postgres
08:16:28    port: 5432
08:16:28    user: airflow
08:16:28    database: dbt_test_db
08:16:28    schema: public
08:16:28    connect_timeout: 10
08:16:28    role: None
08:16:28    search_path: None
08:16:28    keepalives_idle: 0
08:16:28    sslmode: None
08:16:28    sslcert: None
08:16:28    sslkey: None
08:16:28    sslrootcert: None
08:16:28    application_name: dbt
08:16:28    retries: 1
08:16:28  Registered adapter: postgres=1.9.1
08:16:29    Connection test: [OK connection ok]

08:16:29  All checks passed!

now let's move to the airflow DAG, explain it to me in each step : 

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.email import EmailOperator
from datetime import datetime
import os

default_args = {
    'owner': 'marwen',
    'email_on_failure': True,
    'email': ['your_email@gmail.com'],
}

with DAG(
    'dbt_dq_pipeline',
    default_args=default_args,
    description='Run dbt tests and send alerts on failure',
    schedule_interval='@daily',
    start_date=datetime(2025, 10, 1),
    catchup=False,
) as dag:

    dbt_seed = BashOperator(
        task_id='dbt_seed',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt seed --profiles-dir .'
    )

    dbt_run = BashOperator(
        task_id='dbt_run',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt run --profiles-dir .'
    )

    dbt_test = BashOperator(
        task_id='dbt_test',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt test --profiles-dir . || echo "DBT TEST FAILED"'
    )

    send_alert = EmailOperator(
        task_id='send_alert',
        to='your_email@gmail.com',
        subject='🚨 DBT Test Failure Alert',
        html_content='DBT tests failed! Please check Airflow logs for details.'
    )

    dbt_seed >> dbt_run >> dbt_test
    dbt_test >> send_alert
You said:
okay here is my DAG to update it : 

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.email import EmailOperator
from datetime import datetime
import os

default_args = {
    'owner': 'marwen',
    'email_on_failure': True,
    'email': ['mejri.marwen00@gmail.com'],
}

with DAG(
    'dbt_dq_pipeline',
    default_args=default_args,
    description='Run dbt tests and send alerts on failure',
    schedule_interval='@daily',
    start_date=datetime(2025, 10, 1),
    catchup=False,
) as dag:

    dbt_seed = BashOperator(
        task_id='dbt_seed',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt seed --profiles-dir .'
    )

    dbt_run = BashOperator(
        task_id='dbt_run',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt run --profiles-dir .'
    )

    dbt_test = BashOperator(
        task_id='dbt_test',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt test --profiles-dir . || echo "DBT TEST FAILED"'
    )

    send_alert = EmailOperator(
        task_id='send_alert',
        to='mejri.marwen00@gmail.com',
        subject='🚨 DBT Test Failure Alert',
        html_content='DBT tests failed! Please check Airflow logs for details.'
    )

    dbt_seed >> dbt_run >> dbt_test
    dbt_test >> send_alert

but first let's test the email is working with airflow , and by the way i didn't created the app from my gmail and never provided the password for this filed : 

AIRFLOW__SMTP__SMTP_PASSWORD=your_app_password
You said:
airflow@5b60acdf8f9a:/opt/airflow$ ipython
Python 3.11.9 (main, Jul 23 2024, 07:22:56) [GCC 12.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 9.6.0 -- An enhanced Interactive Python. Type '?' for help.
Tip: Use F2 or %edit with no arguments to open an empty editor with a temporary file.

In [1]: from airflow.utils.email import send_email_smtp
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.

In [2]: send_email_smtp(to='mejri.marwen00@gmail.com', subject='Airflow SMTP Test', html_content='✅ Your Airflow email setup works!')
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2025-10-26T08:35:38.634+0000] {configuration.py:1053} WARNING - section/key [smtp/smtp_user] not found in config
[2025-10-26T08:35:38.635+0000] {email.py:271} INFO - Email alerting: attempt 1
---------------------------------------------------------------------------
ConnectionRefusedError                    Traceback (most recent call last)
Cell In[2], line 1
----> 1 send_email_smtp(to='mejri.marwen00@gmail.com', subject='Airflow SMTP Test', html_content='✅ Your Airflow email setup works!')

File ~/.local/lib/python3.11/site-packages/airflow/utils/email.py:154, in send_email_smtp(to, subject, html_content, files, dryrun, cc, bcc, mime_subtype, mime_charset, conn_id, from_email, custom_headers, **kwargs)
    139     mail_from = from_email
    141 msg, recipients = build_mime_message(
    142     mail_from=mail_from,
    143     to=to,
   (...)    151     custom_headers=custom_headers,
    152 )
--> 154 send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)

File ~/.local/lib/python3.11/site-packages/airflow/utils/email.py:273, in send_mime_email(e_from, e_to, mime_msg, conn_id, dryrun)
    271 log.info("Email alerting: attempt %s", str(attempt))
    272 try:
--> 273     smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
    274 except smtplib.SMTPServerDisconnected:
    275     if attempt == smtp_retry_limit:

File ~/.local/lib/python3.11/site-packages/airflow/utils/email.py:317, in _get_smtp_connection(host, port, timeout, with_ssl)
    307 """
    308 Return an SMTP connection to the specified host and port, with optional SSL encryption.
    309 
   (...)    314 :return: An SMTP connection to the specified host and port.
    315 """
    316 if not with_ssl:
--> 317     return smtplib.SMTP(host=host, port=port, timeout=timeout)
    318 else:
    319     ssl_context_string = conf.get("email", "SSL_CONTEXT")

File /usr/local/lib/python3.11/smtplib.py:255, in SMTP.__init__(self, host, port, local_hostname, timeout, source_address)
    252 self._auth_challenge_count = 0
    254 if host:
--> 255     (code, msg) = self.connect(host, port)
    256     if code != 220:
    257         self.close()

File /usr/local/lib/python3.11/smtplib.py:341, in SMTP.connect(self, host, port, source_address)
    339     port = self.default_port
    340 sys.audit("smtplib.connect", self, host, port)
--> 341 self.sock = self._get_socket(host, port, self.timeout)
    342 self.file = None
    343 (code, msg) = self.getreply()

File /usr/local/lib/python3.11/smtplib.py:312, in SMTP._get_socket(self, host, port, timeout)
    310 if self.debuglevel > 0:
    311     self._print_debug('connect: to', (host, port), self.source_address)
--> 312 return socket.create_connection((host, port), timeout,
    313                                 self.source_address)

File /usr/local/lib/python3.11/socket.py:851, in create_connection(address, timeout, source_address, all_errors)
    849 try:
    850     if not all_errors:
--> 851         raise exceptions[0]
    852     raise ExceptionGroup("create_connection failed", exceptions)
    853 finally:
    854     # Break explicitly a reference cycle

File /usr/local/lib/python3.11/socket.py:836, in create_connection(address, timeout, source_address, all_errors)
    834 if source_address:
    835     sock.bind(source_address)
--> 836 sock.connect(sa)
    837 # Break explicitly a reference cycle
    838 exceptions.clear()

ConnectionRefusedError: [Errno 111] Connection refused
You said:
but i did that so tell me how i can see the airlfow config file because the nano command didn't work inside the container for the file airflow.cfg and also how can i be able to see the variables from the Airflow UI
You said:
here is what i got 

airflow@93a47e53d77d:/opt/airflow$ cat airflow.cfg | less
airflow@93a47e53d77d:/opt/airflow$ grep smtp /opt/airflow/airflow.cfg
email_backend = airflow.utils.email.send_email_smtp
email_conn_id = smtp_default
[smtp]
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
# Username to authenticate when connecting to smtp server.
# Example: smtp_user = airflow
# smtp_user = 
# Password to authenticate when connecting to smtp server.
# Example: smtp_password = airflow
# smtp_password = 
smtp_port = 25
smtp_mail_from = airflow@example.com
smtp_timeout = 30
smtp_retry_limit = 5
[smtp_provider]

and i still got the same error when trying to test the send email smtp
You said:
let's use these : 

environment:
  - AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
  - AIRFLOW__SMTP__SMTP_PORT=587
  - AIRFLOW__SMTP__SMTP_STARTTLS=True
  - AIRFLOW__SMTP__SMTP_SSL=False
  - AIRFLOW__SMTP__SMTP_USER=mejri.marwen00@gmail.com
  - AIRFLOW__SMTP__SMTP_PASSWORD=your_app_password
  - AIRFLOW__SMTP__SMTP_MAIL_FROM=Airflow <mejri.marwen00@gmail.com>

but i want you to always add to the .env file and then add them to docker-compose as variables : 

.env file : 

POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__FERNET_KEY=iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8=
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=True
AIRFLOW__SMTP__SMTP_SSL=False
AIRFLOW__SMTP__SMTP_USER=mejri.marwen00@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=ddqj bvbf tjoq oiwb
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=mejri.marwen00@gmail.com

docker-compose.yml : 

version: "3.8"

services:
  postgres:
    image: postgres:15
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    build: .
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    command: bash -c "airflow db init && airflow users create -u ${_AIRFLOW_WWW_USER_USERNAME} -p ${_AIRFLOW_WWW_USER_PASSWORD} -r Admin -e admin@example.com -f Admin -l User"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2

  airflow-webserver:
    build: .
    container_name: airflow-webserver
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__WEBSERVER__RBAC: "true"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp"
      AIRFLOW__EMAIL__SMTP_HOST: ${AIRFLOW__SMTP__SMTP_HOST}
      AIRFLOW__EMAIL__SMTP_PORT: ${AIRFLOW__SMTP__SMTP_PORT}
      AIRFLOW__EMAIL__SMTP_USER: ${AIRFLOW__SMTP__SMTP_USER}
      AIRFLOW__EMAIL__SMTP_PASSWORD: ${AIRFLOW__SMTP__SMTP_PASSWORD}
      AIRFLOW__EMAIL__SMTP_STARTTLS: ${AIRFLOW__SMTP__SMTP_STARTTLS}
      AIRFLOW__EMAIL__SMTP_SSL: ${AIRFLOW__SMTP__SMTP_SSL}
      AIRFLOW__EMAIL__FROM_EMAIL: "Airflow <${AIRFLOW__SMTP__SMTP_MAIL_FROM}>"

      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: bash -c "airflow webserver"

  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: bash -c "airflow scheduler"

volumes:
  postgres_data:
You said:
give me the fucking hole docker-compose file with these updates and keep in mind to always doing this not returning only pieces otherwise i will seek assistance from another AI 
You said:
what the fuck is this dude , i told you to update the file docker-compose i gave it to you with the values in the .env file and to fix the airflow  env name not to recreate it as you did and i am really frustrated now you are making me loosing too much time 
You said:
env: 

POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__FERNET_KEY=iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8=
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=True
AIRFLOW__SMTP__SMTP_SSL=False
AIRFLOW__SMTP__SMTP_USER=mejri.marwen00@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=ddqjbvbftjoqoiwb
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=mejri.marwen00@gmail.com

docker-compose: 
version: "3.8"

services:
  postgres:
    image: postgres:15
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    build: .
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    command: bash -c "airflow db init && airflow users create -u ${_AIRFLOW_WWW_USER_USERNAME} -p ${_AIRFLOW_WWW_USER_PASSWORD} -r Admin -e admin@example.com -f Admin -l User"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2

  airflow-webserver:
    build: .
    container_name: airflow-webserver
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__WEBSERVER__RBAC: "true"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp"
      AIRFLOW__EMAIL__SMTP_HOST: ${AIRFLOW__SMTP__SMTP_HOST}
      AIRFLOW__EMAIL__SMTP_PORT: ${AIRFLOW__SMTP__SMTP_PORT}
      AIRFLOW__EMAIL__SMTP_USER: ${AIRFLOW__SMTP__SMTP_USER}
      AIRFLOW__EMAIL__SMTP_PASSWORD: ${AIRFLOW__SMTP__SMTP_PASSWORD}
      AIRFLOW__EMAIL__SMTP_STARTTLS: ${AIRFLOW__SMTP__SMTP_STARTTLS}
      AIRFLOW__EMAIL__SMTP_SSL: ${AIRFLOW__SMTP__SMTP_SSL}
      AIRFLOW__EMAIL__FROM_EMAIL: "Airflow <${AIRFLOW__SMTP__SMTP_MAIL_FROM}>"

      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: bash -c "airflow webserver"

  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2
    command: bash -c "airflow scheduler"

volumes:
  postgres_data:
You said:
rflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | 
airflow-init       | airflow users create command error: the following arguments are required: -e/--email, -f/--firstname, -l/--lastname, -r/--role, -u/--username, see help above.
airflow-init       | Usage: airflow users create [-h] -e EMAIL -f FIRSTNAME -l LASTNAME
airflow-init       |                             [-p PASSWORD] -r ROLE [--use-random-password] -u
airflow-init       |                             USERNAME [-v]
airflow-init       | 
airflow-init       | Create a user
airflow-init       | 
airflow-init       | Options:
airflow-init       |   -h, --help            show this help message and exit
airflow-init       |   -e, --email EMAIL     Email of the user
airflow-init       |   -f, --firstname FIRSTNAME
airflow-init       |                         First name of the user
airflow-init       |   -l, --lastname LASTNAME
airflow-init       |                         Last name of the user
airflow-init       |   -p, --password PASSWORD
airflow-init       |                         Password of the user, required to create a user without --use-random-password
airflow-init       |   -r, --role ROLE       Role of the user. Existing roles include Admin, User, Op, Viewer, and Public
airflow-init       |   --use-random-password
airflow-init       |                         Do not prompt for password. Use random string instead. Required to create a user without --password
airflow-init       |   -u, --username USERNAME
airflow-init       |                         Username of the user
airflow-init       |   -v, --verbose         Make logging output more verbose
airflow-init       | 
airflow-init       | examples:
airflow-init       | To create an user with "Admin" role and username equals to "admin", run:
airflow-init       | 
airflow-init       |     $ airflow users create \
airflow-init       |           --username admin \
airflow-init       |           --firstname FIRST_NAME \
airflow-init       |           --lastname LAST_NAME \
airflow-init       |           --role Admin \
airflow-init       |           --email admin@example.org
airflow-init       | /bin/bash: line 2: -u: command not found
airflow-init       | /bin/bash: line 3: -p: command not found
airflow-init       | /bin/bash: line 4: -r: command not found
airflow-init       | /bin/bash: line 5: -e: command not found
airflow-init       | /bin/bash: line 6: -f: command not found
airflow-init       | /bin/bash: line 7: -l: command not found
You said:
hey the email send test worked fine but when starting my DAG for the first time all 3 first tasks worked but the 4th failed , also i couldn't read the logs as shown in the screenshot , how to fix log issue 
You said:
how to fix this issue ounce for all : 

rflow-init       | DB: postgresql+psycopg2://airflow:***@postgres/airflow
airflow-init       | [2025-10-28T10:13:43.224+0000] {migration.py:215} INFO - Context impl PostgresqlImpl.
airflow-init       | [2025-10-28T10:13:43.225+0000] {migration.py:218} INFO - Will assume transactional DDL.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow_postgres   | 2025-10-28 10:13:43.762 UTC [116] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:13:43.762 UTC [116] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:13:43.754060+00:00'::timestamptz, 'cli_scheduler', 'airflow', '{"host_name": "eb314cca68e5", "full_command": "[''/home/airflow/.local/bin/airflow'', ''scheduler'']"}') RETURNING log.id
airflow-webserver  | ERROR: You need to initialize the database. Please run airflow db init. Make sure the command is run using Airflow version 2.9.3.
airflow-scheduler  | ERROR: You need to initialize the database. Please run airflow db init. Make sure the command is run using Airflow version 2.9.3.
airflow-webserver exited with code 0
airflow-scheduler exited with code 0
airflow_postgres   | 2025-10-28 10:13:51.801 UTC [118] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:13:51.801 UTC [118] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:13:51.792169+00:00'::timestamptz, 'cli_check', 'airflow', '{"host_name": "1e4e028816ab", "full_command": "[''/home/airflow/.local/bin/airflow'', ''db'', ''check'']"}') RETURNING log.id
airflow_postgres   | 2025-10-28 10:13:52.106 UTC [127] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:13:52.106 UTC [127] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:13:52.097806+00:00'::timestamptz, 'cli_check', 'airflow', '{"host_name": "eb314cca68e5", "full_command": "[''/home/airflow/.local/bin/airflow'', ''db'', ''check'']"}') RETURNING log.id
airflow-webserver  | 
airflow-scheduler  | 
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
airflow-init       | INFO  [alembic.runtime.migration] Will assume transactional DDL.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | ERROR: You need to initialize the database. Please run airflow db init. Make sure the command is run using Airflow version 2.9.3.
airflow-init       | INFO  [alembic.runtime.migration] Running stamp_revision  -> 686269002441
airflow-init       | Traceback (most recent call last):
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/cryptography/fernet.py", line 34, in __init__
airflow-init       |     key = base64.urlsafe_b64decode(key)
airflow-init       |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init       |   File "/usr/local/lib/python3.11/base64.py", line 134, in urlsafe_b64decode
airflow-init       |     return b64decode(s)
airflow-init       |            ^^^^^^^^^^^^
airflow-init       |   File "/usr/local/lib/python3.11/base64.py", line 88, in b64decode
airflow-init       |     return binascii.a2b_base64(s, strict_mode=validate)
airflow-init       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init       | binascii.Error: Incorrect padding
airflow-init       | 
airflow-init       | The above exception was the direct cause of the following exception:
airflow-init       | 
airflow-init       | Traceback (most recent call last):
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/crypto.py", line 86, in get_fernet
airflow-init       |     [Fernet(fernet_part.encode("utf-8")) for fernet_part in fernet_key.split(",")]
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/crypto.py", line 86, in <listcomp>
airflow-init       |     [Fernet(fernet_part.encode("utf-8")) for fernet_part in fernet_key.split(",")]
airflow-init       |      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/cryptography/fernet.py", line 36, in __init__
airflow-init       |     raise ValueError(
airflow-init       | ValueError: Fernet key must be 32 url-safe base64-encoded bytes.
airflow-init       | 
airflow-init       | During handling of the above exception, another exception occurred:
airflow-init       | 
airflow-init       | Traceback (most recent call last):
airflow-init       |   File "/home/airflow/.local/bin/airflow", line 8, in <module>
airflow-init       |     sys.exit(main())
airflow-init       |              ^^^^^^
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/__main__.py", line 58, in main
airflow-init       |     args.func(args)
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py", line 49, in command
airflow-init       |     return func(*args, **kwargs)
airflow-init       |            ^^^^^^^^^^^^^^^^^^^^^
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
airflow-init       |     return func(*args, **kwargs)
airflow-init       |            ^^^^^^^^^^^^^^^^^^^^^
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py", line 54, in initdb
airflow-init       |     db.initdb()
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 79, in wrapper
airflow-init       |     return func(*args, session=session, **kwargs)
airflow-init       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/db.py", line 756, in initdb
airflow-init       |     create_default_connections(session=session)
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 76, in wrapper
airflow-init       |     return func(*args, **kwargs)
airflow-init       |            ^^^^^^^^^^^^^^^^^^^^^
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/db.py", line 157, in create_default_connections
airflow-init       |     Connection(
airflow-init       |   File "<string>", line 4, in __init__
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
airflow-init       |     with util.safe_reraise():
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
airflow-init       |     compat.raise_(
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
airflow-init       |     raise exception
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
airflow-init       |     return manager.original_init(*mixed[1:], **kwargs)
airflow-init       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/connection.py", line 175, in __init__
airflow-init       |     self.extra = extra
airflow-init       |     ^^^^^^^^^^
airflow-init       |   File "<string>", line 1, in __set__
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/connection.py", line 376, in set_extra
airflow-init       |     fernet = get_fernet()
airflow-init       |              ^^^^^^^^^^^^
airflow-init       |   File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/crypto.py", line 90, in get_fernet
airflow-init       |     raise AirflowException(f"Could not create Fernet object: {value_error}")
airflow-init       | airflow.exceptions.AirflowException: Could not create Fernet object: Fernet key must be 32 url-safe base64-encoded bytes.
airflow-init       | /bin/bash: line 2: --username: command not found
airflow-init       | /bin/bash: line 3: --password: command not found
airflow-init       | /bin/bash: line 4: --firstname: command not found
airflow-init       | /bin/bash: line 5: --lastname: command not found
airflow-init       | /bin/bash: line 6: --role: command not found
airflow-init       | /bin/bash: line 7: --email: command not found
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py:229 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
You said:
here is my env file : 
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__FERNET_KEY=iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8=
AIRFLOW__WEBSERVER__SECRET_KEY=super_secret_shared_key_123
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=True
AIRFLOW__SMTP__SMTP_SSL=False
AIRFLOW__SMTP__SMTP_USER=mejri.marwen00@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=ddqjbvbftjoqoiwb
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=mejri.marwen00@gmail.com

and here is the output i got when starting my containers 

marwen  docker compose --env-file .env up --build

[+] Building 6.6s (17/17) FINISHED                                                                                                                                    
 => [internal] load local bake definitions                                                                                                                       0.0s
 => => reading from stdin 1.12kB                                                                                                                                 0.0s
 => [airflow-scheduler internal] load build definition from Dockerfile                                                                                           0.2s
 => => transferring dockerfile: 761B                                                                                                                             0.0s
 => [airflow-webserver internal] load metadata for docker.io/apache/airflow:2.9.3-python3.11                                                                     1.0s
 => [airflow-init internal] load .dockerignore                                                                                                                   0.2s
 => => transferring context: 2B                                                                                                                                  0.0s
 => [airflow-scheduler 1/6] FROM docker.io/apache/airflow:2.9.3-python3.11@sha256:cc5fcb91e93e4dfe4fd8b1b53a9155dfa2670fb829891a9658a0f36ac55f67ef               0.0s
 => [airflow-scheduler internal] load build context                                                                                                              0.2s
 => => transferring context: 4.62kB                                                                                                                              0.0s
 => CACHED [airflow-init 2/6] RUN apt-get update && apt-get install -y     git     libpq-dev  && apt-get clean  && rm -rf /var/lib/apt/lists/*                   0.0s
 => CACHED [airflow-init 3/6] RUN pip install --no-cache-dir dbt-core dbt-postgres psycopg2-binary                                                               0.0s
 => CACHED [airflow-init 4/6] WORKDIR /opt/airflow                                                                                                               0.0s
 => CACHED [airflow-init 5/6] COPY --chown=airflow:root dbt_test_lab2 /opt/airflow/dbt_test_lab2                                                                 0.0s
 => CACHED [airflow-init 6/6] COPY --chown=airflow:root dags /opt/airflow/dags                                                                                   0.0s
 => [airflow-scheduler] exporting to image                                                                                                                       0.9s
 => => exporting layers                                                                                                                                          0.0s
 => => writing image sha256:1420bfd3f49bba74438f81c334d7d9fd0515cef3c854a785827a4e7cfb4a53a4                                                                     0.5s
 => => naming to docker.io/library/dbt_dq_pipeline-airflow-scheduler                                                                                             0.4s
 => [airflow-init] exporting to image                                                                                                                            0.7s
 => => exporting layers                                                                                                                                          0.0s
 => => writing image sha256:1420bfd3f49bba74438f81c334d7d9fd0515cef3c854a785827a4e7cfb4a53a4                                                                     0.3s
 => => naming to docker.io/library/dbt_dq_pipeline-airflow-init                                                                                                  0.1s
 => [airflow-webserver] exporting to image                                                                                                                       0.5s
 => => exporting layers                                                                                                                                          0.0s
 => => writing image sha256:1420bfd3f49bba74438f81c334d7d9fd0515cef3c854a785827a4e7cfb4a53a4                                                                     0.1s
 => => naming to docker.io/library/dbt_dq_pipeline-airflow-webserver                                                                                             0.3s
 => [airflow-init] resolving provenance for metadata file                                                                                                        0.3s
 => [airflow-scheduler] resolving provenance for metadata file                                                                                                   0.2s
 => [airflow-webserver] resolving provenance for metadata file                                                                                                   0.0s
[+] Running 9/9
 ✔ airflow-init                            Built                                                                                                                 0.0s 
 ✔ airflow-webserver                       Built                                                                                                                 0.0s 
 ✔ airflow-scheduler                       Built                                                                                                                 0.0s 
 ✔ Network dbt_dq_pipeline_default         Created                                                                                                               0.3s 
 ✔ Volume "dbt_dq_pipeline_postgres_data"  Created                                                                                                               0.2s 
 ✔ Container airflow_postgres              Created                                                                                                               0.7s 
 ✔ Container airflow-init                  Created                                                                                                               1.2s 
 ✔ Container airflow-webserver             Created                                                                                                               1.3s 
 ✔ Container airflow-scheduler             Created                                                                                                               1.2s 
Attaching to airflow-init, airflow-scheduler, airflow-webserver, airflow_postgres
airflow_postgres   | The files belonging to this database system will be owned by user "postgres".
airflow_postgres   | This user must also own the server process.
airflow_postgres   | 
airflow_postgres   | The database cluster will be initialized with locale "en_US.utf8".
airflow_postgres   | The default database encoding has accordingly been set to "UTF8".
airflow_postgres   | The default text search configuration will be set to "english".
airflow_postgres   | 
airflow_postgres   | Data page checksums are disabled.
airflow_postgres   | 
airflow_postgres   | fixing permissions on existing directory /var/lib/postgresql/data ... ok
airflow_postgres   | creating subdirectories ... ok
airflow_postgres   | selecting dynamic shared memory implementation ... posix
airflow_postgres   | selecting default max_connections ... 100
airflow_postgres   | selecting default shared_buffers ... 128MB
airflow_postgres   | selecting default time zone ... Etc/UTC
airflow_postgres   | creating configuration files ... ok
airflow_postgres   | running bootstrap script ... ok
airflow_postgres   | performing post-bootstrap initialization ... ok
airflow_postgres   | syncing data to disk ... ok
airflow_postgres   | 
airflow_postgres   | 
airflow_postgres   | Success. You can now start the database server using:
airflow_postgres   | 
airflow_postgres   |     pg_ctl -D /var/lib/postgresql/data -l logfile start
airflow_postgres   | 
airflow_postgres   | initdb: warning: enabling "trust" authentication for local connections
airflow_postgres   | initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.
airflow_postgres   | waiting for server to start....2025-10-28 10:16:11.639 UTC [48] LOG:  starting PostgreSQL 15.14 (Debian 15.14-1.pgdg13+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
airflow_postgres   | 2025-10-28 10:16:11.681 UTC [48] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
airflow_postgres   | 2025-10-28 10:16:11.796 UTC [51] LOG:  database system was shut down at 2025-10-28 10:16:10 UTC
airflow_postgres   | 2025-10-28 10:16:11.850 UTC [48] LOG:  database system is ready to accept connections
airflow_postgres   |  done
airflow_postgres   | server started
airflow_postgres   | CREATE DATABASE
airflow_postgres   | 
airflow_postgres   | 
airflow_postgres   | /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*
airflow_postgres   | 
airflow_postgres   | waiting for server to shut down...2025-10-28 10:16:12.251 UTC [48] LOG:  received fast shutdown request
airflow_postgres   | .2025-10-28 10:16:12.442 UTC [48] LOG:  aborting any active transactions
airflow_postgres   | 2025-10-28 10:16:12.444 UTC [48] LOG:  background worker "logical replication launcher" (PID 54) exited with exit code 1
airflow_postgres   | 2025-10-28 10:16:12.444 UTC [49] LOG:  shutting down
airflow_postgres   | 2025-10-28 10:16:12.526 UTC [49] LOG:  checkpoint starting: shutdown immediate
airflow_postgres   | .2025-10-28 10:16:13.476 UTC [49] LOG:  checkpoint complete: wrote 922 buffers (5.6%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.269 s, sync=0.331 s, total=1.032 s; sync files=301, longest=0.228 s, average=0.002 s; distance=4239 kB, estimate=4239 kB
airflow_postgres   | 2025-10-28 10:16:13.484 UTC [48] LOG:  database system is shut down
airflow_postgres   |  done
airflow_postgres   | server stopped
airflow_postgres   | 
airflow_postgres   | PostgreSQL init process complete; ready for start up.
airflow_postgres   | 
airflow_postgres   | 2025-10-28 10:16:13.671 UTC [1] LOG:  starting PostgreSQL 15.14 (Debian 15.14-1.pgdg13+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
airflow_postgres   | 2025-10-28 10:16:13.671 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
airflow_postgres   | 2025-10-28 10:16:13.671 UTC [1] LOG:  listening on IPv6 address "::", port 5432
airflow_postgres   | 2025-10-28 10:16:13.784 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
airflow_postgres   | 2025-10-28 10:16:13.861 UTC [64] LOG:  database system was shut down at 2025-10-28 10:16:13 UTC
airflow_postgres   | 2025-10-28 10:16:13.907 UTC [1] LOG:  database system is ready to accept connections
airflow_postgres   | 2025-10-28 10:16:59.882 UTC [101] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:16:59.882 UTC [101] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:16:59.869838+00:00'::timestamptz, 'cli_check', 'airflow', '{"host_name": "1739923d7d1d", "full_command": "[''/home/airflow/.local/bin/airflow'', ''db'', ''check'']"}') RETURNING log.id
airflow_postgres   | 2025-10-28 10:17:00.064 UTC [102] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:17:00.064 UTC [102] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:17:00.056140+00:00'::timestamptz, 'cli_check', 'airflow', '{"host_name": "2bc37266072d", "full_command": "[''/home/airflow/.local/bin/airflow'', ''db'', ''check'']"}') RETURNING log.id
airflow-init       | 
airflow-webserver  | 
airflow_postgres   | 2025-10-28 10:17:00.539 UTC [103] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:17:00.539 UTC [103] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:17:00.529573+00:00'::timestamptz, 'cli_check', 'airflow', '{"host_name": "033ce339c8e3", "full_command": "[''/home/airflow/.local/bin/airflow'', ''db'', ''check'']"}') RETURNING log.id
airflow-scheduler  | 
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py:48 DeprecationWarning: db init is deprecated.  Use db migrate instead to migrate the db and/or airflow connections create-default-connections to create the default connections
airflow-webserver  | [2025-10-28T10:17:02.081+0000] {configuration.py:2090} INFO - Creating new FAB webserver config file in: /opt/airflow/webserver_config.py
airflow_postgres   | 2025-10-28 10:17:02.182 UTC [112] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:17:02.182 UTC [112] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:17:02.174240+00:00'::timestamptz, 'cli_webserver', 'airflow', '{"host_name": "2bc37266072d", "full_command": "[''/home/airflow/.local/bin/airflow'', ''webserver'']"}') RETURNING log.id
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow_postgres   | 2025-10-28 10:17:02.645 UTC [113] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:17:02.645 UTC [113] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:17:02.637283+00:00'::timestamptz, 'cli_scheduler', 'airflow', '{"host_name": "033ce339c8e3", "full_command": "[''/home/airflow/.local/bin/airflow'', ''scheduler'']"}') RETURNING log.id
airflow-init       | DB: postgresql+psycopg2://airflow:***@postgres/airflow
airflow-init       | [2025-10-28T10:17:03.077+0000] {migration.py:215} INFO - Context impl PostgresqlImpl.
airflow-init       | [2025-10-28T10:17:03.078+0000] {migration.py:218} INFO - Will assume transactional DDL.
airflow-webserver  | ERROR: You need to initialize the database. Please run airflow db init. Make sure the command is run using Airflow version 2.9.3.
airflow-scheduler  | ERROR: You need to initialize the database. Please run airflow db init. Make sure the command is run using Airflow version 2.9.3.
airflow-webserver exited with code 0
airflow-scheduler exited with code 0
airflow_postgres   | 2025-10-28 10:17:11.128 UTC [126] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:17:11.128 UTC [126] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:17:11.120685+00:00'::timestamptz, 'cli_check', 'airflow', '{"host_name": "2bc37266072d", "full_command": "[''/home/airflow/.local/bin/airflow'', ''db'', ''check'']"}') RETURNING log.id
airflow_postgres   | 2025-10-28 10:17:11.423 UTC [127] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:17:11.423 UTC [127] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:17:11.414356+00:00'::timestamptz, 'cli_check', 'airflow', '{"host_name": "033ce339c8e3", "full_command": "[''/home/airflow/.local/bin/airflow'', ''db'', ''check'']"}') RETURNING log.id
airflow-webserver  | 
airflow-scheduler  | 
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow_postgres   | 2025-10-28 10:17:13.153 UTC [128] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:17:13.153 UTC [128] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:17:13.144962+00:00'::timestamptz, 'cli_webserver', 'airflow', '{"host_name": "2bc37266072d", "full_command": "[''/home/airflow/.local/bin/airflow'', ''webserver'']"}') RETURNING log.id
airflow-webserver  | ERROR: You need to initialize the database. Please run airflow db init. Make sure the command is run using Airflow version 2.9.3.
airflow_postgres   | 2025-10-28 10:17:13.393 UTC [129] ERROR:  relation "log" does not exist at character 13
airflow_postgres   | 2025-10-28 10:17:13.393 UTC [129] STATEMENT:  INSERT INTO log (dttm, event, owner, extra) VALUES ('2025-10-28T10:17:13.384102+00:00'::timestamptz, 'cli_scheduler', 'airflow', '{"host_name": "033ce339c8e3", "full_command": "[''/home/airflow/.local/bin/airflow'', ''scheduler'']"}') RETURNING log.id
airflow-scheduler  | ERROR: You need to initialize the database. Please run airflow db init. Make sure the command is run using Airflow version 2.9.3.
airflow-init       | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
airflow-init       | INFO  [alembic.runtime.migration] Will assume transactional DDL.
airflow-init       | INFO  [alembic.runtime.migration] Running stamp_revision  -> 686269002441
airflow-webserver exited with code 0
airflow-scheduler exited with code 0
airflow-init       | Initialization done
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | 
airflow-webserver  | 
airflow-init       | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init       | 
airflow-init       | airflow users create command error: the following arguments are required: -e/--email, -f/--firstname, -l/--lastname, -r/--role, -u/--username, see help above.
airflow-init       | Usage: airflow users create [-h] -e EMAIL -f FIRSTNAME -l LASTNAME
airflow-init       |                             [-p PASSWORD] -r ROLE [--use-random-password] -u
airflow-init       |                             USERNAME [-v]
airflow-init       | 
airflow-init       | Create a user
airflow-init       | 
airflow-init       | Options:
airflow-init       |   -h, --help            show this help message and exit
airflow-init       |   -e, --email EMAIL     Email of the user
airflow-init       |   -f, --firstname FIRSTNAME
airflow-init       |                         First name of the user
airflow-init       |   -l, --lastname LASTNAME
airflow-init       |                         Last name of the user
airflow-init       |   -p, --password PASSWORD
airflow-init       |                         Password of the user, required to create a user without --use-random-password
airflow-init       |   -r, --role ROLE       Role of the user. Existing roles include Admin, User, Op, Viewer, and Public
airflow-init       |   --use-random-password
airflow-init       |                         Do not prompt for password. Use random string instead. Required to create a user without --password
airflow-init       |   -u, --username USERNAME
airflow-init       |                         Username of the user
airflow-init       |   -v, --verbose         Make logging output more verbose
airflow-init       | 
airflow-init       | examples:
airflow-init       | To create an user with "Admin" role and username equals to "admin", run:
airflow-init       | 
airflow-init       |     $ airflow users create \
airflow-init       |           --username admin \
airflow-init       |           --firstname FIRST_NAME \
airflow-init       |           --lastname LAST_NAME \
airflow-init       |           --role Admin \
airflow-init       |           --email admin@example.org
airflow-init       | /bin/bash: line 2: --username: command not found
airflow-init       | /bin/bash: line 3: --password: command not found
airflow-init       | /bin/bash: line 4: --firstname: command not found
airflow-init       | /bin/bash: line 5: --lastname: command not found
airflow-init       | /bin/bash: line 6: --role: command not found
airflow-init       | /bin/bash: line 7: --email: command not found
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-init exited with code 127
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py:229 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  |   ____________       _____________
airflow-scheduler  |  ____    |__( )_________  __/__  /________      __
airflow-scheduler  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
airflow-scheduler  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
airflow-scheduler  |  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
airflow-scheduler  | [2025-10-28T10:17:22.955+0000] {task_context_logger.py:63} INFO - Task context logging is enabled
airflow-scheduler  | [2025-10-28T10:17:22.956+0000] {executor_loader.py:235} INFO - Loaded executor: LocalExecutor
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/www/app.py:87 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | [2025-10-28 10:17:23 +0000] [24] [INFO] Starting gunicorn 22.0.0
airflow-scheduler  | [2025-10-28 10:17:23 +0000] [24] [INFO] Listening at: http://[::]:8793 (24)
airflow-scheduler  | [2025-10-28 10:17:23 +0000] [24] [INFO] Using worker: sync
airflow-scheduler  | [2025-10-28T10:17:23.294+0000] {scheduler_job_runner.py:799} INFO - Starting the scheduler
airflow-scheduler  | [2025-10-28T10:17:23.294+0000] {scheduler_job_runner.py:806} INFO - Processing each file at most -1 times
airflow-scheduler  | [2025-10-28 10:17:23 +0000] [29] [INFO] Booting worker with pid: 29
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | [2025-10-28 10:17:23 +0000] [102] [INFO] Booting worker with pid: 102
airflow-scheduler  | [2025-10-28T10:17:23.424+0000] {manager.py:170} INFO - Launched DagFileProcessorManager with pid: 162
airflow-scheduler  | [2025-10-28T10:17:23.426+0000] {scheduler_job_runner.py:1598} INFO - Adopting or resetting orphaned tasks for active dag runs
airflow-scheduler  | [2025-10-28T10:17:23.429+0000] {settings.py:60} INFO - Configured default timezone UTC
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-scheduler  | /home/airflow/.local/lib/python3.11/site-packages/airflow/dag_processing/manager.py:390 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow-webserver  | /home/airflow/.local/lib/python3.11/site-packages/flask_limiter/extension.py:333 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
airflow-webserver  | [2025-10-28T10:17:25.461+0000] {override.py:1454} INFO - Inserted Role: Admin
airflow-webserver  | [2025-10-28T10:17:25.499+0000] {override.py:1454} INFO - Inserted Role: Public
airflow-webserver  | [2025-10-28T10:17:25.502+0000] {override.py:951} WARNING - No user yet created, use flask fab command to do it.
airflow-webserver  | [2025-10-28T10:17:25.713+0000] {override.py:1858} INFO - Created Permission View: can edit on Passwords
airflow-webserver  | [2025-10-28T10:17:25.750+0000] {override.py:1909} INFO - Added Permission can edit on Passwords to role Admin
You said:
to solve the logs issue now we did corrupted the hole infrastcture airflow was working good and the user was created but now with the latest version of docker compose it always fails please give me  the version of docker compose working 
You said:
I think you are wasting my time ypou can read all previous messages and there is no docker comose file that we shared that contains this common airflow i think you are lost 
You said:
go ahead
You said:
no idiot no here is the version : 
version: "3.8" services: postgres: image: postgres:15 container_name: airflow_postgres environment: POSTGRES_USER: ${POSTGRES_USER} POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} POSTGRES_DB: ${POSTGRES_DB} ports: - "5433:5432" volumes: - postgres_data:/var/lib/postgresql/data healthcheck: test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"] interval: 10s timeout: 5s retries: 5 airflow-init: build: . container_name: airflow-init depends_on: postgres: condition: service_healthy environment: AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR} AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN} AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY} command: bash -c "airflow db init && airflow users create -u ${_AIRFLOW_WWW_USER_USERNAME} -p ${_AIRFLOW_WWW_USER_PASSWORD} -r Admin -e admin@example.com -f Admin -l User" volumes: - ./dags:/opt/airflow/dags - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2 airflow-webserver: build: . container_name: airflow-webserver restart: always depends_on: postgres: condition: service_healthy environment: AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR} AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN} AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY} AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES} AIRFLOW__WEBSERVER__RBAC: "true" AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true" AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp" AIRFLOW__EMAIL__SMTP_HOST: ${AIRFLOW__SMTP__SMTP_HOST} AIRFLOW__EMAIL__SMTP_PORT: ${AIRFLOW__SMTP__SMTP_PORT} AIRFLOW__EMAIL__SMTP_USER: ${AIRFLOW__SMTP__SMTP_USER} AIRFLOW__EMAIL__SMTP_PASSWORD: ${AIRFLOW__SMTP__SMTP_PASSWORD} AIRFLOW__EMAIL__SMTP_STARTTLS: ${AIRFLOW__SMTP__SMTP_STARTTLS} AIRFLOW__EMAIL__SMTP_SSL: ${AIRFLOW__SMTP__SMTP_SSL} AIRFLOW__EMAIL__FROM_EMAIL: "Airflow <${AIRFLOW__SMTP__SMTP_MAIL_FROM}>" DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2 ports: - "8080:8080" volumes: - ./dags:/opt/airflow/dags - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2 command: bash -c "airflow webserver" airflow-scheduler: build: . container_name: airflow-scheduler restart: always depends_on: postgres: condition: service_healthy airflow-webserver: condition: service_started environment: AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR} AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN} AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY} DBT_PROFILES_DIR: /opt/airflow/dbt_test_lab2 volumes: - ./dags:/opt/airflow/dags - ./dbt_test_lab2:/opt/airflow/dbt_test_lab2 command: bash -c "airflow scheduler" volumes: postgres_data:
You said:
 [~/Documents/Projects/DBT_DQ_pipeline]
 marwen  docker exec -it airflow-webserver bash
airflow@19964e13ac4e:/opt/airflow$ dbt debug
08:36:19  Running with dbt=1.10.13
08:36:19  dbt version: 1.10.13
08:36:19  python version: 3.11.9
08:36:19  python path: /home/airflow/.local/bin/python
08:36:19  os info: Linux-6.10.14-linuxkit-x86_64-with-glibc2.36
08:36:19  Using profiles dir at /opt/airflow/dbt_test_lab2
08:36:19  Using profiles.yml file at /opt/airflow/dbt_test_lab2/profiles.yml
08:36:19  Using dbt_project.yml file at /opt/airflow/dbt_project.yml
08:36:19  adapter type: postgres
08:36:19  adapter version: 1.9.1
08:36:19  Configuration:
08:36:19    profiles.yml file [OK found and valid]
08:36:19    dbt_project.yml file [ERROR not found]
08:36:19  Required dependencies:
08:36:19   - git [OK found]

08:36:19  Connection:
08:36:19    host: postgres
08:36:19    port: 5432
08:36:19    user: airflow
08:36:19    database: dbt_test_db
08:36:19    schema: public
08:36:19    connect_timeout: 10
08:36:19    role: None
08:36:19    search_path: None
08:36:19    keepalives_idle: 0
08:36:19    sslmode: None
08:36:19    sslcert: None
08:36:19    sslkey: None
08:36:19    sslrootcert: None
08:36:19    application_name: dbt
08:36:19    retries: 1
08:36:19  Registered adapter: postgres=1.9.1
08:36:20    Connection test: [OK connection ok]

08:36:20  1 check failed:
08:36:20  Project loading failed for the following reason:
 project path </opt/airflow/dbt_project.yml> not found

but it worked thank you and i found the log 
here is the logs for the send email task that failed : 

f6985f5fb582
*** Found local files:
***   * /opt/airflow/logs/dag_id=dbt_dq_pipeline/run_id=scheduled__2025-10-28T00:00:00+00:00/task_id=send_alert/attempt=1.log
[2025-10-29, 08:37:38 UTC] {local_task_job_runner.py:120} ▶ Pre task execution logs
[2025-10-29, 08:37:38 UTC] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.11/site-packages/***/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2025-10-29, 08:37:38 UTC] {configuration.py:1053} WARNING - section/key [smtp/smtp_user] not found in config
[2025-10-29, 08:37:38 UTC] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-29, 08:37:38 UTC] {taskinstance.py:441} ▶ Post task execution logs
You said:
I already done that idiot : 

POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__FERNET_KEY=iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8=
AIRFLOW__WEBSERVER__SECRET_KEY=super_secret_shared_key_123
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=True
AIRFLOW__SMTP__SMTP_SSL=False
AIRFLOW__SMTP__SMTP_USER=mejri.marwen00@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=ddqjbvbftjoqoiwb
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=mejri.marwen00@gmail.com

here is the configuration from the UI : 

email	email_backend	airflow.utils.email.send_email_smtp	env var
email	email_conn_id	smtp_default	airflow.cfg
email	default_email_on_retry	True	airflow.cfg
email	default_email_on_failure	True	airflow.cfg
email	ssl_context	default	airflow.cfg
email	smtp_port	587	env var
email	smtp_ssl	False	env var
email	smtp_starttls	True	env var
email	smtp_user	mejri.marwen00@gmail.com	env var
email	smtp_password	ddqjbvbftjoqoiwb	env var
email	smtp_host	smtp.gmail.com	env var
email	from_email	Airflow <mejri.marwen00@gmail.com>	env var
smtp	smtp_host	localhost	airflow.cfg
smtp	smtp_starttls	True	airflow.cfg
smtp	smtp_ssl	False	airflow.cfg
smtp	smtp_port	25	airflow.cfg
smtp	smtp_mail_from	airflow@example.com	airflow.cfg
smtp	smtp_timeout	30	airflow.cfg
smtp	smtp_retry_limit	5	airflow.cfg
You said:
i modified manually the smtp_default but i got this error form the log : 

[2025-10-29T09:03:57.981+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-10-29T09:03:58.007+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dbt_dq_pipeline.send_alert manual__2025-10-29T09:03:39.976802+00:00 [queued]>
[2025-10-29T09:03:58.018+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dbt_dq_pipeline.send_alert manual__2025-10-29T09:03:39.976802+00:00 [queued]>
[2025-10-29T09:03:58.019+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-10-29T09:03:58.059+0000] {taskinstance.py:2330} INFO - Executing <Task(EmailOperator): send_alert> on 2025-10-29 09:03:39.976802+00:00
[2025-10-29T09:03:58.063+0000] {standard_task_runner.py:64} INFO - Started process 572 to run task
[2025-10-29T09:03:58.065+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'dbt_dq_pipeline', 'send_alert', 'manual__2025-10-29T09:03:39.976802+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/dbt_dq_pipeline_dag.py', '--cfg-path', '/tmp/tmpbveabdqm']
[2025-10-29T09:03:58.067+0000] {standard_task_runner.py:91} INFO - Job 9: Subtask send_alert
[2025-10-29T09:03:58.097+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.11/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-10-29T09:03:58.123+0000] {task_command.py:426} INFO - Running <TaskInstance: dbt_dq_pipeline.send_alert manual__2025-10-29T09:03:39.976802+00:00 [running]> on host f6985f5fb582
[2025-10-29T09:03:58.232+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='mejri.marwen00@gmail.com' AIRFLOW_CTX_DAG_OWNER='marwen' AIRFLOW_CTX_DAG_ID='dbt_dq_pipeline' AIRFLOW_CTX_TASK_ID='send_alert' AIRFLOW_CTX_EXECUTION_DATE='2025-10-29T09:03:39.976802+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-29T09:03:39.976802+00:00'
[2025-10-29T09:03:58.233+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-10-29T09:03:58.376+0000] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-29T09:03:58.376+0000] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-29T09:03:58.377+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-10-29T09:03:58.377+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/email.py", line 79, in execute
    send_email(
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2025-10-29T09:03:58.387+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=dbt_dq_pipeline, task_id=send_alert, run_id=manual__2025-10-29T09:03:39.976802+00:00, execution_date=20251029T090339, start_date=20251029T090358, end_date=20251029T090358
[2025-10-29T09:03:58.395+0000] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-29T09:03:58.396+0000] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-29T09:03:58.403+0000] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-29T09:03:58.404+0000] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-29T09:03:58.404+0000] {taskinstance.py:879} ERROR - Failed to send email to: ['mejri.marwen00@gmail.com']
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2676, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2701, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/email.py", line 79, in execute
    send_email(
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2025-10-29T09:03:58.450+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 9 for task send_alert ([Errno 111] Connection refused; 572)
[2025-10-29T09:03:58.493+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 1
[2025-10-29T09:03:58.508+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-10-29T09:03:58.546+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
You said:
 marwen  docker exec -it airflow-webserver bash
apt update && apt install -y telnet
telnet smtp.gmail.com 587

airflow@19964e13ac4e:/opt/airflow$ apt update && apt install -y telnet
Reading package lists... Done
E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)
airflow@19964e13ac4e:/opt/airflow$ apt update && apt install -y telnet
Reading package lists... Done
E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)
airflow@19964e13ac4e:/opt/airflow$ telnet smtp.gmail.com 587
bash: telnet: command not found
airflow@19964e13ac4e:/opt/airflow$ 
You said:
airflow@19964e13ac4e:/opt/airflow$ ipython
import smtplibPython 3.11.9 (main, Jul 23 2024, 07:22:56) [GCC 12.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 9.6.0 -- An enhanced Interactive Python. Type '?' for help.
Tip: You can find how to type a Unicode symbol by back-completing it, eg \Ⅷ<tab> will expand to \ROMAN NUMERAL EIGHT.

In [1]: import smtplib

In [2]: 

In [2]: s=smtplib.SMTP('smtp.gmail.com', 587, timeout=10)

In [3]: print('Connected OK')
Connected OK

In [4]: s.quit()
Out[4]: 
(221,
 b'2.0.0 closing connection ffacd0b85a97d-429952df473sm25174376f8f.42 - gsmtp')

In [5]: 

let's do it inside the airflow UI
You said:
airflow@19964e13ac4e:/opt/airflow$ airflow connections get smtp_default
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
   |              |           |             |              |        |              |              |      |              | is_extra_enc |              |               
id | conn_id      | conn_type | description | host         | schema | login        | password     | port | is_encrypted | rypted       | extra_dejson | get_uri       
===+==============+===========+=============+==============+========+==============+==============+======+==============+==============+==============+===============
60 | smtp_default | email     |             | smtp.gmail.c |        | mejri.marwen | ddqjbvbftjoq | 587  | True         | True         | {'smtp_start | email://mejri.
   |              |           |             | om           |        | 00@gmail.com | oiwb         |      |              |              | tls': True,  | marwen00%40gma
   |              |           |             |              |        |              |              |      |              |              | 'smtp_ssl':  | il.com:ddqjbvb
   |              |           |             |              |        |              |              |      |              |              | False,       | ftjoqoiwb@smtp
   |              |           |             |              |        |              |              |      |              |              | 'from_email' | .gmail.com:587
   |              |           |             |              |        |              |              |      |              |              | :            | /?__extra__=%7
   |              |           |             |              |        |              |              |      |              |              | 'mejri.marwe | B%22smtp_start
   |              |           |             |              |        |              |              |      |              |              | n00@gmail.co | tls%22%3A+true
   |              |           |             |              |        |              |              |      |              |              | m'}          | %2C+%22smtp_ss
   |              |           |             |              |        |              |              |      |              |              |              | l%22%3A+false%
   |              |           |             |              |        |              |              |      |              |              |              | 2C+%22from_ema
   |              |           |             |              |        |              |              |      |              |              |              | il%22%3A+%22me
   |              |           |             |              |        |              |              |      |              |              |              | jri.marwen00%4
   |              |           |             |              |        |              |              |      |              |              |              | 0gmail.com%22%
   |              |           |             |              |        |              |              |      |              |              |              | 7D            
                                                                                                                                                                      
airflow@19964e13ac4e:/opt/airflow$ 

but always same exit code error
You said:
airflow@8ebbaae8293c:/opt/airflow$ nano $AIRFLOW_HOME/airflow.cfg
bash: nano: command not found
airflow@8ebbaae8293c:/opt/airflow$ apt-get update && apt-get install -y nano
Reading package lists... Done
E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)
airflow@8ebbaae8293c:/opt/airflow$ 

bcb9a96a4d7a
*** Found local files:
***   * /opt/airflow/logs/dag_id=test_email_dag/run_id=manual__2025-10-30T09:57:40.128899+00:00/task_id=send_test_email/attempt=1.log
[2025-10-30, 09:57:40 UTC] {local_task_job_runner.py:120} ▼ Pre task execution logs
[2025-10-30, 09:57:40 UTC] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: test_email_dag.send_test_email manual__2025-10-30T09:57:40.128899+00:00 [queued]>
[2025-10-30, 09:57:40 UTC] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: test_email_dag.send_test_email manual__2025-10-30T09:57:40.128899+00:00 [queued]>
[2025-10-30, 09:57:40 UTC] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-10-30, 09:57:41 UTC] {taskinstance.py:2330} INFO - Executing <Task(EmailOperator): send_test_email> on 2025-10-30 09:57:40.128899+00:00
[2025-10-30, 09:57:41 UTC] {standard_task_runner.py:64} INFO - Started process 306 to run task
[2025-10-30, 09:57:41 UTC] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'test_email_dag', 'send_test_email', 'manual__2025-10-30T09:57:40.128899+00:00', '--job-id', '16', '--raw', '--subdir', 'DAGS_FOLDER/test_email_dag.py', '--cfg-path', '/tmp/tmpf84cmlxu']
[2025-10-30, 09:57:41 UTC] {standard_task_runner.py:91} INFO - Job 16: Subtask send_test_email
[2025-10-30, 09:57:41 UTC] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.11/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-10-30, 09:57:41 UTC] {task_command.py:426} INFO - Running <TaskInstance: test_email_dag.send_test_email manual__2025-10-30T09:57:40.128899+00:00 [running]> on host bcb9a96a4d7a
[2025-10-30, 09:57:41 UTC] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='test_email_dag' AIRFLOW_CTX_TASK_ID='send_test_email' AIRFLOW_CTX_EXECUTION_DATE='2025-10-30T09:57:40.128899+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-30T09:57:40.128899+00:00'
[2025-10-30, 09:57:41 UTC] {taskinstance.py:430} ▲▲▲ Log group end
[2025-10-30, 09:57:41 UTC] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-30, 09:57:41 UTC] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-30, 09:57:41 UTC] {taskinstance.py:441} ▼ Post task execution logs
[2025-10-30, 09:57:41 UTC] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/email.py", line 79, in execute
    send_email(
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2025-10-30, 09:57:41 UTC] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=test_email_dag, task_id=send_test_email, run_id=manual__2025-10-30T09:57:40.128899+00:00, execution_date=20251030T095740, start_date=20251030T095740, end_date=20251030T095741
[2025-10-30, 09:57:41 UTC] {standard_task_runner.py:110} ERROR - Failed to execute job 16 for task send_test_email ([Errno 111] Connection refused; 306)
[2025-10-30, 09:57:41 UTC] {local_task_job_runner.py:243} INFO - Task exited with return code 1
[2025-10-30, 09:57:41 UTC] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-10-30, 09:57:41 UTC] {local_task_job_runner.py:222} ▲▲▲ Log group end
You said:
irflow@8ebbaae8293c:/opt/airflow$ ipython
Python 3.11.9 (main, Jul 23 2024, 07:22:56) [GCC 12.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 9.6.0 -- An enhanced Interactive Python. Type '?' for help.
Tip: IPython 9.0+ has hooks to integrate AI/LLM completions.

In [1]: import socket
   ...: try:
   ...:     s = socket.create_connection(("smtp.gmail.com", 587), timeout=10)
   ...:     print("✅ Connection to smtp.gmail.com:587 OK")
   ...: except Exception as e:
   ...:     print("❌ Connection failed:", e)
   ...: 
✅ Connection to smtp.gmail.com:587 OK


hey dude you are wasting my time here think again for a resolution of the issue otherwise we will try slack alerting and be aware of this : 

In [2]: exit
airflow@8ebbaae8293c:/opt/airflow$ telnet smtp.gmail.com 587
bash: telnet: command not found
airflow@8ebbaae8293c:/opt/airflow$ 

i didn't test yet network_mode: "host" because i am not sure if it will work 
You said:
In [3]: import smtplib, ssl
   ...: host = "smtp.gmail.com"
   ...: port = 587
   ...: user = "mejri.marwen00@gmail.com"
   ...: pw = "ddqjbvbftjoqoiwb"   # use app password if using 2FA
   ...: try:
   ...:     s = smtplib.SMTP(host, port, timeout=10)
   ...:     s.set_debuglevel(1)
   ...:     s.ehlo()
   ...:     s.starttls(context=ssl.create_default_context())
   ...:     s.ehlo()
   ...:     s.login(user, pw)
   ...:     s.sendmail(user, [user], "Subject: Airflow test\n\nThis is a test")
   ...:     s.quit()
   ...:     print("✅ SMTP full handshake + login + send succeeded")
   ...: except Exception as e:
   ...:     print("❌ SMTP test failed:", type(e).__name__, e)
   ...: 
send: 'ehlo [172.18.0.3]\r\n'
reply: b'250-smtp.gmail.com at your service, [196.178.201.60]\r\n'
reply: b'250-SIZE 35882577\r\n'
reply: b'250-8BITMIME\r\n'
reply: b'250-STARTTLS\r\n'
reply: b'250-ENHANCEDSTATUSCODES\r\n'
reply: b'250-PIPELINING\r\n'
reply: b'250-CHUNKING\r\n'
reply: b'250 SMTPUTF8\r\n'
reply: retcode (250); Msg: b'smtp.gmail.com at your service, [196.178.201.60]\nSIZE 35882577\n8BITMIME\nSTARTTLS\nENHANCEDSTATUSCODES\nPIPELINING\nCHUNKING\nSMTPUTF8'
send: 'STARTTLS\r\n'
reply: b'220 2.0.0 Ready to start TLS\r\n'
reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'
send: 'ehlo [172.18.0.3]\r\n'
reply: b'250-smtp.gmail.com at your service, [196.178.201.60]\r\n'
reply: b'250-SIZE 35882577\r\n'
reply: b'250-8BITMIME\r\n'
reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\r\n'
reply: b'250-ENHANCEDSTATUSCODES\r\n'
reply: b'250-PIPELINING\r\n'
reply: b'250-CHUNKING\r\n'
reply: b'250 SMTPUTF8\r\n'
reply: retcode (250); Msg: b'smtp.gmail.com at your service, [196.178.201.60]\nSIZE 35882577\n8BITMIME\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\nENHANCEDSTATUSCODES\nPIPELINING\nCHUNKING\nSMTPUTF8'
send: 'AUTH PLAIN AG1lanJpLm1hcndlbjAwQGdtYWlsLmNvbQBkZHFqYnZiZnRqb3FvaXdi\r\n'
reply: b'235 2.7.0 Accepted\r\n'
reply: retcode (235); Msg: b'2.7.0 Accepted'
send: 'mail FROM:<mejri.marwen00@gmail.com> size=39\r\n'
reply: b'250 2.1.0 OK 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp\r\n'
reply: retcode (250); Msg: b'2.1.0 OK 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp'
send: 'rcpt TO:<mejri.marwen00@gmail.com>\r\n'
reply: b'250 2.1.5 OK 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp\r\n'
reply: retcode (250); Msg: b'2.1.5 OK 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp'
send: 'data\r\n'
reply: b'354 Go ahead 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp\r\n'
reply: retcode (354); Msg: b'Go ahead 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp'
data: (354, b'Go ahead 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp')
send: b'Subject: Airflow test\r\n\r\nThis is a test\r\n.\r\n'
reply: b'250 2.0.0 OK  1761819122 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp\r\n'
reply: retcode (250); Msg: b'2.0.0 OK  1761819122 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp'
data: (250, b'2.0.0 OK  1761819122 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp')
send: 'quit\r\n'
reply: b'221 2.0.0 closing connection 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp\r\n'
reply: retcode (221); Msg: b'2.0.0 closing connection 5b1f17b1804b1-477289a5932sm35435485e9.6 - gsmtp'
✅ SMTP full handshake + login + send succeeded

airflow@8ebbaae8293c:/opt/airflow$ airflow connections get smtp_default --verbose
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-10-30T10:30:31.029+0000] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
   |              |           |             |              |        |              |              |      |              | is_extra_enc |              |               
id | conn_id      | conn_type | description | host         | schema | login        | password     | port | is_encrypted | rypted       | extra_dejson | get_uri       
===+==============+===========+=============+==============+========+==============+==============+======+==============+==============+==============+===============
60 | smtp_default | email     |             | smtp.gmail.c |        | mejri.marwen | ddqjbvbftjoq | 587  | True         | True         | {'smtp_start | email://mejri.
   |              |           |             | om           |        | 00@gmail.com | oiwb         |      |              |              | tls': True,  | marwen00%40gma
   |              |           |             |              |        |              |              |      |              |              | 'smtp_ssl':  | il.com:ddqjbvb
   |              |           |             |              |        |              |              |      |              |              | False,       | ftjoqoiwb@smtp
   |              |           |             |              |        |              |              |      |              |              | 'from_email' | .gmail.com:587
   |              |           |             |              |        |              |              |      |              |              | :            | /?__extra__=%7
   |              |           |             |              |        |              |              |      |              |              | 'mejri.marwe | B%22smtp_start
   |              |           |             |              |        |              |              |      |              |              | n00@gmail.co | tls%22%3A+true
   |              |           |             |              |        |              |              |      |              |              | m'}          | %2C+%22smtp_ss
   |              |           |             |              |        |              |              |      |              |              |              | l%22%3A+false%
   |              |           |             |              |        |              |              |      |              |              |              | 2C+%22from_ema
   |              |           |             |              |        |              |              |      |              |              |              | il%22%3A+%22me
   |              |           |             |              |        |              |              |      |              |              |              | jri.marwen00%4
   |              |           |             |              |        |              |              |      |              |              |              | 0gmail.com%22%
   |              |           |             |              |        |              |              |      |              |              |              | 7D            
You said:
airflow@8ebbaae8293c:/opt/airflow$ airflow connections delete smtp_default || true

airflow connections add 'smtp_default' \
  --conn-type 'smtp' \
  --conn-host 'smtp.gmail.com' \
  --conn-login 'mejri.marwen00@gmail.com' \
  --conn-password 'ddqjbvbftjoqoiwb' \
  --conn-port '587' \
  --conn-extra '{"smtp_starttls": true, "smtp_ssl": false, "from_email":"mejri.marwen00@gmail.com"}'
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
Successfully deleted connection with conn_id=smtp_default
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-10-30T10:33:43.539+0000] {providers_manager.py:282} INFO - Optional provider feature disabled when importing 'airflow.providers.google.leveldb.hooks.leveldb.LevelDBHook' from 'apache-airflow-providers-google' package
/home/airflow/.local/lib/python3.11/site-packages/snowflake/sqlalchemy/base.py:1068 SAWarning: The GenericFunction 'flatten' is already registered and is going to be overridden.
Successfully added conn_id=smtp_default : smtp://mejri.marwen00@gmail.com:******@smtp.gmail.com:587
airflow@8ebbaae8293c:/opt/airflow$ airflow connections get smtp_default --verbose
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-10-30T10:33:55.088+0000] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
   |              |           |             |              |        |              |              |      |              | is_extra_enc |              |               
id | conn_id      | conn_type | description | host         | schema | login        | password     | port | is_encrypted | rypted       | extra_dejson | get_uri       
===+==============+===========+=============+==============+========+==============+==============+======+==============+==============+==============+===============
61 | smtp_default | smtp      | None        | smtp.gmail.c | None   | mejri.marwen | ddqjbvbftjoq | 587  | True         | True         | {'smtp_start | smtp://mejri.m
   |              |           |             | om           |        | 00@gmail.com | oiwb         |      |              |              | tls': True,  | arwen00%40gmai
   |              |           |             |              |        |              |              |      |              |              | 'smtp_ssl':  | l.com:ddqjbvbf
   |              |           |             |              |        |              |              |      |              |              | False,       | tjoqoiwb@smtp.
   |              |           |             |              |        |              |              |      |              |              | 'from_email' | gmail.com:587/
   |              |           |             |              |        |              |              |      |              |              | :            | ?__extra__=%7B
   |              |           |             |              |        |              |              |      |              |              | 'mejri.marwe | %22smtp_startt
   |              |           |             |              |        |              |              |      |              |              | n00@gmail.co | ls%22%3A+true%
   |              |           |             |              |        |              |              |      |              |              | m'}          | 2C+%22smtp_ssl
   |              |           |             |              |        |              |              |      |              |              |              | %22%3A+false%2
   |              |           |             |              |        |              |              |      |              |              |              | C+%22from_emai
   |              |           |             |              |        |              |              |      |              |              |              | l%22%3A%22mejr
   |              |           |             |              |        |              |              |      |              |              |              | i.marwen00%40g
   |              |           |             |              |        |              |              |      |              |              |              | mail.com%22%7D
                                                                                                                                                                      
airflow@8ebbaae8293c:/opt/airflow$ 

dude you are wasting my time the host you said was wrong it wasn't just read carefully : 

bcb9a96a4d7a
*** Found local files:
***   * /opt/airflow/logs/dag_id=test_email_dag/run_id=manual__2025-10-30T10:36:34.879705+00:00/task_id=send_test_email/attempt=1.log
[2025-10-30, 10:36:35 UTC] {local_task_job_runner.py:120} ▶ Pre task execution logs
[2025-10-30, 10:36:35 UTC] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-30, 10:36:35 UTC] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-30, 10:36:35 UTC] {taskinstance.py:441} ▼ Post task execution logs
[2025-10-30, 10:36:35 UTC] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/email.py", line 79, in execute
    send_email(
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2025-10-30, 10:36:35 UTC] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=test_email_dag, task_id=send_test_email, run_id=manual__2025-10-30T10:36:34.879705+00:00, execution_date=20251030T103634, start_date=20251030T103635, end_date=20251030T103635
[2025-10-30, 10:36:35 UTC] {standard_task_runner.py:110} ERROR - Failed to execute job 18 for task send_test_email ([Errno 111] Connection refused; 215)
[2025-10-30, 10:36:36 UTC] {local_task_job_runner.py:243} INFO - Task exited with return code 1
[2025-10-30, 10:36:36 UTC] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-10-30, 10:36:36 UTC] {local_task_job_runner.py:222} ▲▲▲ Log group end
You said:
 marwen  docker exec -it airflow-webserver bash
airflow@8ebbaae8293c:/opt/airflow$ apt update && apt install -y telnet
telnet smtp.gmail.com 587
Reading package lists... Done
E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)
bash: telnet: command not found
airflow@8ebbaae8293c:/opt/airflow$ 

You said:
root@8ebbaae8293c:/opt/airflow# telnet smtp.gmail.com 587
Trying 108.177.15.109...
Connected to smtp.gmail.com.
Escape character is '^]'.
220 smtp.gmail.com ESMTP 5b1f17b1804b1-477289b38f7sm32769375e9.9 - gsmtp
You said:
dude you are still wasting my time and i can assure you that the host was correctly set but you keep saying this pfffffffffffff and tell me how can i go back from this : 

Setting up telnet (0.17+2.4-2+deb12u1) ...
root@8ebbaae8293c:/opt/airflow# telnet smtp.gmail.com 587
Trying 108.177.15.109...
Connected to smtp.gmail.com.
Escape character is '^]'.
220 smtp.gmail.com ESMTP 5b1f17b1804b1-477289b38f7sm32769375e9.9 - gsmtp
^C



exit
You said:
i think we are in a dead end : 

bcb9a96a4d7a
*** Found local files:
***   * /opt/airflow/logs/dag_id=test_email_dag/run_id=manual__2025-10-30T10:44:17.530304+00:00/task_id=send_test_email/attempt=1.log
[2025-10-30, 10:44:17 UTC] {local_task_job_runner.py:120} ▶ Pre task execution logs
[2025-10-30, 10:44:18 UTC] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-30, 10:44:18 UTC] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-30, 10:44:18 UTC] {taskinstance.py:441} ▼ Post task execution logs
[2025-10-30, 10:44:18 UTC] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/email.py", line 79, in execute
    send_email(
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2025-10-30, 10:44:18 UTC] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=test_email_dag, task_id=send_test_email, run_id=manual__2025-10-30T10:44:17.530304+00:00, execution_date=20251030T104417, start_date=20251030T104417, end_date=20251030T104418
[2025-10-30, 10:44:18 UTC] {standard_task_runner.py:110} ERROR - Failed to execute job 19 for task send_test_email ([Errno 111] Connection refused; 367)
[2025-10-30, 10:44:18 UTC] {local_task_job_runner.py:243} INFO - Task exited with return code 1
[2025-10-30, 10:44:18 UTC] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-10-30, 10:44:18 UTC] {local_task_job_runner.py:222} ▲▲▲ Log group end

root@8ebbaae8293c:/opt/airflow# airflow connections get smtp_default --verbose
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-10-30T10:44:57.541+0000] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
id | conn_id      | conn_type | description | host           | schema | login                 | password         | port | is_encrypted | is_extra_encrypted | extra_dejson          | get_uri               
===+==============+===========+=============+================+========+=======================+==================+======+==============+====================+=======================+=======================
62 | smtp_default | smtp      | None        | smtp.gmail.com | None   | mejri.marwen00@gmail. | ddqjbvbftjoqoiwb | 587  | True         | True               | {'smtp_starttls':     | smtp://mejri.marwen00%
   |              |           |             |                |        | com                   |                  |      |              |                    | True, 'smtp_ssl':     | 40gmail.com:ddqjbvbftj
   |              |           |             |                |        |                       |                  |      |              |                    | False, 'from_email':  | oqoiwb@smtp.gmail.com:
   |              |           |             |                |        |                       |                  |      |              |                    | 'mejri.marwen00@gmail | 587/?__extra__=%7B%22s
   |              |           |             |                |        |                       |                  |      |              |                    | .com'}                | mtp_starttls%22%3A+tru
   |              |           |             |                |        |                       |                  |      |              |                    |                       | e%2C+%22smtp_ssl%22%3A
   |              |           |             |                |        |                       |                  |      |              |                    |                       | +false%2C+%22from_emai
   |              |           |             |                |        |                       |                  |      |              |                    |                       | l%22%3A+%22mejri.marwe
   |              |           |             |                |        |                       |                  |      |              |                    |                       | n00%40gmail.com%22%7D 
                                                                                                                                                                                                            
root@8ebbaae8293c:/opt/airflow# 
You said:
marwen  docker exec -it airflow-webserver bash

airflow@8ebbaae8293c:/opt/airflow$ ipython
Python 3.11.9 (main, Jul 23 2024, 07:22:56) [GCC 12.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 9.6.0 -- An enhanced Interactive Python. Type '?' for help.
Tip: Use --theme, or the %colors magic to change IPython's themes and colors.

In [1]: import smtplib

In [2]: s = smtplib.SMTP('smtp.gmail.com', 587, timeout=10)

In [3]: s.starttls()
Out[3]: (220, b'2.0.0 Ready to start TLS')

In [4]: print('Connected OK')
Connected OK

In [5]: 

 marwen  docker exec -it airflow-webserver bash

airflow@f4c740727b09:/opt/airflow$ cat /etc/resolv.conf
# Generated by Docker Engine.
# This file can be edited; Docker Engine will not make further changes once it
# has been modified.

nameserver 127.0.0.11
options ndots:0

# Based on host file: '/etc/resolv.conf' (internal resolver)
# ExtServers: [host(192.168.65.7)]
# Overrides: []
# Option ndots from: internal
airflow@f4c740727b09:/opt/airflow$ 


i didn't implemented this : 

network_mode: "host"

because i am not convinvced
You said:
airflow@f4c740727b09:/opt/airflow$ airflow info | grep email
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow@f4c740727b09:/opt/airflow$ airflow config get-value email email_backend
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
airflow.utils.email.send_email_smtp
airflow@f4c740727b09:/opt/airflow$ grep -A3 "\[email\]" $AIRFLOW_HOME/airflow.cfg
[email]
# Configuration email backend and whether to
# send email alerts on retry or failure

airflow@f4c740727b09:/opt/airflow$ 
email	email_backend	airflow.utils.email.send_email_smtp	env var
email	email_conn_id	smtp_default	airflow.cfg
email	default_email_on_retry	True	airflow.cfg
email	default_email_on_failure	True	airflow.cfg
email	ssl_context	default	airflow.cfg
email	smtp_port	587	env var
email	smtp_ssl	False	env var
email	smtp_starttls	True	env var
email	smtp_user	mejri.marwen00@gmail.com	env var
email	smtp_password	ddqjbvbftjoqoiwb	env var
email	smtp_host	smtp.gmail.com	env var
email	from_email	Airflow <mejri.marwen00@gmail.com>	env var
smtp	smtp_host	localhost	airflow.cfg
smtp	smtp_starttls	True	airflow.cfg
smtp	smtp_ssl	False	airflow.cfg
smtp	smtp_port	25	airflow.cfg
smtp	smtp_mail_from	airflow@example.com	airflow.cfg
smtp	smtp_timeout	30	airflow.cfg
smtp	smtp_retry_limit	5	airflow.cfg
You said:
this doesn't work:

airflow webserver restart
airflow scheduler restart

oot@73cfaf33061a:/opt/airflow# airflow webserver restart
airflow scheduler restart
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:815 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:741 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/models/base.py:72 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
Usage: airflow [-h] GROUP_OR_COMMAND ...

Positional Arguments:
  GROUP_OR_COMMAND

    Groups
      config         View configuration
      connections    Manage connections
      dags           Manage DAGs
      db             Database operations
      jobs           Manage jobs
      pools          Manage pools
      providers      Display providers
      roles          Manage roles
      tasks          Manage tasks
      users          Manage users
      variables      Manage variables

    Commands:
      cheat-sheet    Display cheat sheet
      dag-processor  Start a standalone Dag Processor instance
      info           Show information about current Airflow and environment
      kerberos       Start a kerberos ticket renewer
      plugins        Dump information about loaded plugins
      rotate-fernet-key
                     Rotate encrypted connection credentials and variables
      scheduler      Start a scheduler instance
      standalone     Run an all-in-one copy of Airflow
      sync-perm      Update permissions for existing roles and optionally DAGs
      triggerer      Start a triggerer instance
      version        Show the version
      webserver      Start a Airflow webserver instance


and here is the file airlfow.cfg 

 Trigger a DAG
#
# Variable: AIRFLOW__WEBSERVER__REQUIRE_CONFIRMATION_DAG_CHANGE
#
require_confirmation_dag_change = False

[email]
# Configuration email backend and whether to
# send email alerts on retry or failure

# Email backend to use
#
# Variable: AIRFLOW__EMAIL__EMAIL_BACKEND
#
email_backend = airflow.utils.email.send_email_smtp

# Email connection to use
#
# Variable: AIRFLOW__EMAIL__EMAIL_CONN_ID
#
email_conn_id = smtp_default

# Whether email alerts should be sent when a task is retried
#
# Variable: AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_RETRY
#
default_email_on_retry = True

# Whether email alerts should be sent when a task failed

default_email_on_retry = True

# Whether email alerts should be sent when a task failed
#
# Variable: AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_FAILURE
#
default_email_on_failure = True

# File that will be used as the template for Email subject (which will be rendered using Jinja2).
# If not set, Airflow uses a base template.
#
# Example: subject_template = /path/to/my_subject_template_file
#
# Variable: AIRFLOW__EMAIL__SUBJECT_TEMPLATE
#
# subject_template = 

# File that will be used as the template for Email content (which will be rendered using Jinja2).
# If not set, Airflow uses a base template.
#
# Example: html_content_template = /path/to/my_html_content_template_file
#
# Variable: AIRFLOW__EMAIL__HTML_CONTENT_TEMPLATE
#
# html_content_template = 

# Email address that will be used as sender address.
# It can either be raw email or the complete address in a format `Sender Name <sender@email.com>
#
# Example: from_email = Airflow <airflow@example.com>
#
# Variable: AIRFLOW__EMAIL__FROM_EMAIL
#
# from_email =  ssl context to use when using SMTP and IMAP SSL connections. By default, the context is "default"
# which sets it to `ssl.create_default_context() which provides the right balance between
# compatibility and security, it however requires that certificates in your operating system are
# updated and that SMTP/IMAP servers of yours have valid certificates that have corresponding public
# keys installed on your machines. You can switch it to "none" if you want to disable checking
# of the certificates, but it is not recommended as it allows MITM (man-in-the-middle) attacks
# if your infrastructure is not sufficiently secured. It should only be set temporarily while you
# are fixing your certificate configuration. This can be typically done by upgrading to newer
# version of the operating system you run Airflow components on,by upgrading/refreshing proper
# certificates in the OS or by updating certificates for your mail servers.
#
# Example: ssl_context = default
#
# Variable: AIRFLOW__EMAIL__SSL_CONTEXT
#
ssl_context = default

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here

# Specifies the host server address used by Airflow when sending out email notifications via SMTP.
#
# Variable: AIRFLOW__SMTP__SMTP_HOST


# Determines whether to use the STARTTLS command when connecting to the SMTP server.
#
# Variable: AIRFLOW__SMTP__SMTP_STARTTLS


# Determines whether to use an SSL connection when talking to the SMTP server.
#
# Variable: AIRFLOW__SMTP__SMTP_SSL

# Username to authenticate when connecting to smtp server.
#
# Example: smtp_user = airflow
#
# Variable: AIRFLOW__SMTP__SMTP_USER
#
# smtp_user = 

# Password to authenticate when connecting to smtp server.
#
# Example: smtp_password = airflow
#
# Variable: AIRFLOW__SMTP__SMTP_PASSWORD
#
# smtp_password = 

# Defines the port number on which Airflow connects to the SMTP server to send email notifications.
#
# Variable: AIRFLOW__SMTP__SMTP_PORT# Determines the maximum time (in seconds) the Apache Airflow system will wait for a
# connection to the SMTP server to be established.
#
# Variable: AIRFLOW__SMTP__SMTP_TIMEOUT


# Defines the maximum number of times Airflow will attempt to connect to the SMTP server.
#
# Variable: AIRFLOW__SMTP__SMTP_RETRY_LIMIT
#
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_user = mejri.marwen00@gmail.com
smtp_password = ddqjbvbftjoqoiwb
smtp_port = 587
smtp_mail_from = mejri.marwen00@gmail.com
smtp_timeout = 30
smtp_retry_limit = 5

[sentry]
You said:
the file got saved and the configutrattions got changed in the UI :

email	email_backend	airflow.utils.email.send_email_smtp	env var
email	email_conn_id	smtp_default	airflow.cfg
email	default_email_on_retry	True	airflow.cfg
email	default_email_on_failure	True	airflow.cfg
email	ssl_context	default	airflow.cfg
email	smtp_port	587	env var
email	smtp_ssl	False	env var
email	smtp_starttls	True	env var
email	smtp_user	mejri.marwen00@gmail.com	env var
email	smtp_password	ddqjbvbftjoqoiwb	env var
email	smtp_host	smtp.gmail.com	env var
email	from_email	Airflow <mejri.marwen00@gmail.com>	env var
smtp	smtp_host	smtp.gmail.com	airflow.cfg
smtp	smtp_starttls	True	airflow.cfg
smtp	smtp_ssl	False	airflow.cfg
smtp	smtp_port	587	airflow.cfg
smtp	smtp_mail_from	mejri.marwen00@gmail.com	airflow.cfg
smtp	smtp_timeout	30	airflow.cfg
smtp	smtp_retry_limit	5	airflow.cfg
smtp	smtp_user	mejri.marwen00@gmail.com	airflow.cfg
smtp	smtp_password	ddqjbvbftjoqoiwb

but task always failed : 

6a63e7ffade3
*** Found local files:
***   * /opt/airflow/logs/dag_id=dbt_dq_pipeline/run_id=manual__2025-10-30T10:50:08.022576+00:00/task_id=send_alert/attempt=2.log
[2025-10-30, 21:59:32 UTC] {local_task_job_runner.py:120} ▼ Pre task execution logs
[2025-10-30, 21:59:32 UTC] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dbt_dq_pipeline.send_alert manual__2025-10-30T10:50:08.022576+00:00 [queued]>
[2025-10-30, 21:59:32 UTC] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dbt_dq_pipeline.send_alert manual__2025-10-30T10:50:08.022576+00:00 [queued]>
[2025-10-30, 21:59:32 UTC] {taskinstance.py:2306} INFO - Starting attempt 2 of 2
[2025-10-30, 21:59:32 UTC] {taskinstance.py:2330} INFO - Executing <Task(EmailOperator): send_alert> on 2025-10-30 10:50:08.022576+00:00
[2025-10-30, 21:59:32 UTC] {standard_task_runner.py:64} INFO - Started process 220 to run task
[2025-10-30, 21:59:32 UTC] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'dbt_dq_pipeline', 'send_alert', 'manual__2025-10-30T10:50:08.022576+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/dbt_dq_pipeline_dag.py', '--cfg-path', '/tmp/tmposl92olf']
[2025-10-30, 21:59:32 UTC] {standard_task_runner.py:91} INFO - Job 29: Subtask send_alert
[2025-10-30, 21:59:32 UTC] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.11/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-10-30, 21:59:32 UTC] {task_command.py:426} INFO - Running <TaskInstance: dbt_dq_pipeline.send_alert manual__2025-10-30T10:50:08.022576+00:00 [running]> on host 6a63e7ffade3
[2025-10-30, 21:59:32 UTC] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='mejri.marwen00@gmail.com' AIRFLOW_CTX_DAG_OWNER='marwen' AIRFLOW_CTX_DAG_ID='dbt_dq_pipeline' AIRFLOW_CTX_TASK_ID='send_alert' AIRFLOW_CTX_EXECUTION_DATE='2025-10-30T10:50:08.022576+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-30T10:50:08.022576+00:00'
[2025-10-30, 21:59:32 UTC] {taskinstance.py:430} ▲▲▲ Log group end
[2025-10-30, 21:59:32 UTC] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-30, 21:59:32 UTC] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-30, 21:59:32 UTC] {taskinstance.py:441} ▼ Post task execution logs
[2025-10-30, 21:59:32 UTC] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/email.py", line 79, in execute
    send_email(
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2025-10-30, 21:59:32 UTC] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=dbt_dq_pipeline, task_id=send_alert, run_id=manual__2025-10-30T10:50:08.022576+00:00, execution_date=20251030T105008, start_date=20251030T215932, end_date=20251030T215932
[2025-10-30, 21:59:32 UTC] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-30, 21:59:32 UTC] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-30, 21:59:32 UTC] {base.py:84} INFO - Using connection ID 'smtp_default' for task execution.
[2025-10-30, 21:59:32 UTC] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-30, 21:59:32 UTC] {taskinstance.py:879} ERROR - Failed to send email to: ['mejri.marwen00@gmail.com']
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2676, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2701, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/email.py", line 79, in execute
    send_email(
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2025-10-30, 21:59:32 UTC] {standard_task_runner.py:110} ERROR - Failed to execute job 29 for task send_alert ([Errno 111] Connection refused; 220)
[2025-10-30, 21:59:32 UTC] {local_task_job_runner.py:243} INFO - Task exited with return code 1
[2025-10-30, 21:59:32 UTC] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-10-30, 21:59:32 UTC] {local_task_job_runner.py:222} ▲▲▲ Log group end
You said:
root@6a63e7ffade3:/opt/airflow# telnet smtp.gmail.com 587telnet smtp.gmail.com 587
telnet: too many arguments
root@6a63e7ffade3:/opt/airflow# telnet smtp.gmail.com 587
Trying 64.233.184.108...
Connected to smtp.gmail.com.
Escape character is '^]'.
220 smtp.gmail.com ESMTP ffacd0b85a97d-429952ca569sm33363690f8f.12 - gsmtp
)quit
502-5.5.1 Unrecognized command. For more information, go to
502 5.5.1  https://support.google.com/a/answer/3221692 ffacd0b85a97d-429952ca569sm33363690f8f.12 - gsmtp
)
502-5.5.1 Unrecognized command. For more information, go to
502 5.5.1  https://support.google.com/a/answer/3221692 ffacd0b85a97d-429952ca569sm33363690f8f.12 - gsmtp
quit
221 2.0.0 closing connection ffacd0b85a97d-429952ca569sm33363690f8f.12 - gsmtp
You said:
we arleady tested all of these and they are all working fines except the task in the DAG to send the eamil 
You said:
here is my Dag from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.email import EmailOperator
from datetime import datetime
import os

default_args = {
    'owner': 'marwen',
    'email_on_failure': True,
    'email': ['mejri.marwen00@gmail.com'],
}

with DAG(
    'dbt_dq_pipeline',
    default_args=default_args,
    description='Run dbt tests and send alerts on failure',
    schedule_interval='@daily',
    start_date=datetime(2025, 10, 1),
    catchup=False,
) as dag:

    dbt_seed = BashOperator(
        task_id='dbt_seed',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt seed --profiles-dir .'
    )

    dbt_run = BashOperator(
        task_id='dbt_run',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt run --profiles-dir .'
    )

    dbt_test = BashOperator(
        task_id='dbt_test',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt test --profiles-dir . || echo "DBT TEST FAILED"'
    )

    send_alert = EmailOperator(
        task_id='send_alert',
        to='mejri.marwen00@gmail.com',
        subject='🚨 DBT Test Failure Alert',
        html_content='DBT tests failed! Please check Airflow logs for details.'
    )

    dbt_seed >> dbt_run >> dbt_test
    dbt_test >> send_alert

and i want to ask you why we don't use this function send_email_smtp instead the eamiloperator puisque ot's working and sending emails 
You said:
no idiot the test dbt task works fine and even the test fails it was marked as succeeded so i am asking you to change the use of Emailoperator with the send_email_smtp function and make it inside maybe a python operator or whatever because i wasted 2 days looking for a solution for this issue with email operator wihout any advancmenet 
You said:
even though my airflow.cfg is configured : 
smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_user = mejri.marwen00@gmail.com
smtp_password = ddqjbvbftjoqoiwb
smtp_port = 587
smtp_mail_from = mejri.marwen00@gmail.com
smtp_timeout = 30
smtp_retry_limit = 5

# Specifies the host server address used by Airflow when sending out email notifications via SMTP.
#
# Variable: AIRFLOW__SMTP__SMTP_HOST

# Determines whether to use the STARTTLS command when connecting to the SMTP server.
#
# Variable: AIRFLOW__SMTP__SMTP_STARTTLS

# Determines whether to use an SSL connection when talking to the SMTP server.
#
# Variable: AIRFLOW__SMT

and my conn smtp_default was also configured throughout the UI i found the configuration for smtp worng on the UI : 

email	email_backend	airflow.utils.email.send_email_smtp	env var
email	email_conn_id	smtp_default	airflow.cfg
email	default_email_on_retry	True	airflow.cfg
email	default_email_on_failure	True	airflow.cfg
email	ssl_context	default	airflow.cfg
email	smtp_port	587	env var
email	smtp_ssl	False	env var
email	smtp_starttls	True	env var
email	smtp_user	mejri.marwen00@gmail.com	env var
email	smtp_password	ddqjbvbftjoqoiwb	env var
email	smtp_host	smtp.gmail.com	env var
email	from_email	Airflow <mejri.marwen00@gmail.com>	env var
smtp	smtp_host	localhost	airflow.cfg
smtp	smtp_starttls	True	airflow.cfg
smtp	smtp_ssl	False	airflow.cfg
smtp	smtp_port	25	airflow.cfg
smtp	smtp_mail_from	airflow@example.com	airflow.cfg
smtp	smtp_timeout	30	airflow.cfg
smtp	smtp_retry_limit	5

hot to force this 
You said:
I already done that but is there any other solution to force the use of the config i want to use via the function call (params ...)
You said:
let's update this DAG : 

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.email import send_email_smtp
from datetime import datetime

def send_custom_email():
    # Force override your SMTP configuration here
    smtp_config = {
        'smtp_host': 'smtp.gmail.com',
        'smtp_starttls': True,
        'smtp_ssl': False,
        'smtp_user': 'mejri.marwen00@gmail.com',
        'smtp_password': 'ddqjbvbftjoqoiwb',
        'smtp_port': 587,
        'smtp_mail_from': 'mejri.marwen00@gmail.com',
        'smtp_timeout': 30,
        'smtp_retry_limit': 5,
    }

    send_email_smtp(
        to='mejri.marwen00@gmail.com',
        subject='🚨 DBT Test Failure Alert',
        html_content='DBT tests failed! Please check Airflow logs for details.',
        conn_id=None,          # Bypass Airflow connection system
        mime_subtype='mixed',
        files=None,
        dryrun=False,
        **smtp_config           # <— This forces your config to override Airflow’s
    )

default_args = {
    'owner': 'marwen',
    'email_on_failure': False,
}

with DAG(
    'dbt_dq_pipeline_alert',
    default_args=default_args,
    description='Run dbt tests and send alerts on failure',
    schedule_interval='@daily',
    start_date=datetime(2025, 10, 1),
    catchup=False,
) as dag:

    dbt_seed = BashOperator(
        task_id='dbt_seed',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt seed --profiles-dir .'
    )

    dbt_run = BashOperator(
        task_id='dbt_run',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt run --profiles-dir .'
    )

    dbt_test = BashOperator(
        task_id='dbt_test',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt test --profiles-dir . || echo "DBT TEST FAILED"'
    )

    send_alert = PythonOperator(
        task_id='send_custom_email',
        python_callable=send_custom_email
    )

    dbt_seed >> dbt_run >> dbt_test >> send_alert


by changing these values to be loaded from the env file here is my .env file : 

POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__FERNET_KEY=iJE7VTXXU4SHkeziJEl08_gUN3ropIBHSFijAiWkXk8=
AIRFLOW__WEBSERVER__SECRET_KEY=super_secret_shared_key_123
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=True
AIRFLOW__SMTP__SMTP_SSL=False
AIRFLOW__SMTP__SMTP_USER=mejri.marwen00@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=ddqjbvbftjoqoiwb
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=mejri.marwen00@gmail.com

and also let's make the dbt test task fail if the test fails because it went green even though the test failed for one test and here is the output of the dbt test ;*: 

[2025-10-31T09:29:05.882+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-10-31T09:29:05.910+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dbt_dq_pipeline_alert.dbt_test manual__2025-10-31T09:28:41.175056+00:00 [queued]>
[2025-10-31T09:29:05.916+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dbt_dq_pipeline_alert.dbt_test manual__2025-10-31T09:28:41.175056+00:00 [queued]>
[2025-10-31T09:29:05.917+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-10-31T09:29:05.960+0000] {taskinstance.py:2330} INFO - Executing <Task(BashOperator): dbt_test> on 2025-10-31 09:28:41.175056+00:00
[2025-10-31T09:29:05.964+0000] {standard_task_runner.py:64} INFO - Started process 976 to run task
[2025-10-31T09:29:05.967+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'dbt_dq_pipeline_alert', 'dbt_test', 'manual__2025-10-31T09:28:41.175056+00:00', '--job-id', '39', '--raw', '--subdir', 'DAGS_FOLDER/dbt_dq_pipeline_dag_alert.py', '--cfg-path', '/tmp/tmp_a_la3k3']
[2025-10-31T09:29:05.969+0000] {standard_task_runner.py:91} INFO - Job 39: Subtask dbt_test
[2025-10-31T09:29:06.001+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.11/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-10-31T09:29:06.029+0000] {task_command.py:426} INFO - Running <TaskInstance: dbt_dq_pipeline_alert.dbt_test manual__2025-10-31T09:28:41.175056+00:00 [running]> on host 335c1402106b
[2025-10-31T09:29:06.143+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='marwen' AIRFLOW_CTX_DAG_ID='dbt_dq_pipeline_alert' AIRFLOW_CTX_TASK_ID='dbt_test' AIRFLOW_CTX_EXECUTION_DATE='2025-10-31T09:28:41.175056+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-31T09:28:41.175056+00:00'
[2025-10-31T09:29:06.144+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-10-31T09:29:06.154+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-10-31T09:29:06.154+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'cd /opt/***/dbt_test_lab2 && dbt test --profiles-dir . || echo "DBT TEST FAILED"']
[2025-10-31T09:29:06.164+0000] {subprocess.py:86} INFO - Output:
[2025-10-31T09:29:07.519+0000] {subprocess.py:93} INFO - [0m09:29:07  Running with dbt=1.10.13
[2025-10-31T09:29:07.725+0000] {subprocess.py:93} INFO - [0m09:29:07  Registered adapter: postgres=1.9.1
[2025-10-31T09:29:08.048+0000] {subprocess.py:93} INFO - [0m09:29:08  Found 1 model, 1 seed, 9 data tests, 449 macros
[2025-10-31T09:29:08.051+0000] {subprocess.py:93} INFO - [0m09:29:08
[2025-10-31T09:29:08.051+0000] {subprocess.py:93} INFO - [0m09:29:08  Concurrency: 4 threads (target='dev')
[2025-10-31T09:29:08.052+0000] {subprocess.py:93} INFO - [0m09:29:08
[2025-10-31T09:29:08.231+0000] {subprocess.py:93} INFO - [0m09:29:08  1 of 9 START test accepted_values_adid_data_cohort__one__two__three__four__five__six  [RUN]
[2025-10-31T09:29:08.232+0000] {subprocess.py:93} INFO - [0m09:29:08  2 of 9 START test cohort_max_adids_test_adid_data_cohort ....................... [RUN]
[2025-10-31T09:29:08.233+0000] {subprocess.py:93} INFO - [0m09:29:08  3 of 9 START test not_null_adid_data_adid ...................................... [RUN]
[2025-10-31T09:29:08.234+0000] {subprocess.py:93} INFO - [0m09:29:08  4 of 9 START test not_null_adid_data_city ...................................... [RUN]
[2025-10-31T09:29:08.536+0000] {subprocess.py:93} INFO - [0m09:29:08  1 of 9 PASS accepted_values_adid_data_cohort__one__two__three__four__five__six . [[32mPASS[0m in 0.30s]
[2025-10-31T09:29:08.545+0000] {subprocess.py:93} INFO - [0m09:29:08  5 of 9 START test not_null_adid_data_cohort .................................... [RUN]
[2025-10-31T09:29:08.584+0000] {subprocess.py:93} INFO - [0m09:29:08  4 of 9 PASS not_null_adid_data_city ............................................ [[32mPASS[0m in 0.35s]
[2025-10-31T09:29:08.586+0000] {subprocess.py:93} INFO - [0m09:29:08  6 of 9 START test not_null_adid_data_eventdate ................................. [RUN]
[2025-10-31T09:29:08.628+0000] {subprocess.py:93} INFO - [0m09:29:08  3 of 9 PASS not_null_adid_data_adid ............................................ [[32mPASS[0m in 0.39s]
[2025-10-31T09:29:08.629+0000] {subprocess.py:93} INFO - [0m09:29:08  2 of 9 FAIL 2 cohort_max_adids_test_adid_data_cohort ........................... [[31mFAIL 2[0m in 0.39s]
[2025-10-31T09:29:08.633+0000] {subprocess.py:93} INFO - [0m09:29:08  7 of 9 START test not_null_adid_data_latitude .................................. [RUN]
[2025-10-31T09:29:08.634+0000] {subprocess.py:93} INFO - [0m09:29:08  8 of 9 START test not_null_adid_data_longitude ................................. [RUN]
[2025-10-31T09:29:08.710+0000] {subprocess.py:93} INFO - [0m09:29:08  5 of 9 PASS not_null_adid_data_cohort .......................................... [[32mPASS[0m in 0.16s]
[2025-10-31T09:29:08.712+0000] {subprocess.py:93} INFO - [0m09:29:08  9 of 9 START test unique_adid_data_adid ........................................ [RUN]
[2025-10-31T09:29:08.787+0000] {subprocess.py:93} INFO - [0m09:29:08  6 of 9 PASS not_null_adid_data_eventdate ....................................... [[32mPASS[0m in 0.20s]
[2025-10-31T09:29:08.860+0000] {subprocess.py:93} INFO - [0m09:29:08  8 of 9 PASS not_null_adid_data_longitude ....................................... [[32mPASS[0m in 0.22s]
[2025-10-31T09:29:08.903+0000] {subprocess.py:93} INFO - [0m09:29:08  7 of 9 PASS not_null_adid_data_latitude ........................................ [[32mPASS[0m in 0.27s]
[2025-10-31T09:29:08.914+0000] {subprocess.py:93} INFO - [0m09:29:08  9 of 9 PASS unique_adid_data_adid .............................................. [[32mPASS[0m in 0.20s]
[2025-10-31T09:29:08.930+0000] {subprocess.py:93} INFO - [0m09:29:08
[2025-10-31T09:29:08.931+0000] {subprocess.py:93} INFO - [0m09:29:08  Finished running 9 data tests in 0 hours 0 minutes and 0.88 seconds (0.88s).
[2025-10-31T09:29:08.972+0000] {subprocess.py:93} INFO - [0m09:29:08
[2025-10-31T09:29:08.972+0000] {subprocess.py:93} INFO - [0m09:29:08  [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[2025-10-31T09:29:08.973+0000] {subprocess.py:93} INFO - [0m09:29:08
[2025-10-31T09:29:08.974+0000] {subprocess.py:93} INFO - [0m09:29:08  [31mFailure in test cohort_max_adids_test_adid_data_cohort (models/schema.yml)[0m
[2025-10-31T09:29:08.975+0000] {subprocess.py:93} INFO - [0m09:29:08    Got 2 results, configured to fail if != 0
[2025-10-31T09:29:08.976+0000] {subprocess.py:93} INFO - [0m09:29:08
[2025-10-31T09:29:08.976+0000] {subprocess.py:93} INFO - [0m09:29:08    compiled code at target/compiled/dbt_test_lab2/models/schema.yml/cohort_max_adids_test_adid_data_cohort.sql
[2025-10-31T09:29:08.977+0000] {subprocess.py:93} INFO - [0m09:29:08
[2025-10-31T09:29:08.978+0000] {subprocess.py:93} INFO - [0m09:29:08    See test failures:
[2025-10-31T09:29:08.978+0000] {subprocess.py:93} INFO -   ------------------------------------------------------------------------------------------
[2025-10-31T09:29:08.978+0000] {subprocess.py:93} INFO -   select * from "dbt_test_db"."public_test_results"."cohort_max_adids_test_adid_data_cohort"
[2025-10-31T09:29:08.978+0000] {subprocess.py:93} INFO -   ------------------------------------------------------------------------------------------
[2025-10-31T09:29:08.979+0000] {subprocess.py:93} INFO - [0m09:29:08
[2025-10-31T09:29:08.979+0000] {subprocess.py:93} INFO - [0m09:29:08  Done. PASS=8 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=9
[2025-10-31T09:29:12.756+0000] {subprocess.py:93} INFO - DBT TEST FAILED
[2025-10-31T09:29:12.761+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2025-10-31T09:29:12.761+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-10-31T09:29:13.012+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=dbt_dq_pipeline_alert, task_id=dbt_test, run_id=manual__2025-10-31T09:28:41.175056+00:00, execution_date=20251031T092841, start_date=20251031T092905, end_date=20251031T092913
[2025-10-31T09:29:13.059+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 0
[2025-10-31T09:29:13.074+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-10-31T09:29:13.099+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
You said:
let's do more: i want to create an utils python file where i save my two functions to make the DAG clean and then i would make the send eamil task only run when the test fails it will only be dependant of the test task 
You said:
i encoutered an error saying  : 
Broken DAG: [/opt/airflow/dags/dbt_dq_pipeline_dag_alert.py] Traceback (most recent call last):
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/dbt_dq_pipeline_dag_alert.py", line 5, in <module>
    from utils.email_utils import run_dbt_test_fail, send_custom_email
ModuleNotFoundError: No module named 'utils'

but i made sure that inside my container i have my utils module created : 

 marwen  docker exec -u root -it airflow-scheduler bash

root@335c1402106b:/opt/airflow# ls -l
total 120
-rw-------  1 airflow root 105269 Oct 31 09:04 airflow.cfg
drwxrwxr-x  3 root    root   4096 Oct 30 22:23 dags
drwxr-xr-x 10 airflow root   4096 Oct 25 12:33 dbt
drwxrwxr-x 10 root    root   4096 Oct 26 07:40 dbt_test_lab2
drwxrwxr-x  7 root    root   4096 Oct 31 09:01 logs
root@335c1402106b:/opt/airflow# ls -l dags/
total 20
drwxr-xr-x 2 root root 4096 Oct 31 09:40 __pycache__
-rw-rw-r-- 1 root root 1262 Oct 29 09:03 dbt_dq_pipeline_dag.py
-rw-rw-r-- 1 root root 1221 Oct 31 09:40 dbt_dq_pipeline_dag_alert.py
-rw-rw-r-- 1 root root  470 Oct 30 10:36 test_email_dag.py
drwxrwxr-x 2 root root 4096 Oct 31 09:40 utils
root@335c1402106b:/opt/airflow# ls -l dags/utils/
total 4
-rw-rw-r-- 1 root root    0 Oct 31 09:39 __init__.py
-rw-rw-r-- 1 root root 1628 Oct 31 09:40 email_utils.py
root@335c1402106b:/opt/airflow# 

let's fix this nad also let's twoick the content of the email bu adding some details by reading the run_results and showing in the email the tests falied and the message,  the compiled code, the relation name ...
You said:
here is an example of a failed test result : 

     {
            "status": "fail",
            "timing": [
                {
                    "name": "compile",
                    "started_at": "2025-10-31T09:49:11.272658Z",
                    "completed_at": "2025-10-31T09:49:11.290059Z"
                },
                {
                    "name": "execute",
                    "started_at": "2025-10-31T09:49:11.310680Z",
                    "completed_at": "2025-10-31T09:49:11.581433Z"
                }
            ],
            "thread_id": "Thread-2 (worker)",
            "execution_time": 0.33519792556762695,
            "adapter_response": {
                "_message": "SELECT 1",
                "code": "SELECT",
                "rows_affected": 1
            },
            "message": "Got 2 results, configured to fail if != 0",
            "failures": 2,
            "unique_id": "test.dbt_test_lab2.cohort_max_adids_test_adid_data_cohort.8795d5bb9e",
            "compiled": true,
            "compiled_code": "\n\nSELECT cohort, COUNT(DISTINCT adid) AS adids_count\nFROM \"dbt_test_db\".\"public\".\"adid_data\"\nGROUP BY cohort\nHAVING COUNT(DISTINCT adid) > 100\n\n",
            "relation_name": "\"dbt_test_db\".\"public_test_results\".\"cohort_max_adids_test_adid_data_cohort\"",
            "batch_results": null
        },

so please correct your parsing function 
You said:
why you did this bro let's go back to the last implementation only i told you to correct ypour parsing startefie 
You said:
ohhhhhhh what the hack i meant to correc this version : 

import os
import json
import subprocess
from airflow.utils.email import send_email_smtp


def run_dbt_test_fail():
    """Run dbt tests and raise an exception if any test fails."""
    dbt_dir = "/opt/airflow/dbt_test_lab2"
    cmd = f"cd {dbt_dir} && dbt test --profiles-dir . --store-failures"
    process = subprocess.run(cmd, shell=True, capture_output=True, text=True)

    print(process.stdout)
    print(process.stderr)

    results_path = os.path.join(dbt_dir, "target", "run_results.json")

    # Load test results if they exist
    failed_tests = []
    if os.path.exists(results_path):
        with open(results_path, "r") as f:
            data = json.load(f)
            for result in data.get("results", []):
                if result.get("status") != "pass":
                    failed_tests.append({
                        "test_name": result["unique_id"],
                        "message": result.get("message", "No message"),
                        "compiled_code": result.get("compiled_code", "N/A"),
                        "relation_name": result.get("node", {}).get("alias", "N/A"),
                    })

    if failed_tests:
        # Save failed test details to XCom or file for later use
        summary_path = os.path.join(dbt_dir, "target", "failed_tests_summary.json")
        with open(summary_path, "w") as f:
            json.dump(failed_tests, f, indent=2)
        raise Exception(f"❌ DBT test(s) failed — {len(failed_tests)} test(s) failed. See summary in {summary_path}.")

    return "✅ All DBT tests passed successfully."


def send_custom_email():
    """Send email alert with DBT failure details."""
    dbt_dir = "/opt/airflow/dbt_test_lab2"
    summary_path = os.path.join(dbt_dir, "target", "failed_tests_summary.json")

    if os.path.exists(summary_path):
        with open(summary_path, "r") as f:
            failed_tests = json.load(f)
    else:
        failed_tests = []

    # Build HTML content
    if failed_tests:
        details_html = "<ul>"
        for t in failed_tests:
            details_html += f"""
            <li>
                <b>{t['relation_name']}</b> — {t['message']}<br>
                <pre>{t['compiled_code']}</pre>
            </li>
            """
        details_html += "</ul>"
    else:
        details_html = "<p>No failed test details found.</p>"

    html_content = f"""
    <h3>🚨 DBT Test Failure Alert</h3>
    <p>The latest dbt test run encountered failures.</p>
    <p><b>Total failed tests:</b> {len(failed_tests)}</p>
    {details_html}
    """

    send_email_smtp(
        to='mejri.marwen00@gmail.com',
        subject='🚨 DBT Test Failure Alert',
        html_content=html_content
    )
You said:
correc the send custom email : 

def send_custom_email():
    """Send email alert with DBT failure details."""
    dbt_dir = "/opt/airflow/dbt_test_lab2"
    summary_path = os.path.join(dbt_dir, "target", "failed_tests_summary.json")

    if os.path.exists(summary_path):
        with open(summary_path, "r") as f:
            failed_tests = json.load(f)
    else:
        failed_tests = []

    if failed_tests:
        details_html = "<ul>"
        for t in failed_tests:
            details_html += f"""
            <li>
                <b>Relation:</b> {t['relation_name']}<br>
                <b>Message:</b> {t['message']}<br>
                <b>Compiled SQL:</b><br>
                <pre>{t['compiled_code']}</pre>
            </li>
            """
        details_html += "</ul>"
    else:
        details_html = "<p>No failed test details found.</p>"

    html_content = f"""
    <h3>🚨 DBT Test Failure Alert</h3>
    <p>The latest dbt test run encountered failures.</p>
    <p><b>Total failed tests:</b> {len(failed_tests)}</p>
    {details_html}
    """

    send_email_smtp(
        to='mejri.marwen00@gmail.com',
        subject='🚨 DBT Test Failure Alert',
        html_content=html_content
    )
✅ Fix summary:

Works with both {"results": [...]} and raw list JSON structures.

Uses rel

TO USE THESE 
def send_custom_email():
    """Send email alert using SMTP credentials (from environment or airflow.cfg)."""
    smtp_config = {
        'smtp_host': os.getenv('AIRFLOW__SMTP__SMTP_HOST'),
        'smtp_starttls': os.getenv('AIRFLOW__SMTP__SMTP_STARTTLS', 'True').lower() == 'true',
        'smtp_ssl': os.getenv('AIRFLOW__SMTP__SMTP_SSL', 'False').lower() == 'true',
        'smtp_user': os.getenv('AIRFLOW__SMTP__SMTP_USER'),
        'smtp_password': os.getenv('AIRFLOW__SMTP__SMTP_PASSWORD'),
        'smtp_port': int(os.getenv('AIRFLOW__SMTP__SMTP_PORT', 587)),
        'smtp_mail_from': os.getenv('AIRFLOW__SMTP__SMTP_MAIL_FROM'),
        'smtp_timeout': 30,
        'smtp_retry_limit': 5,
    }

    send_email_smtp(
        to='mejri.marwen00@gmail.com',
        subject='🚨 DBT Test Failure Alert',
        html_content='<h3>DBT tests failed!</h3><p>Please check Airflow logs for details.</p>',
        conn_id=None,
        mime_subtype='mixed',
        dryrun=False,
        **smtp_config
    )
You said:
okay great the pipeline is working smooth but i want to update my dag the way you suggested in 2 messages ago like this : 

from airflow.operators.python import PythonOperator, ShortCircuitOperator
from utils import parse_dbt_test_results, send_failure_email

def check_dbt_results(**context):
    has_failures, message = parse_dbt_test_results("/path/to/target/run_results.json")
    context['ti'].xcom_push(key='dbt_message', value=message)
    return has_failures  # True means continue → email task will run

def send_email_task(**context):
    message = context['ti'].xcom_pull(key='dbt_message')
    send_failure_email(
        subject="DBT Test Failures",
        body=message,
        to=["your_email@example.com"]
    )

check_tests = ShortCircuitOperator(
    task_id="check_dbt_results",
    python_callable=check_dbt_results,
    provide_context=True,
    dag=dag,
)

send_email = PythonOperator(
    task_id="send_failure_email",
    python_callable=send_email_task,
    provide_context=True,
    dag=dag,
)

dbt_test_task >> check_tests >> send_email

but let's focus on keeping the DAG working here is the DAG and the email utils file : 

email_utils file : 

import os
import json
import subprocess
from airflow.utils.email import send_email_smtp


def run_dbt_test_fail():
    """Run dbt tests and raise an exception if any test fails."""
    dbt_dir = "/opt/airflow/dbt_test_lab2"
    cmd = f"cd {dbt_dir} && dbt test --profiles-dir . --store-failures"
    process = subprocess.run(cmd, shell=True, capture_output=True, text=True)

    print("*************    process.stdout:     ", process.stdout)
    print("*************    process.stderr:     ", process.stderr)

    results_path = os.path.join(dbt_dir, "target", "run_results.json")

    failed_tests = []
    if os.path.exists(results_path):
        with open(results_path, "r") as f:
            data = json.load(f)

            # Handle both possible formats: dict with "results" or list
            results = data.get("results", data if isinstance(data, list) else [])

            for result in results:
                status = result.get("status", "").lower()
                failures = result.get("failures", 0)

                if status == "fail" or failures > 0:
                    failed_tests.append({
                        "test_name": result.get("unique_id", "N/A"),
                        "message": result.get("message", "No message"),
                        "compiled_code": result.get("compiled_code", "N/A"),
                        "relation_name": result.get("relation_name", "N/A"),
                    })

    if failed_tests:
        summary_path = os.path.join(dbt_dir, "target", "failed_tests_summary.json")
        with open(summary_path, "w") as f:
            json.dump(failed_tests, f, indent=2)
        raise Exception(f"❌ DBT test(s) failed — {len(failed_tests)} test(s) failed. See summary in {summary_path}.")

    return "✅ All DBT tests passed successfully."


def send_custom_email():
    """Send email alert with DBT failure details using dynamic SMTP configuration."""
    import os
    import json
    from airflow.utils.email import send_email_smtp

    dbt_dir = "/opt/airflow/dbt_test_lab2"
    summary_path = os.path.join(dbt_dir, "target", "failed_tests_summary.json")

    if os.path.exists(summary_path):
        with open(summary_path, "r") as f:
            failed_tests = json.load(f)
    else:
        failed_tests = []

    # Build email content
    if failed_tests:
        details_html = "<ul>"
        for t in failed_tests:
            details_html += f"""
            <li>
                <b>Relation:</b> {t.get('relation_name', 'N/A')}<br>
                <b>Message:</b> {t.get('message', 'N/A')}<br>
                <b>Compiled SQL:</b><br>
                <pre>{t.get('compiled_code', 'N/A')}</pre>
            </li>
            """
        details_html += "</ul>"
    else:
        details_html = "<p>No failed test details found.</p>"

    html_content = f"""
    <h3>🚨 DBT Test Failure Alert</h3>
    <p>The latest dbt test run encountered failures.</p>
    <p><b>Total failed tests:</b> {len(failed_tests)}</p>
    {details_html}
    """

    # Dynamically load SMTP config from environment (Airflow resolves these from airflow.cfg)
    smtp_config = {
        'smtp_host': os.getenv('AIRFLOW__SMTP__SMTP_HOST', 'smtp.gmail.com'),
        'smtp_starttls': os.getenv('AIRFLOW__SMTP__SMTP_STARTTLS', 'True').lower() == 'true',
        'smtp_ssl': os.getenv('AIRFLOW__SMTP__SMTP_SSL', 'False').lower() == 'true',
        'smtp_user': os.getenv('AIRFLOW__SMTP__SMTP_USER'),
        'smtp_password': os.getenv('AIRFLOW__SMTP__SMTP_PASSWORD'),
        'smtp_port': int(os.getenv('AIRFLOW__SMTP__SMTP_PORT', 587)),
        'smtp_mail_from': os.getenv('AIRFLOW__SMTP__SMTP_MAIL_FROM', 'Airflow <mejri.marwen00@gmail.com>'),
        'smtp_timeout': int(os.getenv('AIRFLOW__SMTP__SMTP_TIMEOUT', 30)),
        'smtp_retry_limit': int(os.getenv('AIRFLOW__SMTP__SMTP_RETRY_LIMIT', 5)),
    }

    # Send the email using Airflow’s SMTP utility but forcing config params
    send_email_smtp(
        to='mejri.marwen00@gmail.com',
        subject='🚨 DBT Test Failure Alert',
        html_content=html_content,
        conn_id=None,
        mime_subtype='mixed',
        dryrun=False,
        **smtp_config
    )

and here is my DAG : 

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime

import sys
import os

# Ensure utils module is importable
sys.path.append(os.path.join(os.path.dirname(__file__)))

from utils.email_utils import run_dbt_test_fail, send_custom_email

default_args = {
    'owner': 'marwen',
    'email_on_failure': False,
}

with DAG(
    'dbt_dq_pipeline_alert',
    default_args=default_args,
    description='Run dbt tests and send alerts on failure',
    schedule_interval='@daily',
    start_date=datetime(2025, 10, 1),
    catchup=False,
) as dag:

    dbt_seed = BashOperator(
        task_id='dbt_seed',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt seed --profiles-dir .'
    )

    dbt_run = BashOperator(
        task_id='dbt_run',
        bash_command='cd /opt/airflow/dbt_test_lab2 && dbt run --profiles-dir .'
    )

    dbt_test = PythonOperator(
        task_id='dbt_test',
        python_callable=run_dbt_test_fail
    )

    send_alert = PythonOperator(
        task_id='send_custom_email',
        python_callable=send_custom_email,
        trigger_rule='one_failed'  # Runs only if dbt_test fails
    )

    # DAG flow
    dbt_seed >> dbt_run >> dbt_test
    dbt_test >> send_alert  # only triggers if dbt_test fails
You said:
but where you made it ugly by writing this inside the DAG : 

"""Check dbt test results and return True if there are failures (to trigger email)."""
    dbt_dir = "/opt/airflow/dbt_test_lab2"
    results_path = os.path.join(dbt_dir, "target", "run_results.json")
    summary_path = os.path.join(dbt_dir, "target", "failed_tests_summary.json")

    failed_tests = []

    if os.path.exists(results_path):
        with open(results_path, "r") as f:
            data = json.load(f)
            results = data.get("results", data if isinstance(data, list) else [])
            for result in results:
                status = result.get("status", "").lower()
                if status == "fail" or result.get("failures", 0) > 0:
                    failed_tests.append({
                        "relation_name": result.get("relation_name", "N/A"),
                        "message": result.get("message", "N/A"),
                        "compiled_code": result.get("compiled_code", "N/A")
                    })

    # Save failed tests for reference (email_utils will pick it up)
    if failed_tests:
        with open(summary_path, "w") as f:
            json.dump(failed_tests, f, indent=2)

why not leaving it in the email utils file : 

like this previous implementation : 

def run_dbt_test_fail():
    """Run dbt tests and raise an exception if any test fails."""
    dbt_dir = "/opt/airflow/dbt_test_lab2"
    cmd = f"cd {dbt_dir} && dbt test --profiles-dir . --store-failures"
    process = subprocess.run(cmd, shell=True, capture_output=True, text=True)

    print("*************    process.stdout:     ", process.stdout)
    print("*************    process.stderr:     ", process.stderr)

    results_path = os.path.join(dbt_dir, "target", "run_results.json")

    failed_tests = []
    if os.path.exists(results_path):
        with open(results_path, "r") as f:
            data = json.load(f)

            # Handle both possible formats: dict with "results" or list
            results = data.get("results", data if isinstance(data, list) else [])

            for result in results:
                status = result.get("status", "").lower()
                failures = result.get("failures", 0)

                if status == "fail" or failures > 0:
                    failed_tests.append({
                        "test_name": result.get("unique_id", "N/A"),
                        "message": result.get("message", "No message"),
                        "compiled_code": result.get("compiled_code", "N/A"),
                        "relation_name": result.get("relation_name", "N/A"),
                    })

    if failed_tests:
        summary_path = os.path.join(dbt_dir, "target", "failed_tests_summary.json")
        with open(summary_path, "w") as f:
            json.dump(failed_tests, f, indent=2)
        raise Exception(f"❌ DBT test(s) failed — {len(failed_tests)} test(s) failed. See summary in {summary_path}.")

    return "✅ All DBT tests passed successfully."

but of course by tweaking it and then uploading it to your DAG and cleaning the check_dbt_results function 
You said:
well let's also retuen the failed_tests and then pushing them and the len as you did earlier to the XCOM to make it more visible to the Aifrlow UI 
You said:
okay great for now i wantyou to explain to me the DAG structure (i mean codeeddee) and also let's made this change : 

I want my task test_dbt to be red (failed) when a there is a test fail and then the alert task will be executed, otherwise if the test went without failure it will be marked green and the alert task will be skipped 

Now it's executed as : test fails = > task green and send alert triguered and succeded

please do the update in a new DAG (with new dag_id)
You said:
great so now how you would propose to scale up the hole project pipeline maybe transforming the project to new real life scenario where we use at first step web scraping to scrape product data 'raw data) and then we ingest it into snowflake and then we will use DBT to transform data and create our model (transform/clean.... and ofc testing and alerting ) and then ingest the structured data into postgres at the end why not do some BI reporting using Looker or Superset

ofc we will always use open source tools and use docker for ease of deployment we can also think of dbt tests as 2 types :

- Data checks for validation and data quality and also so business data checks for monitoring purpose like nbr of product_ids ingested in each run versus the last run, heavy price changes to capture aberants values and so on 

I want you to guide me and ofc suggest whatever you see would help me in my learning journey and gainign new skills 

I want to scrape different data models as raw not only products we can ssay product and its properties and then the second model would be reviews or anything else and maybe categories , brand model ......

maybe also we can add some AI model use inside our pipeline to implement a business need 

enjoy!
You said:
let's start with a pretty porject description,pipeline architecture, tools models to scrape and so on i want to use gut and github to push my project there 
You said:
well we will use Snowflake to ingest our scraped data and then postgres for ingesting transformed and structured data for BI reporting 

so let's start with tne next steps
You said:
you forget the readme file with the description (full description) to start with github
You said:
could you create a project architecture image for me 
You said:
well make it look more professionnal and use tools image and you can take this other project architecture as an example : 

You said:
it s great but you missed the Postgres, Superset , Minio (before Snowflake) Layers I will give you again the project architecture : 
Architecture (high-level)
Scraping layer — Python scraper(s) (e.g. scraper_utils.py) produce raw JSON/Parquet files (products, reviews, categories, brands).
Landing / Raw zone — Raw files saved in MinIO (S3-compatible) and then uploaded  to Snowflake staging.
Ingestion layer — Airflow DAGs run ingestion tasks (PythonOperators) to load raw data into Snowflake (COPY/PUT or Snowflake connector).
Transformation layer — dbt project (models, macros, tests) for transforming raw into curated marts. dbt test outputs saved to target/run_results.json.
Testing + Alerting — Airflow runs dbt; on failures, we parse run_results.json and send alerts (email or Slack).
Serving layer — Transformed data loaded into PostgreSQL for BI; Superset connects to PostgreSQL for dashboards.
You said:
now i want you to recreate the project description for me and you can take this project description as an example : 

Stock Market Real-Time Data Streaming Project
🔍 Project Overview

This project is a real-time data streaming pipeline for stock market data. We ingest live and historical data from sources like Yahoo Finance, Alpha Vantage, and Twelve Data WebSocket API. The data is processed using Kafka and Spark, stored in MinIO, and sent to Snowflake for analytics. Apache Airflow orchestrates every component, and everything runs inside Docker containers for easy development and deployment.

📊 Technical Requirements

🧰 Technologies Used
Data Sources:
Yahoo Finance
Alpha Vantage
Twelve Data WebSocket API
Streaming and Processing:
Apache Kafka (For real-time data ingestion and processing)
Apache Spark (Batch + Streaming) (For data transformation and analysis)
Storage:
MinIO (For storing intermediate results)
Snowflake (For analytics and long-term storage)
Orchestration:
Apache Airflow (For orchestrating the entire pipeline)
Containerization:
Docker & Docker Compose
Other Tools:
PostgreSQL
Python
WebSocket
requests
yfinance
kafka-python
Pipeline Architecture
Below is the high-level architecture of the project:

Project Architecture

🚀 Current Progress (As of April 2025)

✅ Dockerized Infrastructure (DONE)
Kafka (3 brokers), Zookeeper
Spark Master + 3 Workers (resource-limited)
PostgreSQL
MinIO
Airflow Webserver + Scheduler
Custom container for data ingestion scripts
Kafka volume-based persistence implemented
Automatic topic creation with partitions and replication factor
✅ Historical Data Ingestion (DONE)
ingest_historical_yahoo.py: Stores daily OHLCV data to Kafka/MinIO.
ingest_historical_alpha.py: Same logic for Alpha Vantage.
✅ Real-Time Streaming Ingestion (DONE)
ingest_realtime_yahoo.py: Simulates live data by looping historical day data.
ingest_realtime_alpha_vantage.py: Loops Alpha's 1-minute endpoint.
ingest_realtime_twelvedata_ws.py: Full WebSocket-based live feed.
Kafka Producer logging and connection check added.
✅ Scripts Containerization (DONE)
Dockerfile with:
Dependencies in requirements.txt
scripts/ folder copied
Can docker exec to launch manually or used in Airflow later.
📆 Project Structure (Simplified)

├── dags/ # Airflow DAGs (to be implemented) ├── scripts/ # Python ingestion scripts ├── config/ # API + Kafka config JSONs ├── logs/ # Airflow logs ├── requirements.txt # Python dependencies ├── Dockerfile # For custom ingestion container ├── .dockerignore ├── docker-compose.yml # Full pipeline orchestration └── README.md # This file

✅ What's Next

Spark Structured Streaming: Consume and transform Kafka data.
Store Processed Data in MinIO: Save intermediate results in Parquet format.
Archive and Compact Topics: Manage Kafka topics efficiently.
Export Processed Data to Snowflake: Use Airflow to load data into Snowflake.
Orchestrate Ingestion + ETL in Airflow DAGs: Automate the entire pipeline.
Optional Cloud Deployment: Deploy on ECS / EC2 / Azure.
🔎 Screenshots (To Add)

Airflow UI
MinIO bucket structure
Spark UI with running job
Kafka CLI producing/consuming messages
📂 GitHub Repo https://github.com/marwenmejri/Stock-Market-Real-Time-Data-Streaming

This repo is under active development. Contributions and feedback are welcome!

📅 Maintained by Marwen Mejri - Senior Data Engineer

Let's stream the market ⚡

So update the last REDAME file you generated for me with what needed 

and then add a gigtignore file 
You said:
regenrate the image for me and you can take the image i shared as exmaple for the architecture but be aware that the image is containing too many typos and wrong tool images 
You said:
no you made it very wrong i want the same architecture (lines + rectangles) but fix the typo in the wrods and use a correct real tool images
You said:
okay i want to pass to a new chat give me a full description for all waht we discussed here 