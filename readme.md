# Web Scraping & Data Pipeline Project

## ğŸ” Project Overview

This project implements an end-to-end **data pipeline for web-scraped e-commerce data**. The pipeline automates the extraction, ingestion, transformation, and visualization of structured and unstructured data from multiple sources. It combines modern data engineering tools to ensure scalability, maintainability, and observability.

The system collects product, category, brand, and review data using Python scrapers, stores raw data in **MinIO**, loads it into **Snowflake** for centralized analytics, transforms it using **dbt**, and serves curated data to **PostgreSQL** for **Apache Superset** dashboards. **Apache Airflow** orchestrates all processes, ensuring reliability and automation.

## ğŸ“Š Architecture Overview

### High-Level Workflow

1. **Scraping Layer**: Python scrapers collect e-commerce data (products, prices, reviews, categories) in JSON/Parquet format.
2. **Landing/Raw Zone**: Raw data files are uploaded to MinIO (S3-compatible) for storage.
3. **Ingestion Layer**: Airflow DAGs load raw files from MinIO to Snowflake staging tables using the Snowflake Python connector.
4. **Transformation Layer**: dbt models clean, normalize, and aggregate data into business-ready marts.
5. **Testing & Alerting**: dbt tests are triggered via Airflow; failed tests trigger alert notifications.
6. **Serving Layer**: Transformed data is pushed to PostgreSQL and visualized in Superset for BI dashboards.

## ğŸ› ï¸ Technologies Used

### **Data Collection**

* **Python (requests, BeautifulSoup, or Scrapy)** â€“ for scraping static and dynamic web data.

### **Storage & Processing**

* **MinIO** â€“ raw/landing zone storage (S3-compatible).
* **Snowflake** â€“ main data warehouse for structured data and analytics.
* **PostgreSQL** â€“ serves as the BI and reporting database.

### **Orchestration & Transformation**

* **Apache Airflow** â€“ orchestrates scraping, ingestion, and transformation tasks.
* **dbt (Data Build Tool)** â€“ transforms and tests data within Snowflake.

### **Visualization & Reporting**

* **Apache Superset** â€“ used for dashboarding and data exploration.

### **Containerization & Deployment**

* **Docker & Docker Compose** â€“ all services are containerized for consistent local and production environments.

## ğŸ› ï¸ Tools Summary

| Layer              | Tool                          | Purpose                                   |
| ------------------ | ----------------------------- | ----------------------------------------- |
| Scraping           | Python                        | Extracts raw data from websites           |
| Storage            | MinIO                         | Stores raw JSON/Parquet files             |
| Ingestion          | Airflow + Snowflake Connector | Loads data into Snowflake staging         |
| Transformation     | dbt                           | Cleans and models data                    |
| Testing & Alerting | dbt + Airflow                 | Validates data and triggers notifications |
| Serving            | PostgreSQL                    | Stores transformed datasets for BI        |
| Visualization      | Apache Superset               | Dashboards and analytics                  |

## ğŸš€ Pipeline Architecture

See the visual architecture diagram in the project 

![Data Pipeline Architecture](assets/data_pipeline_architecture.png)


## ğŸ”— Project Structure

```
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â”œâ”€â”€ scrape_to_minio_dag.py
â”‚   â”œâ”€â”€ load_to_snowflake_dag.py
â”‚   â”œâ”€â”€ transform_dbt_dag.py
â”‚   â””â”€â”€ test_and_alert_dag.py
â”œâ”€â”€ dbt/                       # dbt project folder
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ macros/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ scripts/                   # Python scrapers & ingestion utilities
â”‚   â”œâ”€â”€ scraper_utils.py
â”‚   â”œâ”€â”€ upload_to_minio.py
â”‚   â””â”€â”€ minio_snowflake_loader.py
â”œâ”€â”€ docker-compose.yml          # Docker services definition
â”œâ”€â”€ Dockerfile                  # Custom image for scraper/ingestion
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ .gitignore                  # Git ignored files
```

## ğŸ“… Current Progress (As of November 2025)

âœ… Dockerized Infrastructure (MinIO, PostgreSQL, Airflow, dbt, Superset)

âœ… Scraping Framework (scraper_utils.py) for multiple data sources

âœ… Airflow DAGs for ingestion and transformation

âœ… dbt models for product and review transformation

âœ… Alerting system integrated with dbt test results

## ğŸ”„ Next Steps

* Implement real alert notifications via Slack or email in Airflow.
* Add metadata tracking (data freshness, record counts) to Airflow logs.
* Extend scrapers to handle multi-language and multi-region product listings.
* Optimize dbt performance and add incremental models.
* Automate Superset dashboard refresh after dbt runs.

## ğŸ–¼ï¸ GitHub Repository

**Repository:** [https://github.com/marwenmejri/OpenCommerce-Data-Platform](https://github.com/marwenmejri/OpenCommerce-Data-Platform)

Maintained by **Marwen Mejri â€“ Senior Data Engineer**

"From web to warehouse â€“ automated, reliable, and production-ready."
